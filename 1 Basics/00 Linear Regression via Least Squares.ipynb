{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09288cd2",
   "metadata": {},
   "source": [
    "**Information:** *Basic concepts and simple examples of linear regression via least squares*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:** *06.02.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb6226",
   "metadata": {},
   "source": [
    "# Basic concepts\n",
    "\n",
    "## Regression\n",
    "- **Regression** refers to a set of methods for modeling the relationship between one or more independent variables and a dependent variable. \n",
    "    - In the natural sciences and social sciences, the purpose of regression is most often to **characterize** the relationship between the inputs and outputs\n",
    "    - In machine learning, it is most often concerned with **prediction**\n",
    "- Regression problems pop up whenever a prediction for a **numerical value** is wanted.\n",
    "    - Predicting prices\n",
    "    - Predicting length of stay\n",
    "    - Demand forecasting\n",
    "    - ...\n",
    "- Mathematical Representation\n",
    "    - Given $n$ observations consisting of $$\\mathbf{X}_{1:n}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_n\\}$$ which is usually known as *inputs*, *features*, etc. and $$\\mathbf{y}_{1:n}=\\{y_1,y_2,\\cdots,y_n\\}$$ which is **continuous** and is usually known as *outputs*, *targets*, etc.\n",
    "    - The Regression Problem is to use the data to learn the **map** between $\\mathbf{x}$ and $y$\n",
    "\n",
    "## Linear Regression\n",
    "- May be both the **simplest** and most **popular** among the standard tools to regression\n",
    "- A kind of traditional **supervised machine learning**\n",
    "- **Assumptions**:\n",
    "    - The relationship between the independent variables $\\mathbf{x}$ and the dependent variable $y$ is **linear**\n",
    "        - That is to say, $y$ can be expressed as a **weighted sum** of the elements in $x$\n",
    "        - Or in other words, *Linear regression* models the output as a line ($\\text{dim}(\\mathbf{x})=1$) or hyperplane ($\\text{dim}(\\mathbf{x}>1)$)\n",
    "    - There exist some noise on the observations and any noise is **well-behaved** (following a Gaussian distribution)\n",
    "- **Linear Regression Model**:\n",
    "    - The linear regression model is defined by the coefficients (or **parameters**) for each feature\n",
    "    - For $\\mathbf{x}\\in\\mathbb{R}^d,\\mathbf{x}=[x_1,x_2,\\cdots,x_d]^T$, denote the parameters to the $\\theta$: $$\\hat{y}=f(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_dx_d$$ Let $\\boldsymbol{\\theta}=[\\theta_0,\\theta_1,\\theta_2,\\cdots,\\theta_{d}]^T$ and augmented $\\tilde{\\mathbf{x}}=[1,x_1,x_2,\\cdots,x_d]^T$ The the model can be written as $$\\hat{y}=f(x)=\\boldsymbol{\\theta}^T\\tilde{\\mathbf{x}}$$\n",
    "    - This is known as **parametric model**\n",
    "- **Goal of Linear Regression**:\n",
    "    - Notice that in **assumption**, we assume that there exist some noises in the observations following a Gaussian distribution. Therefore, we are **not** going to directly solve the equation and expect to get a result $\\boldsymbol{\\theta}$ that  $$y_i=\\boldsymbol{\\theta}\\tilde{\\mathbf{x}}_i\\ \\text{for}\\ 1\\le i\\le n$$ \n",
    "    - Even when we are confident that the underlying relationship is linear, the noise term should be taken into consideration\n",
    "    - The goal of linear regression is to find the parameters $\\boldsymbol{\\theta}$ that **minimize the prediction error**\n",
    "\n",
    "## Loss function\n",
    "The most *popular* loss function in regression problems is the **squared error**. \n",
    "- When the prediction for an example $i$ is $\\hat{y}_i$ and the corresponding true label is $y_i$, the squared error is given by $$l_i(\\boldsymbol{\\theta})=\\left(\\hat{y}_i-y_i\\right)^2=\\left(\\boldsymbol{\\theta}^T\\tilde{\\mathbf{x}}_i-y_i\\right)^2$$\n",
    "    - Note: In some notations, there is a $\\frac{1}{2}$ term for the convenience of notating derivatives, but it makes no difference as optimization\n",
    "- To measure the quality of a model on the entire dataset of $n$ examples, we simply sum (or **equivalently**, average) the losses on dataset $$L(\\boldsymbol{\\theta})=\\underset{i=1}{\\overset{n}{\\Sigma}}l_i(\\boldsymbol{\\theta})=\\underset{i=1}{\\overset{n}{\\Sigma}}\\left(\\boldsymbol{\\theta}^T\\tilde{\\mathbf{x}}_i-y_i\\right)^2$$ In matrix notation: $$L(\\boldsymbol{\\theta})=\\|\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\|^2_2$$ Where $\\mathbf{y}=[y_1,y_2,\\cdots,y_n]^T\\in\\mathbb{R}^n,\\ \\boldsymbol{\\theta}=\\left[\\theta_0,\\theta_1,\\theta_2,\\cdots,\\theta_{d}\\right]^T\\in\\mathbb{R}^{d+1},\\ \\tilde{\\mathbf{X}}=\\left[\\tilde{\\mathbf{x}}_1,\\cdots,\\tilde{\\mathbf{x}}_n\\right]^T\\in\\mathbb{R}^{n\\times (d+1)},\\tilde{\\mathbf{x}}=[1,x_1,x_2,\\cdots,x_d]^T$\n",
    "    - When averaging the losses on dataset, it's called **Mean Squared Error**\n",
    "- Known as **Ordinary Least Squares (OLS)**\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fbfd4",
   "metadata": {},
   "source": [
    "## Closed-form solution for OLS\n",
    "Calculate the gradient of OLS\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\|^2_2\n",
    "&=\\nabla_{\\boldsymbol{\\theta}}\\left[\\left(\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)^T\\left(\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)\\right]\\\\\n",
    "&=\\left[2\\left(\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)^T\\cdot\\nabla_{\\boldsymbol{\\theta}}\\left[\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right]\\right]^T\\\\\n",
    "&=2\\left(\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)\\cdot\\left(-\\tilde{\\mathbf{X}}\\right)^T\\\\\n",
    "&=2\\left(-\\tilde{\\mathbf{X}}^T\\mathbf{y}+\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Set the gradient to zero *(first-order optimization)* and solve:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "2\\left(-\\tilde{\\mathbf{X}}^T\\mathbf{y}+\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}^*\\right)&=0\\\\\n",
    "\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}^*&=\\tilde{\\mathbf{X}}^T\\mathbf{y}\\\\\n",
    "\\boldsymbol{\\theta}^*&=\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T\\mathbf{y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "- This is known as **normal equation**, finds the regression coefficients **analytically**.\n",
    "- It's an one-step learning algorithm (as opposed to Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ae909",
   "metadata": {},
   "source": [
    "## Normal Equation vs Gradient Descent\n",
    "### Gradient Descent\n",
    "- Needs to choose GD-based algorithms and set appropriate parameters\n",
    "- Needs to do a lot of iterations\n",
    "- Works well with large $d$ (dimension of input data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22571be7",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "- Gets rid of setting parameters\n",
    "- Does not need to iterate - compute in one step\n",
    "- Slow if $d$ is large $(n\\le10^4)$\n",
    "- Needs to compute inverse of $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$, which is very slow\n",
    "    - Sometimes use math tricks like *QR factorization* to speed up computation\n",
    "- Leads to problems if $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is not invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da45af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
