{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d44ba2",
   "metadata": {},
   "source": [
    "**Information:** *Basic concepts and simple examples of Bayesian linear regression*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:** *07.01.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce8147",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "## Motivation\n",
    "In the chapter talking about ***Generalization and Regularization***, the concepts of parameter estimation, bias and variance are used to formally characterize notions of generalization, underfitting and overfitting. Here are some important remarks.\n",
    "\n",
    "- View the parameter estimator $\\hat{\\boldsymbol{\\theta}}$ as a **function** of the sampled training dataset $$\\hat{\\boldsymbol{\\theta}}=g\\left(\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\cdots,\\mathbf{x}^{(m)}\\right)$$\n",
    "\n",
    "\n",
    "- The datasets (training, testing and probably validation) are generated by a **i.i.d.** probability distribution over datasets called the **data-generating process** (i.i.d. assumptions can be applied to almost all the common tasks)\n",
    "\n",
    "\n",
    "- Assume that the true parameter value $\\boldsymbol{\\theta}$ is fixed but unknown\n",
    "\n",
    "\n",
    "- Since the **data** is drawn from a **random process**, any function of the data is random, which means the parameter estimator $\\hat{\\boldsymbol{\\theta}}$ is a **random variable**\n",
    "\n",
    "\n",
    "The concepts of **bias** and **variance** are used to measure the performance of a parameter estimator. However, **for obtaining a good estimator**, it's not a good idea to guess that some function might make a good estimator and then to analyze its bias and variance. This motivated some principles from which specific functions that are good estimators for different models can be derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed9051",
   "metadata": {},
   "source": [
    "## Definition\n",
    "- Consider a set of $m$ examples $\\mathcal{D}=\\left\\{\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\cdots,\\mathbf{x}^{(m)}\\right\\}$ drawn independently from the true but unknown data-generating distribution $p_{\\text{data}}\\left(\\mathbf{x}\\right)$. Let $p_{\\text{model}}\\left(\\mathbf{x}|\\boldsymbol{\\theta}\\right)$ be a parametric family of probability distributions over the same space indexed by $\\boldsymbol{\\theta}$\n",
    "    - That is to say, $p_{\\text{model}}$ maps any configuration $\\mathbf{x}$ to a real number estimating the true probability $p_{\\text{data}}(\\mathbf{x})$\n",
    "\n",
    "\n",
    "- Particularly, focus on the **likelihood** which is first introduced in the prerequisite chapter ***Probability Theory*** $$\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(m)}\\left|\\ \\boldsymbol{\\theta}\\right.\\sim p_{\\text{model}}\\left(\\mathbf{x}^{(1:m)}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)$$ As a fast review, the likelihood tells us how *plausible* it is to observe $\\mathbf{x}^{(1:m)}$ if we know the model parameters are $\\mathbf{\\theta}$\n",
    "\n",
    "\n",
    "- Since the examples are assumed to be drawn **independently**, the likelihood can be factorized $$p_{\\text{model}}\\left(\\mathbf{x}^{(1:m)}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)=\\underset{i=1}{\\overset{m}{\\Pi}}p_{\\text{model}}\\left(\\mathbf{x}^{(i)}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)$$ Then **maximum likelihood** estimator for $\\boldsymbol{\\theta}$ is then defined as $$\\boldsymbol{\\theta}_{\\text{ML}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}\\underset{i=1}{\\overset{m}{\\Pi}}p_{\\text{model}}\\left(\\mathbf{x}^{(i)}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)$$\n",
    "\n",
    "\n",
    "- While this simple production may lead to a lot of inconveniences such as **numerical underflow**, taking the **logarithm** of the likelihood does not change the location for maximum ($\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}$) but does conveniently transform a product into a sum $$\\boldsymbol{\\theta}_{\\text{ML}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}\\underset{i=1}{\\overset{m}{\\Sigma}}\\text{log}p_{\\text{model}}\\left(\\mathbf{x}^{(i)}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)$$\n",
    "\n",
    "\n",
    "- Obviously, rescaling the likelihood does not change the location for maximum ($\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}$), we can divide by $m$ to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution $\\hat{p}_{\\text{data}}$ defined by the training data $$\\boldsymbol{\\theta}_{\\text{ML}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{\\text{data}}}\\left[\\text{log}p_{\\text{model}}\\left(\\mathbf{x}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)\\right]$$\n",
    "\n",
    "\n",
    "- The most common choice for the likelihood of a single measurement is to pick it to be **Gaussian**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed9e80",
   "metadata": {},
   "source": [
    "## KL divergence\n",
    "- Maximum likelihood estimation can be viewed as minimizing the dissimilarity between the empirical distribution $\\hat{p}_{\\text{data}}$, defined by the training set and the model distribution, with the degree of dissimilarity between the two measure by the **KL divergence** $$D_{\\text{KL}}\\left(\\hat{p}_{\\text{data}}\\left\\|p_{\\text{model}}\\right.\\right)=\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{\\text{data}}}\\left[\\text{log}\\hat{p}_{\\text{data}}\\left(\\mathbf{x}\\right)-\\text{log}p_{\\text{model}}\\left(\\mathbf{x}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)\\right]$$ The term on the left is a function only of the data-generating process, not the model. This means when we train the model to minimize the KL divergence, we need only minimize $$-\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{\\text{data}}}\\left[\\text{log}p_{\\text{model}}\\left(\\mathbf{x}\\left|\\ \\boldsymbol{\\theta}\\right.\\right)\\right]$$\n",
    "\n",
    "\n",
    "- Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. By definition, any loss consisting a negative log-likelihood is a **cross-entropy** between the **empirical distribution** defined by the training set ($\\hat{p}_{\\text{data}}$), and the **probability distribution** defined by the model ($p_{\\text{model}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507f33e",
   "metadata": {},
   "source": [
    "## Conditional Log-Likelihood\n",
    "- To apply *MLE* to most **supervised learning** tasks of predicting $\\mathbf{y}$ given $\\mathbf{x}$, the maximum likelihood estimator is generalized to estimate a conditional probability $p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{x},\\boldsymbol{\\theta}\\right.\\right)$\n",
    "\n",
    "\n",
    "- Consider a set of $m$ examples $\\mathcal{D}=\\left\\{\\left(\\mathbf{x}^{(1)},\\mathbf{y}^{(1)}\\right),\\left(\\mathbf{x}^{(2)},\\mathbf{y}^{(2)}\\right),\\cdots,\\left(\\mathbf{x}^{(m)},\\mathbf{y}^{(m)}\\right)\\right\\}$ drawn independently from the true but unknown data-generating distribution $p_{\\text{data}}\\left(\\mathbf{x},\\mathbf{y}\\right)$. Factorize the data-generating process $$p_{\\text{data}}\\left(\\mathbf{x},\\mathbf{y}\\right)=p_{\\text{data}}\\left(\\mathbf{y}\\left|\\ \\mathbf{x}\\right.\\right)p_{\\text{data}}\\left(\\mathbf{x}\\right)$$ Let $p_{\\text{model}}\\left(\\mathbf{x},\\mathbf{y}|\\ \\boldsymbol{\\theta}\\right)$ be a parametric family of probability distributions over the same space indexed by $\\boldsymbol{\\theta}$. It also can be factorized $$p_{\\text{model}}\\left(\\mathbf{x},\\mathbf{y}|\\ \\boldsymbol{\\theta}\\right)=p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{x},\\boldsymbol{\\theta}\\right.\\right)p_{\\text{data}}\\left(\\mathbf{x}\\right)$$ Notice that the later part $p_{\\text{data}}\\left(\\mathbf{x}\\right)$ is **fixed and shared**, the maximum likelihood estimation is going to focus on $$p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{x},\\boldsymbol{\\theta}\\right.\\right)$$ Under the **i.i.d.** assumption, it can be decomposed into $$\\boldsymbol{\\theta}_{\\text{ML}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg max}}\\underset{i=1}{\\overset{m}{\\Pi}}p_{\\text{model}}\\left(\\mathbf{y}^{(i)}\\left|\\ \\mathbf{x}^{(i)},\\boldsymbol{\\theta}\\right.\\right)$$ Similarly, this optimization problem is usually converted into a minimization problem by the **negative logarithm** operation considering computation issues $$\\boldsymbol{\\theta}_{\\text{ML}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg min}}\\left[-\\underset{i=1}{\\overset{m}{\\Sigma}}\\text{log}\\left[p_{\\text{model}}\\left(\\mathbf{y}^{(i)}\\left|\\ \\mathbf{x}^{(i)},\\boldsymbol{\\theta}\\right.\\right)\\right]\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4585e6",
   "metadata": {},
   "source": [
    "## Least Squares as Maximum Likelihood\n",
    "- Least squares minimizing the mean square error is **equal** to maximum likelihood estimation when the likelihood is assigned to be **Gaussian**\n",
    "\n",
    "\n",
    "- Assume the model is $\\hat{y}=f(\\mathbf{x};\\boldsymbol{\\theta})$ with the dataset $$\\mathbf{X}=\\begin{bmatrix}\\mathbf{x}^{(1)} & \\mathbf{x}^{(2)} & \\cdots & \\mathbf{x}^{(m)}\\end{bmatrix},\\mathbf{y}=\\begin{bmatrix}y^{(1)} & y^{(2)} & \\cdots & y^{(m)}\\end{bmatrix}$$ The solution for $\\boldsymbol{\\theta}$ via least squares would be $$\\boldsymbol{\\theta}_{\\text{LS}}=\\underset{\\boldsymbol{\\theta}}{\\text{arg min}}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2$$\n",
    "\n",
    "\n",
    "- From the point of view of maximum likelihood estimation, think of the model as producing a conditional distribution $p_{\\text{model}}\\left(y\\left|\\ \\mathbf{x},\\boldsymbol{\\theta}\\right.\\right)$ instead of producing a single prediction $\\hat{y}=f(\\mathbf{x};\\boldsymbol{\\theta})$. Assign this likelihood of a single measurement to be Gaussian $$p_{\\text{model}}\\left(y^{(i)}\\left|\\ \\mathbf{x}^{(i)},\\boldsymbol{\\theta},\\sigma\\right.\\right)=\\mathcal{N}\\left(y^{(i)}\\left|f(\\mathbf{x};\\boldsymbol{\\theta}),\\sigma^2\\right.\\right)$$ where $\\sigma$ models the **noise**. This correspond to the belief that the measurement is around the model prediction $f(\\mathbf{x};\\boldsymbol{\\theta})$ but it is contained with Gaussian noise of variance $\\sigma^2$. For all the data, we have $$\\begin{aligned}p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{X},\\boldsymbol{\\theta},\\sigma\\right.\\right)&=\\mathcal{N}\\left(\\mathbf{y}\\left|f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right),\\sigma^2\\mathbf{I}_m\\right.\\right)\\\\&=(2\\pi)^{-m/2}\\sigma^{-m}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\right)\\end{aligned}$$ Then we have the maximum likelihood estimation to be $$\\begin{aligned}\\boldsymbol{\\theta}_{\\text{ML}}&=\\underset{\\boldsymbol{\\theta}}{\\text{arg min}}\\left[-\\text{log}\\left[p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{X},\\boldsymbol{\\theta},\\sigma\\right.\\right)\\right]\\right]\\\\&=\\underset{\\boldsymbol{\\theta}}{\\text{arg min}}\\left[\\frac{m}{2}\\text{log}\\left(2\\pi\\right)+m\\text{log}(\\sigma)+\\frac{1}{2\\sigma^2}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\right]\\\\&=\\underset{\\boldsymbol{\\theta}}{\\text{arg min}}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\\\&=\\boldsymbol{\\theta}_{\\text{LS}}\\end{aligned}$$ Maximizing the likelihood with respect to $\\boldsymbol{\\theta}$ yields the same estimate as minimizing the squared error.\n",
    "\n",
    "\n",
    "- The two criteria have **different values** but the **same location of the optimum**, which justifies the use of the LS as a maximum likelihood estimation procedure.\n",
    "\n",
    "\n",
    "- Notice that $\\sigma$ is also a parameter to be optimized, maximize the likelihood with respect to $\\sigma$ $$\\begin{aligned}\\sigma_{\\text{ML}}&=\\underset{\\sigma}{\\text{arg min}}\\left[-\\text{log}\\left[p_{\\text{model}}\\left(\\mathbf{y}\\left|\\ \\mathbf{X},\\boldsymbol{\\theta},\\sigma\\right.\\right)\\right]\\right]\\\\&=\\underset{\\sigma}{\\text{arg min}}\\left[\\frac{m}{2}\\text{log}\\left(2\\pi\\right)+m\\text{log}(\\sigma)+\\frac{1}{2\\sigma^2}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\right]\\\\&=\\underset{\\sigma}{\\text{arg min}}\\left[m\\text{log}(\\sigma)+\\frac{1}{2\\sigma^2}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\right]\\end{aligned}$$ It can be easily solved by setting the derivative with respect to $\\sigma$ to zero $$\\begin{aligned}m\\frac{1}{\\sigma_{\\text{ML}}}-\\frac{1}{\\sigma_{\\text{ML}}^3}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2&=0\\\\m\\sigma_{\\text{ML}}^2-\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2&=0\\\\ \\sigma_{\\text{ML}}^2&=\\frac{1}{m}\\left\\|\\mathbf{y}-f\\left(\\mathbf{X};\\boldsymbol{\\theta}\\right)\\right\\|^2_2\\end{aligned}$$\n",
    "\n",
    "\n",
    "- With the maximum likelihood estimation $\\boldsymbol{\\theta}_{\\text{ML}},\\sigma_{\\text{ML}}$, we can make **predictions** about $y$ at a new point $\\mathbf{x}$ $$p\\left(y\\left|\\mathbf{x},\\boldsymbol{\\theta}_{\\text{ML}},\\sigma_{\\text{ML}}\\right.\\right)=\\mathcal{N}\\left(y\\left|f(\\mathbf{x};\\boldsymbol{\\theta}_{\\text{ML}}),\\sigma^2_{\\text{ML}}\\right.\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96ef07",
   "metadata": {},
   "source": [
    "## Properties of Maximum Likelihood\n",
    "- Maximum likelihood estimator can be shown to be the **best** estimator asymptotically as the number of examples $m\\rightarrow\\infty$, in terms of its rate of convergence as $m$ increases\n",
    "\n",
    "\n",
    "- Under appropriate conditions, the maximum likelihood estimator has the property of **consistency**\n",
    "    - The true distribution $p_{\\text{data}}$ must lie within the model family $p_{\\text{model}}$\n",
    "    - The true distribution $p_{\\text{data}}$ must correspond to exactly one value of $\\boldsymbol{\\theta}$\n",
    "\n",
    "\n",
    "- The **statistical efficiency**, meaning that one consistent estimator may obtain lower generalization error for a fixed number of samples $m$, of the maximum likelihood estimator is very high among consistent estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9bc75",
   "metadata": {},
   "source": [
    "%%\\latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c4813",
   "metadata": {},
   "source": [
    "## Example: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ec862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
