{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c8fd50",
   "metadata": {},
   "source": [
    "**Information:** *A brief review of basic concepts of Probability, Statistics and Information related theory which is often used in Machine Learning and Deep Learning*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:** *05.29.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0425562",
   "metadata": {},
   "source": [
    "# Basic concepts\n",
    "## Introduction\n",
    "### Definition: \n",
    "- Probability Theory is a mathematical frame work for representing uncertain statements\n",
    "    - Provides a means of quantifying uncertainty as well as axioms for deriving new uncertainty statements\n",
    "    - Allows making uncertain statements and reasoning in the presence of uncertainty\n",
    "\n",
    "### Motivation: \n",
    "- Machine learning must always deal with uncertain quantities and sometimes stochastic (non-deterministic) quantities resulting from:\n",
    "    - Inherent stochasticity in the system being modeled\n",
    "    - Incomplete observability\n",
    "    - Incomplete modeling\n",
    "- It's usually more practical to use a *simple but uncertain rule* rather than a complex but certain one\n",
    "    - \"Most birds fly\" versus \"Birds fly, except for ...\"\n",
    "- Uncertainty can be modeled by probability\n",
    "\n",
    "### Interpretation of probability\n",
    "- **Frequentist probability**\n",
    "    - Probability theory was originally developed to analyze the *frequencies of events*, which are often *repeatable*\n",
    "    - When we say that an outcome has a probability $p$ of occurring, it means that if we repeated the experiment infinitely times, then a proportion $p$ of the repetitions would result in that outcome\n",
    "    - Not seemingly applicable to propositions that are not repeatable\n",
    "- **Bayesian probability**\n",
    "    - Use probability to represent a *degree of belief* for an outcome to occur, with 1 indicating absolute certainty that the outcome occurs and 0 indicating absolute certainty that the outcome doesn't occur\n",
    "    - For properties that we expect common sense reasoning about uncertainty to have, we need to treat Bayesian probabilities as behaving exactly the same as frequentist probabilities.\n",
    "- **The extension logic to deal with uncertainty**\n",
    "    - Probability can be seen as the extension of logic to deal with uncertainty. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913f8af",
   "metadata": {},
   "source": [
    "## Probability Space\n",
    "- A *probability space* is defined by the triple $\\left(\\Omega,\\mathcal{F},P\\right)$, where:\n",
    "    - $\\Omega$ is the *space of possible outcomes* (or outcome space)\n",
    "    - $\\mathcal{F}\\subseteq2^{\\Omega}$ is the *space of (measurable) events* (or event space)\n",
    "    - $P$ is the *probability measure* (or *probability distribution*) that maps an event $E\\in \\mathcal{F}$ to a real value between $0$ and $1$ (think of $P$ as a function)\n",
    "- Given an outcome space $\\Omega$, there is some restrictions as to what subset of $2^{\\Omega}$ can be considered an event space $\\mathcal{F}$:\n",
    "    - The trivial event $\\Omega$ and the empty event $\\mathbb{0}$ is in $\\mathcal{F}$\n",
    "    - The event space $\\mathcal{F}$ is closed under (countable) union\n",
    "        - if $\\alpha,\\beta\\in\\mathcal{F}$, then $\\alpha\\cup\\beta\\in\\mathcal{F}$\n",
    "    - The event space $\\mathcal{F}$ is closed under complement\n",
    "        - if $\\alpha\\in\\mathcal{F}$, then $(\\Omega \\setminus \\alpha)\\in\\mathcal{F}$\n",
    "- Notice: Event space is not always simply the power set of the outcome space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840babb",
   "metadata": {},
   "source": [
    "## Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993031e5",
   "metadata": {},
   "source": [
    "### Definition\n",
    "- A *random variable* is a function maps outcomes in the outcome space $\\Omega$ to real values\n",
    "    - **Not random nor a variable** \n",
    "- May be discrete or continuous\n",
    "    - Discrete: When the image of a random variable is countable, the random variable is called a *discrete random variable*\n",
    "    - Continuous: When the image of random variable is uncountably infinite (usually an interval), then the random variable is called a *continuous random variable*\n",
    "    - [Image](https://en.wikipedia.org/wiki/Image_(mathematics)):In mathematics, the image of a function is the set of all output values it may produce.\n",
    "\n",
    "### Notation\n",
    "- Upper case letters to represent random variables\n",
    "    - $X,Y,Z,...$\n",
    "- Lower case letters to represent the values of random variables\n",
    "    - $x,y,z,...$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3cef2",
   "metadata": {},
   "source": [
    "## Probability Distributions\n",
    "- A **probability distribution** is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way to describe probability distribution depends on whether the variables are discrete or continuous.\n",
    "\n",
    "### Discrete Variables and Probability Mass Functions\n",
    "- **Probability Mass Function (PMF)**: A function that gives the probability a discrete random variable is exactly equal to some value\n",
    "    - Called probability mass function as it divides up a unit mass (the total probability) and places them on different values a random variable can take\n",
    "    - Sometimes called *discrete density function*\n",
    "    - The primary means of defining a discrete probability distribution\n",
    "- **Notations for PMF**: \n",
    "    - Let $X$ be a discrete random variable, the probability mass function of $X$ is:\n",
    "$$P(X=x)=\\mathrm{Probability}\\  \\mathrm{that}\\  \\mathrm{the}\\  \\mathrm{random}\\  \\mathrm{variable}\\ X\\ \\mathrm{takes}\\  \\mathrm{the}\\  \\mathrm{value}\\ x$$\n",
    "        - Can be written in $p_X(x)$ or even directly $p(x)$ when there is no ambiguity\n",
    "    - Sometimes we define a variable first, then use $\\sim$ notation to specify which distribution it follows later: $X\\sim P(X)$\n",
    "- **Properties of PMF**: Given that $X$ is a *discrete random variable*, then:\n",
    "    - The domain of $p$ must be the set of all possible states of $X$\n",
    "    - $\\forall x\\in X,0\\le p(x)\\le 1.$ An impossible event has probability $0$, and no state can be less probable than that. Likewise, an event that is guaranteed to happen has probability $1$, and no state can have a greater chance of occurring\n",
    "    - $\\underset{x}{\\Sigma}p(x)=1$.Usually refer to this property as being *normalized*.\n",
    "    - Probability of $X$ taking either the value $x_1$ or the value $x_2$ (assuming $x_1\\ne x_2$) is:$$P(X=x_1\\ \\mathrm{or}\\ X=x_2)\\equiv P(X\\in\\{x_1,x_2\\})=P(X=x_1)+P(X=x_2)=p(x_1)+p(x_2)$$\n",
    "        - More generally, the probability that the random variable $X$ takes any value in a set $A$ is given by $P(X\\in A)=\\underset{x\\in A}{\\Sigma}p(x)$ \n",
    "    - Let $g(x)$ be a function and $Y=g(X)$ be a new random variable, then the PMF would be: $$P(y)=P(Y=y)=\\underset{x\\in g^{-1}(y)}{\\Sigma}p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588c66a",
   "metadata": {},
   "source": [
    "### Continuous Variables and Probability Density Functions\n",
    "- **Cumulative Distribution Function(CDF)**:A function that gives the probability that a random variable will take a value less than some value\n",
    "    - The random variable can be either discrete or continuous\n",
    "    - Sometimes called *distribution function*\n",
    "- **Notations for CDF**\n",
    "    - Let $X$ be a random variable, then its cumulative distribution function is denoted by:$$F(x)=P(X\\le x)$$\n",
    "- **Properties of CDF**: Given that $X$ is a random variable, then:\n",
    "    - The domain of $F$ must be the set of all possible states of $X$\n",
    "    - $F(x)$ is an increasing function for $x$\n",
    "    - $F(-\\infty)=\\underset{x\\rightarrow -\\infty}{\\mathrm{lim}}F(x)=0$ \n",
    "    - $F(+\\infty)=\\underset{x\\rightarrow - \\infty}{\\mathrm{lim}}F(x)=1$\n",
    "    - $P(a\\le X \\le b)=F(b)-F(a)$\n",
    "\n",
    "- **Probability Density Function (PDF)**: A function whose value at any given sample in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a *relative likelihood* that the value of the random variable would equal that sample\n",
    "    - Definition only for continuous random variables\n",
    "    - While the *absolute likelihood* for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer how much more likely it is that the random variable would equal one sample compared to the other sample.\n",
    "- **Notations for PDF**\n",
    "    - Let $X$ be a continuous random variable, then its probability density function is denoted by:$$p(x)\\simeq\\frac{P(x\\le X \\le x+\\Delta x)}{\\Delta x}$$ for some small $\\Delta x$ according to definition\n",
    "    - Observation:$$\\begin{aligned}p(x)&=\\underset{\\Delta x\\rightarrow 0}{\\mathrm{lim}}\\frac{P(x\\le X \\le x+\\Delta x)}{\\Delta x}\\\\&=\\underset{\\Delta x\\rightarrow 0}{\\mathrm{lim}}\\frac{F(x+\\Delta x)-F(x)}{\\Delta x}\\\\&=F'(x)\\\\&=\\frac{\\mathrm{d}F(x)}{\\mathrm{d}x}\\end{aligned}$$\n",
    "- **Properties of PDF**: Given that $X$ is a *continuous random variable*, then:\n",
    "    - The domain of $p$ must be the set of all possible states of $X$\n",
    "    - $\\forall x \\in X,p(x)\\ge 0$. Note that $p(x)\\le 1$ is not required\n",
    "    - $\\int p(x)dx=1$\n",
    "    - $\\int^b_a p(x)dx=F(b)-F(a)=P(a\\le X \\le b)$\n",
    "    - For any [borel](https://en.wikipedia.org/wiki/Borel_set) subset $A$ of the real numbers:$$P(X\\in A)=\\int_Ap(x)dx$$. This property holds even for random vectors \n",
    "    - Let $g(x)$ be a function and $Y=g(X)$ be a new continuous random variable, then the PDF of $Y$ would be:$$p(y)=p(x=g^{-1}(y))|\\frac{d}{dy}(g^{-1}(y))|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee0fa0",
   "metadata": {},
   "source": [
    "## Expectations and Variance\n",
    "### Expectations\n",
    "- **Definition**: The **expectation**, or **expected value**, of a random variable is denoted by $\\mathbb{E}(X)$\n",
    "    - For a discrete random variable $X$: $$\\mathbb{E}[X]=\\underset{x}{\\Sigma}xp(x)$$ where $P(x)$ is the probability mass function and the sum is over all possible values\n",
    "    - For a continuous random variable $X$: $$\\mathbb{E}[X]=\\int_{x} xp(x)dx$$ where $p(x)$ is the probability density function and the integral is over all possible values\n",
    "    - Can think of the expectation as the value of the random variable that one should \"expect\" to get, but notice that the expectation of a random variable is usually not in the possible values of it.\n",
    "- **Properties**: Let $X$ be a random variable, then:\n",
    "    - Let $g(x)$ be a function:\n",
    "        - For a discrete random variable $X$: $$\\mathbb{E}[g(X)]=\\underset{x}{\\Sigma}g(x)p(x)$$\n",
    "        - For a continuous random variable $X$: $$\\mathbb{E}[g(X)]=\\int_xg(x)p(x)dx$$\n",
    "    - Take any constant $c$: $$\\mathbb{E}[X+c]=\\mathbb{E}[X]+c$$\n",
    "    - Take any constant $\\lambda$: $$\\mathbb{E}[\\lambda X]=\\lambda\\mathbb{E}[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8f938",
   "metadata": {},
   "source": [
    "### Variance\n",
    "- **Definition**: The **variance** measures how much the value of a function of a random variable vary as sampling different values from its probability distribution\n",
    "    - For a discrete/continuous random variable $X$:$$\\mathbb{V}[X]=\\mathbb{E}\\left[\\left(X-\\mathbb{E}[X]\\right)^2\\right]$$\n",
    "    - Can think of the variance as the spread of the random variable around its expectation\n",
    "    - Square root of the variance is known as the **standard deviation**\n",
    "- **Properties**:\n",
    "    - $\\mathbb{V}[X]=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$\n",
    "    - Take any constant $c$: $$\\mathbb{V}[X+c]=\\mathbb{V}[X]$$\n",
    "    - Take any constant $\\lambda$: $$\\mathbb{V}[\\lambda X]=\\lambda^2\\mathbb{V}[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7b290",
   "metadata": {},
   "source": [
    "## Some Important Distributions\n",
    "### Bernoulli\n",
    "- The *Bernoulli Distribution* is one of the most basic distribution. It is a distribution over a single binary random variable and is controlled by a single parameter $\\theta\\in[0,1]$, which gives the probability of the random variable being equal to $1$.\n",
    "- For a *discrete random variable* $X$ following a Bernoulli distribution with parameter $\\theta$, the notation is: $$X\\sim \\mathrm{Bernoulli}(\\theta)$$\n",
    "- Properties:\n",
    "    - $P(X=1)=\\theta$\n",
    "    - $P(X=0)=1-\\theta$\n",
    "    - $P(X=x)=\\theta^{x}(1-\\theta)^{1-x}$ where $x=0$ or $x=1$\n",
    "    - $\\mathbb{E}[X]=\\theta$\n",
    "    - $\\mathbb{V}[X]=\\theta(1-\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f144f2",
   "metadata": {},
   "source": [
    "### Categorical Distribution\n",
    "- The *Categorical Distribution*, also called *Multinoulli Distribution*, is a distribution over a single discrete random variable with $k$ different states, where $k$ is finite. It is parameterized by a vector $\\mathbf{p}\\in[0,1]^{k-1}$, where $p_i$ gives the probability of the $i-th$ state. The final, $k$-th state's probability is given by $1-\\mathbf{1}^T\\mathbf{p}$, where $\\mathbf{1}^T\\mathbf{p}$ should be constrained to less than or equal to $1$.\n",
    "- For a *discrete random variable* $X$ following a Categorical distribution with parameter $\\mathbf{p}$. the notation is:$$X\\sim \\mathrm{Categorical}(p_1,p_2,\\cdots,p_{k-1},1-\\mathbf{1}^T\\mathbf{p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef8c17",
   "metadata": {},
   "source": [
    "### Continuous Uniform Distribution\n",
    "- The *Continuous Uniform Distribution*, also called *rectangular distribution*, models a random variable the takes values within an interval all with equal probability. The interval is defined by the parameters $a$ and $b$, which are the minimum and maximum values. The interval can either be *closed* (i.e. $[a,b]$) or *open* (e.g. $(a,b)$)\n",
    "- For a *continuous random variable* $X$ following a Uniform distribution in the interval $[a,b]$, the notation is:$$X\\sim U([a,b])$$\n",
    "- Probability density function:\n",
    "    - $p(x)=\\left\\{\\begin{aligned}&c &x\\in[a,b]\\\\&0 &\\mathrm{otherwise} \\end{aligned}\\right.$\n",
    "    - $\\int^b_ap(x)dx=1\\Rightarrow c=\\frac{1}{b-a}$\n",
    "- Cumulative distribution function:\n",
    "    - $F(x)=\\left\\{\\begin{aligned}&0 &\\mathrm{if}\\ x<a\\\\ &\\frac{x-a}{b-a} &\\mathrm{if}\\ a\\le x\\le b\\\\ &1 &\\mathrm{otherwise}\\end{aligned}\\right.$\n",
    "- Expectation:\n",
    "    - $\\mathbb{E}[X]=\\frac{a+b}{2}$\n",
    "- Variation:\n",
    "    - $\\mathbb{V}[X]=\\frac{(b-a)^2}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1334785",
   "metadata": {},
   "source": [
    "### Gaussian Distribution\n",
    "- The *Gaussian Distribution*, also know as the *normal distribution*, is the most commonly used distribution over real numbers. It models a random variable that take values from any real value. The distribution is controlled by two parameters $\\mu\\in\\mathbb{R}$ and $\\sigma\\in(0,\\infty)$. The values are concentrated around $\\mu$ and the variance $\\sigma^2$ determines how spread out the function values are around the mean.\n",
    "- For a *continuous random variable* following a Gaussian distribution with expectation $\\mu$ and variance $\\sigma^2$, the notation is:$$X\\sim \\mathcal{N}(\\mu,\\sigma^2)$$ \n",
    "- Probability density function:\n",
    "    - $p(x)=\\mathcal{N}(x|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{exp}\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right)$\n",
    "    - When we need to frequently evaluate the PDF with different parameter values, a more efficient way of parameterizing the distribution is to use a parameter $\\beta\\in(0,\\infty)$ to control the **precision**, or inverse variance, of the distribution\n",
    "        - $\\beta=\\frac{1}{\\sigma^2}$\n",
    "        - $p(x)=\\mathcal{N}(x|\\mu,\\beta^{-1})=\\sqrt{\\frac{\\beta}{2\\pi}}\\mathrm{exp}\\left(-\\frac{1}{2}\\beta(x-\\mu)^2\\right)$\n",
    "- Cumulative distribution function is not *analytically* available:\n",
    "    - $F(x)=P(X\\le x)=\\int^x_{-\\infty}\\mathcal{N}(\\bar{x}|\\mu,\\sigma^2)d\\bar{x}$\n",
    "- Expectation:\n",
    "    - $\\mathbb{E}[X]=\\mu$\n",
    "- Variation:\n",
    "    - $\\mathbb{V}[X]=\\sigma^2$\n",
    "- Remark:\n",
    "    - $P(\\mu-2\\sigma\\le x\\le \\mu+2\\sigma)\\approx 95.45\\%$\n",
    "    - $P(\\mu-3\\sigma\\le x\\le \\mu+3\\sigma)\\approx 99.73\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2752c43",
   "metadata": {},
   "source": [
    "### Standard Normal Distribution\n",
    "- The *Standard Normal Distribution* is the simplest case of a *Gaussian Distribution* where $\\mu=0$ and $\\sigma=1$, denoted as:$$Z\\sim\\mathcal{N}(0,1)$$\n",
    "- Probability density function:\n",
    "    - $\\phi(z)=\\mathcal{N}(z|0,1)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{exp}\\left\\{-\\frac{z^2}{2}\\right\\}$\n",
    "- Cumulative distribution function is also not analytically available:\n",
    "    - $\\Phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\int^z_{-\\infty}\\mathrm{exp}\\left\\{-\\frac{t^2}{2}\\right\\}dt$\n",
    "- Connections between the normal and the standard normal:\n",
    "    - Take a stander normal $Z\\sim\\mathcal{N}(0,1)$ and two numbers $\\mu$ and $\\sigma^2$, make the random variable $X=\\mu+\\sigma Z$, then $x\\sim \\mathcal{N}(\\mu,\\sigma^2)$: $$\\mathcal{N}(\\mu,\\sigma^2)=\\mu+\\sigma Z$$\n",
    "    - Take a normal $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, make the random variable $Z=\\frac{X-\\mu}{\\sigma}$, then $Z\\sim \\mathcal{N}(0,1)$\n",
    "    - Take a normal $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, then $$P(X\\le x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41dfc2",
   "metadata": {},
   "source": [
    "### Exponential and Laplace Distributions\n",
    "- In the context of deep learning, a probability distribution with a sharp point at $x=0$ is often wanted. **Exponential Distribution** can help to accomplish this.\n",
    "$$p(x|\\lambda)=\\lambda\\mathbf{1}_{x\\ge 0}\\mathrm{exp}(-\\lambda x)$$\n",
    "    - The indicator function $\\mathbf{1}_{x\\ge 0}$ is used to assign probability zero to all negative values of $x$\n",
    "- **Laplace distribution** allows us to place a sharp peak of probability mass at an arbitrary point $\\mu$\n",
    "$$\\mathrm{Laplace}(x|\\mu,\\gamma)=\\frac{1}{2\\gamma}\\mathrm{exp}\\left(-\\frac{|x-\\mu|}{\\gamma}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde8f76",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "- The *Poisson Distribution* is a very useful distribution that deal with the arrival of events. It measures probability of the number of events happening over a fixed period of time, given a fixed average rate of occurrence, and that the events take place independently of the time since the last event. It is parameterized by the average arrival rate $\\lambda$.$$P(X=k)=\\frac{\\mathrm{exp}(-\\lambda)\\lambda^{k}}{k!}$$\n",
    "- Expectation:\n",
    "    - $\\mathbb{E}[X]=\\lambda$\n",
    "- Variation:\n",
    "    - $\\mathbb{V}[X]=\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e535f",
   "metadata": {},
   "source": [
    "## Collections of Random Variables\n",
    "### Joint Distributions, Marginal Distributions and Conditional Distributions\n",
    "- **Joint Distributions**: Distributions over multiple random variables\n",
    "    - For discrete random variables $X$ and $Y$, the **joint probability mass function** of the pair $(X,Y)$ is the function $p(x,y)$ giving the probability that $X=x$ and $Y=y$ is denoted as:$$p(x,y)=P(X=x,Y=y)$$\n",
    "        - It's non-negative: $p(x,y)\\ge 0$\n",
    "        - Sum over all the possible values of all random variables is 1: $\\underset{x}{\\Sigma}\\underset{y}{\\Sigma}p(x,y)=1$\n",
    "        - *Sum Rule*: Marginalizing over the values of one of the random variables gets the PMF of the other: $\\underset{y}{\\Sigma}p(x,y)=p(x),\\underset{x}{\\Sigma}p(x,y)=p(y)$. This result is called the **Marginal Distribution**.\n",
    "    - For continuous random variables $X$ and $Y$, the **joint probability density function** of the pair $(X,Y)$ is the function $p(x,y)$ giving the probability that $X=x$ and $Y=y$:$$p(x,y)=\\underset{\\Delta x,\\Delta y\\rightarrow 0}{\\mathrm{lim}}\\frac{P(x\\le X\\le x+\\Delta x,y\\le Y \\le y+\\Delta y)}{\\Delta x\\Delta y}$$\n",
    "        - It's non-negative: $p(x,y)\\ge 0$\n",
    "        - Sum over all the possible values of all random variables is 1: $\\int\\int p(x,y)dxdy=1$\n",
    "        - *Sum Rule*: Marginalizing over the values of one of the random variables gets the PDF of the other:$\\int p(x,y)dy=p(x)$ and $\\int p(x,y)dx=p(y)$. The result is called the **Marginal Distribution**\n",
    "- **Conditional Distributions**: In many cases, the probability of some event given that some other event has happened is wanted. This called a **conditional probability**\n",
    "    - For random variables $X$ and $Y$, if $X=x$ is observed and the state of knowledge about $Y$ is need to be updated, then the conditional PDF is:$$P(Y=y|X=x)=p(y|x)=\\frac{P(X=x,Y=y)}{P(X=x)}=\\frac{p(x,y)}{p(x)}$$\n",
    "        - The conditional probability is only defined when $P(X=x)>0$. The conditional probability conditioned on an event that never happens is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8b505",
   "metadata": {},
   "source": [
    "### Chain Rule and Bayes Rule\n",
    "- **Chain Rule** / Product Rule of Conditional Probabilities $$P(x^{(1)},\\cdots,x^{(n)})=P(x^{(1)})\\underset{i=2}{\\overset{n}{\\Pi}}P(x^{i}|x^{(1)},\\cdots,x^{(i-1)})$$\n",
    "    - Often used to evaluate the joint probability of some random variables and is especially useful when there are (conditional) independence across variables\n",
    "    - Picking the right order to unravel the random variables can often make evaluating the probability much easier\n",
    "    - For example: $$\\begin{aligned}p(a,b,c)&=p(a|b,c)p(b,c)\\\\p(b,c)&=p(b|c)p(c)\\\\p(a,b,c)&=p(a|b,c)p(b|c)p(c)\\end{aligned}$$\n",
    "- **Bayes Rule** $$P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}$$\n",
    "    - If $P(Y)$ is not given, we can always use the equation of calculating marginal probabilities first\n",
    "    - For example: $$\\begin{aligned}P(X,Y|Z)&=\\frac{P(Z|X,Y)P(X,Y)}{P(Z)}=\\frac{P(Y,Z|X)P(X)}{P(Z)}\\\\ P(X|Y,Z)&=\\frac{P(Y|X,Z)P(X,Z)}{P(Y,Z)}=\\frac{P(Y|X,Z)P(X|Z)P(Z)}{P(Y|Z)P(Z)}=\\frac{P(Y|X,Z)P(X|Z)}{P(Y|Z)}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09aa50",
   "metadata": {},
   "source": [
    "### Expectations\n",
    "- Take two random variables $X$ and $Y$ with joint PDF $p(x,y)$, the expectation of their sum is $$\\mathbb{E}[X+Y]=\\mathbb{E}[X]+\\mathbb{E}[Y]$$\n",
    "- **Jenson's Inequality**\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd773ebc",
   "metadata": {},
   "source": [
    "### Correlated Random Variables\n",
    "- **Covariance**: A measure of the jointly variability of two random variables\n",
    "    - Gives some sense of how much two values are linearly related to each other\n",
    "    - For two jointly distributed random variables $X$ and $Y$, the covariance is defined as:$$\\mathbb{C}(X,Y)=\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]$$\n",
    "    - Correlation is not causation\n",
    "    - Two jointly distributed random variables $X$ and $Y$ is called:\n",
    "        - positively correlated if $\\mathbb{C}(X,Y)>0$\n",
    "        - negatively correlated if $\\mathbb{C}(X,Y)<0$\n",
    "        - uncorrelated if $\\mathbb{C}(X,Y)\\approx 0$\n",
    "- **Properties of the covariance**:\n",
    "    - Let $X$ be a random variable: $$\\mathbb{C}[X,X]=\\mathbb{V}[X]$$\n",
    "    - Let $X$ be a random variable and take any constant $\\lambda$: $$\\mathbb{C}[X,\\lambda]=0$$\n",
    "    - Let $X$ and $Y$ be two random variables: $$\\mathbb{C}[Y,X]=\\mathbb{C}[X,Y]$$\n",
    "    - Let $X$ and $Y$ be two random variables and take any constants: $$\\mathbb{C}[\\lambda X,\\mu Y]=\\lambda\\mu\\mathbb{C}[X,Y]$$\n",
    "    - Let $X$ and $Y$ be two random variables and take any constants: $$\\mathbb{C}[X+\\lambda,Y+\\mu]=\\mathbb{C}[X,Y]$$\n",
    "    - Let $X$, $Y$ and $Z$ be three random variables: $$\\mathbb{C}[X,Y+Z]=\\mathbb{C}[X,Y]+\\mathbb{C}[X,Z]$$\n",
    "    - Let $X$ and $Y$ be two random variable: $$\\mathbb{V}[X+Y]=\\mathbb{V}[X]+\\mathbb{V}[Y]+2\\mathbb{C}[X,Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ceeb2",
   "metadata": {},
   "source": [
    "### Independent Random Variables\n",
    "- **Definition**: Two random variables $X$ and $Y$ are independent conditional on $I$, denoted as $X\\perp Y|I$ if and only if conditioning on one does not tell anything about the other $$p(x|y,I)=p(x|I)$$\n",
    "    - When there is no ambiguity, $I$ can be dropped\n",
    "- **Properties**: Assume $X$ and $Y$ are two independent random variables, then:\n",
    "    - $p(x,y)=p(x)p(y)$\n",
    "    - $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$\n",
    "    - $\\mathbb{C}[X,Y]=0$\n",
    "        - The reverse is not true. Uncorrelated variables do not have to be independent\n",
    "    - $\\mathbb{V}[X+Y]=\\mathbb{V}[X]+\\mathbb{V}[Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd16e1",
   "metadata": {},
   "source": [
    "## Random Vectors\n",
    "### Definition\n",
    "- A **list of random variables**, sometimes called **a multivariate random variable**\n",
    "- Let $X_1,X_2,\\cdots,X_N$ be $N$ random variables, then $\\mathbf{X}=\\left[X_1,X_2,\\cdots,X_N\\right]^T$ is called a **random vector**\n",
    "- The **joint probability density function** is denoted as $$p(\\mathbf{x})=p(x_1,\\cdots,x_N)$$ where $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix}$\n",
    "    - Properties:\n",
    "        - $\\forall \\mathbf{x},p(\\mathbf{x})\\ge 0$\n",
    "        - $\\int p(\\mathbf{x})dx_1dx_2\\cdots dx_N=1$\n",
    "        - $p(x_i)=\\int p(\\mathbf{x})dx_1\\cdots dx_{i-1}dx_{i+1}\\cdots dx_N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df15adb",
   "metadata": {},
   "source": [
    "### Expectation and Covariance Matrix\n",
    "- **Expectation** of a random vector is the vector of expectation of *each component* $$\\mathbb{E}[\\mathbf{X}]=\\begin{bmatrix}\\mathbb{E}[X_1]\\\\ \\mathbb{E}[X_2]\\\\ \\vdots \\\\ \\mathbb{E}[X_N]\\end{bmatrix}$$\n",
    "- **Covariance Matrix**:\n",
    "    - Let $\\mathbf{X}$ be a $N$-dimensional random vector, $\\mathbf{Y}$ be an $M$-dimensional random vector, then the covariance of $\\mathbf{X}$ and $\\mathbf{Y}$ is the $N\\times M$ matrix consisting of all covariances between the components of $\\mathbf{X}$ and $\\mathbf{Y}$: $$\\mathbb{C}[\\mathbf{X},\\mathbf{Y}]=\\left[\\mathbb{C}[X_i,Y_j]\\right]_{i,j}$$\n",
    "    - We can easily show that: $$\\mathbb{C}[\\mathbf{X},\\mathbf{Y}]=\\mathbb{E}\\left[\\left(\\mathbf{X}-\\mathbb{E}\\left[\\mathbf{X}\\right]\\right)\\left(\\mathbf{Y}-\\mathbb{E}\\left[\\mathbf{Y}\\right]\\right)^T\\right]$$\n",
    "    - Self-covariance of $N$-dimensional random vector $\\mathbf{X}$ is a $N\\times N$ matrix: $$\\mathbb{C}[\\mathbf{X},\\mathbf{X}]=\\mathbb{E}\\left[\\left(\\mathbf{X}-\\mathbb{E}\\left[\\mathbf{X}\\right]\\right)\\left(\\mathbf{X}-\\mathbb{E}\\left[\\mathbf{X}\\right]\\right)^T\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceeb8d1",
   "metadata": {},
   "source": [
    "### Multivariate Normal - Diagonal Covariance Case\n",
    "- Take the special case of $N$ **independent** random variables $X_1,...,X_N$ each distributed according to a normal with known mean and variance $$X_i=\\mathcal{N}(\\mu_i,\\sigma_i^2)$$ Then $\\mathbf{X}=\\left[X_1,\\cdots,X_N\\right]^T$ is called a **multivariate normal**\n",
    "- Expectation:\n",
    "    - $\\mathbb{E}[\\mathbf{X}]=\\begin{bmatrix}\\mathbb{E}[X_1]\\\\ \\mathbb{E}[X_2]\\\\ \\vdots \\\\ \\mathbb{E}[X_N]\\end{bmatrix}=\\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_N\\end{bmatrix}=\\pmb{\\mu}$\n",
    "- Covariance Matrix:\n",
    "    - Notice that the random variables are independent to each other, which means $$\\mathbb{C}[X_i,X_j]=\\left\\{\\begin{aligned}&0&i\\ne j\\\\&\\sigma_i^2&i=j\\end{aligned}\\right.$$\n",
    "    - Therefore, the covariance matrix of these matrix is $$\\mathbb{C}[\\mathbf{X},\\mathbf{X}]=\\begin{bmatrix}\\sigma_1^2&\\ &\\\\ \\ &\\ddots&\\ \\\\ &\\ &\\sigma_N^2\\end{bmatrix}=\\mathrm{diag}(\\sigma_1^2,\\sigma_2^2,\\cdots,\\sigma_N^2)=\\pmb{\\Sigma}$$\n",
    "- For a multivariate normal with mean vector $\\pmb{\\mu}$ and covariance matrix $\\pmb{\\Sigma}$, the notation is $$\\mathbf{X}\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$$\n",
    "where $$\\pmb{\\mu}=\\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_N\\end{bmatrix},\\pmb{\\Sigma}=\\begin{bmatrix}\\sigma_1^2&\\ &\\\\ \\ &\\ddots&\\ \\\\ &\\ &\\sigma_N^2\\end{bmatrix}$$\n",
    "- Joint probability density function:$$\\mathcal{N}(\\mathbf{x}|\\pmb{\\mu},\\pmb{\\Sigma})=\\sqrt{\\frac{1}{(2\\pi)^n\\mathrm{det}(\\pmb{\\Sigma})}}\\mathrm{exp}\\left(-\\frac{1}{2}(\\mathbf{x}-\\pmb{\\mu})^T\\pmb{\\Sigma}^{-1}(\\mathbf{x}-\\pmb{\\mu})\\right)$$\n",
    "- Connection to the standard normal\n",
    "    - Let $\\mathbf{Z}\\sim \\mathcal{N}(0,\\mathbf{I})$ be a collection of independent standard normal random variables $$Z_i\\sim\\mathcal{N}(0,1)$$ Define the random vector $$\\mathbf{X}=\\pmb{\\mu}+\\pmb{\\Sigma}\\mathbf{Z}$$ Then $$\\mathbf{X}\\sim \\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aad51",
   "metadata": {},
   "source": [
    "### Multivariate Normal - Full Covariance Case\n",
    "- General case of $N$ random variables $X_1,...,X_N$ each distributed according to a normal with known mean and variance $$X_i=\\mathcal{N}(\\mu_i,\\sigma_i^2)$$ The random vector $\\mathbf{X}=\\left[X_1,\\cdots,X_N\\right]^T$ is denoted by $$\\mathbf{X}\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$$\n",
    "- **Restriction of the covariance matrix**\n",
    "    - In this general case, $\\pmb{\\Sigma}$ is no longer a diagonal matrix\n",
    "    - The covariance matrix has to be **positive definite**\n",
    "        - For any $\\mathbf{v}\\ne 0$, $\\mathbf{v}^T\\pmb{\\Sigma}\\mathbf{v}\\ge 0$\n",
    "        - This is so that $p(\\mathbf{x})$ has a global maximum\n",
    "        - Equivalently, $\\pmb{\\Sigma}$ must have positive eigenvalues\n",
    "- Connection to the standard normal\n",
    "    - Let $\\mathbf{Z}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ be a collection of independent standard normal random variables $$Z_i\\sim\\mathcal{N}(0,1)$$ Define the random vector $$\\mathbf{X}=\\pmb{\\mu}+\\mathbf{AZ}$$ Then $$\\mathbf{X}\\sim\\mathcal{N}(\\pmb{\\mu},\\mathbf{AA}^T)$$\n",
    "- **Marginalization**\n",
    "    - Let $\\mathbf{X}$ be a random vector made out of two random vectors $\\mathbf{X}_1$ and $\\mathbf{X}_2$:$$\\mathbf{X}=\\begin{bmatrix}\\mathbf{X}_1\\\\ \\mathbf{X}_2\\end{bmatrix}$$ Assume that$$\\mathbf{X}\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$$ Decompose mean and covariance in blocks: $$\\pmb{\\mu}=\\begin{bmatrix}\\pmb{\\mu}_1\\\\ \\pmb{\\mu}_2\\end{bmatrix},\\ \\pmb{\\Sigma}=\\begin{bmatrix}\\pmb{\\Sigma}_{1,1}&\\pmb{\\Sigma}_{1,2}\\\\ \\pmb{\\Sigma}_{1,2}^T&\\pmb{\\Sigma}_{2,2}\\end{bmatrix}$$ To find the probability density of $\\mathbf{X}_1$, marginalize $$p(\\mathbf{x}_1)=\\int p(\\mathbf{x}_1,\\mathbf{x}_2)d\\mathbf{x}_2=\\mathcal{N}(\\mathbf{x}_1|\\pmb{\\mu},\\pmb{\\Sigma}_{1,1})$$\n",
    "- **Conditioning**\n",
    "    - Let $\\mathbf{X}$ be a random vector made out of two random vectors $\\mathbf{X}_1$ and $\\mathbf{X}_2$:$$\\mathbf{X}=\\begin{bmatrix}\\mathbf{X}_1\\\\ \\mathbf{X}_2\\end{bmatrix}$$ Assume that $$\\mathbf{X}\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma}),\\ \\pmb{\\mu}=\\begin{bmatrix}\\pmb{\\mu}_1\\\\ \\pmb{\\mu}_2\\end{bmatrix},\\ \\pmb{\\Sigma}=\\begin{bmatrix}\\pmb{\\Sigma}_{1,1}&\\pmb{\\Sigma}_{1,2}\\\\ \\pmb{\\Sigma}_{1,2}^T&\\pmb{\\Sigma}_{2,2}\\end{bmatrix}$$ The conditional PDF is $$p(\\mathbf{x}_1|\\mathbf{x}_2)=\\frac{p(\\mathbf{x}_1,\\mathbf{x}_2)}{p(\\mathbf{x}_2)}\\propto p(\\mathbf{x}_1,\\mathbf{x}_2)=\\mathcal{N}(\\mathbf{x}_1|\\pmb{\\mu}_{1|\\mathbf{x}_2},\\pmb{\\Sigma}_{1,1|\\mathbf{x}_2})$$ Where $$\\pmb{\\mu}_{1|\\mathbf{x}_2}=\\pmb{\\mu}_1+\\pmb{\\Sigma}_{1,2}\\pmb{\\Sigma}_{2,2}^{-1}(\\mathbf{x}_2-\\pmb{\\mu}_2),\\ \\pmb{\\Sigma}_{1,1|\\mathbf{x}_2}=\\pmb{\\Sigma}_{1,1}-\\pmb{\\Sigma}_{1,2}\\pmb{\\Sigma}_{2,2}^{-1}\\pmb{\\Sigma}_{1,2}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddc9ee",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9710e22",
   "metadata": {},
   "source": [
    "# Sampling and Monte Carlo Method\n",
    "## Basic Sampling\n",
    "### Pseudo-random number generators\n",
    "- Computers are deterministic machines and they cannot generate completely random numbers\n",
    "- Pseudo-random number generator generates deterministic sequences of numbers that look random\n",
    "\n",
    "### Sample the uniform\n",
    "- Given\n",
    "    - PRNG's generate random integers from 0 to m\n",
    "- Procedure\n",
    "    - Sample a random integer $d$\n",
    "    - Set $x=\\frac{d}{m}$\n",
    "    - Calculate the **empirical CDF** ($\\hat{F}_N(x)$) with the ideal CDF $$\\hat{F}_N(x)=\\frac{\\mathrm{number \\ of\\ elements\\ in\\ sample\\le x}}{N}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33523de8",
   "metadata": {},
   "source": [
    "### Sample the categorical\n",
    "- To sample from a categorical distribution $$X\\sim \\mathrm{Categorical}(p_1,p_2,\\cdots,p_{k-1},1-\\mathbf{1}^T\\mathbf{p})$$\n",
    "    - Draw a uniform number $u\\sim U([0,1])$\n",
    "    - Find $j$ such that $$\\underset{k=0}{\\overset{j-1}{\\Sigma}}p_k\\le u\\lt\\underset{k=0}{\\overset{j}{\\Sigma}}p_k$$\n",
    "    - $j$-th state is the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11990449",
   "metadata": {},
   "source": [
    "### Inverse Sampling\n",
    "- Let $X$ be an arbitrary univariate continuous random variable with CDF $F(x)$\n",
    "    - Draw a uniform number $u\\sim U([0,1])$\n",
    "    - Set $$x=F^{-1}(u)$$ and get the sample\n",
    "- Proof\n",
    "    - Let $U$ be a uniform random variable $$U\\sim U([0,1])$$ For any CDF $F(x)$ define the random variable $$X=F^{-1}(U)$$ The the CDF OF $X$ is $$\\begin{aligned}P(X\\le x)&=P(F^{-1}(U)\\le x)\\\\&=P(F(F^{-1}(U))\\le F(x))\\\\&=P(U\\le F(x))\\\\&=F_U(F(x))\\\\&=F(x)\\end{aligned}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099b4a4",
   "metadata": {},
   "source": [
    "## The Monte Carlo Method for estimating expectations and variances\n",
    "### The uncertainty propagation problem\n",
    "- Given a random variable $X\\sim p(x)$ and a function $g(x)$, to quantify the uncertainty about the model output $Y=g(X)$\n",
    "    - $\\mathbb{E}[Y]=\\mathbb{E}[g(X)]=\\int g(x)p(x)$\n",
    "    - $\\mathbb{V}[Y]=\\int(g(x)-\\mathbb{E}[Y])^2p(x)dx=\\mathbb{E}[[g(x)]^2]-\\left(\\mathbb{E}[g(x)]\\right)^2$\n",
    "    - $P(Y\\ge y)=\\int \\mathbf{1}_{[y,\\infty]}(g(x))p(x)dx=\\mathbb{E}\\left[\\mathbf{1}_{[u,\\infty]}\\left(g(x)\\right)\\right]$\n",
    "- All the statistics are essentially expectations of functions of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3b299",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "- The number of samples needed to estimate an arbitrary function with a given level of accuracy grows **exponentially** with respect to the number of input variables (i.e. dimensionality) of the function\n",
    "    - Take a $d$-dimensional uniform $X\\sim U([0,1]^d)$ and a function $g(x)$, to estimate $$\\mathbb{E}[g(X)]=\\int g(x)p(x)dx$$ use $n$ equidistance points per dimension, which results in $n^d$ boxes each with volume $n^{-d}$.  Evaluate the integral with $$\\mathbb{E}[g(x)]\\approx n^{-d}\\underset{j=1}{\\overset{n^d}{\\Sigma}}g\\left(x_j\\right)$$ and assume it takes a millisecond to evaluate the function $g(x)$ and n = 10, it takes\n",
    "        - 0.1 seconds when $d=1$\n",
    "        - 1 second when $d=2$\n",
    "        - 100 seconds when $d=3$\n",
    "        - 1000 seconds when $d=6$\n",
    "        - 115 days when $d=10$\n",
    "        - 3.17 billion years when $d=20$\n",
    "- **Blessing of dimensionality**\n",
    "    - Surprisingly and despite the expected \"curse of dimensionality\" difficulties, common-sense heuristics based on the most straightforward methods \"can yield results which are almost surely optimal\" for high-dimensional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a9ce0",
   "metadata": {},
   "source": [
    "### The law of large numbers\n",
    "- Take an infinite series of independent random variables $X_1,X_2,\\cdots$ with the same distribution (it doesn't matter what distribution), the sample average $$\\bar{X}_N=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}X_i\\rightarrow \\mu \\ \\mathrm{a.s.}$$ where $\\mathrm{a.s.}$ means \"almost surely\" and $\\mu=\\mathbb{E}[X_i]$ (as $N\\rightarrow \\infty$)\n",
    "- Take a random variable $X\\sim p(x)$ and some function $g(x)$, to estimate the **expectation**: $$I=\\mathbb{E}[g(X)]=\\int g(x)p(x)dx$$ Make independent identical copies of $X$:$$X_1,X_2,\\cdots\\sim p(x)$$ Consider the independent identical distribution: $$Y_1=g(X_1),Y_2=g(X_2),\\cdots$$ Then by the strong law of large numbers: $$I_N=\\frac{Y_1+Y_2+\\cdots+Y_N}{N}\\rightarrow \\mathbb{E}[Y_i]=\\mathbb{E}[g(X_i)]=I \\ \\mathrm{a.s.}$$\n",
    "- Take a random variable $X\\sim p(x)$ and some function $g(x)$, to estimate the **variance**: $$V=\\mathbb{V}[g(x)]=\\mathbb{E}\\left[\\left(g(X)-\\mathbb{E}[g(X)]\\right)^2\\right]=\\mathbb{E}\\left[\\left(g(X)-I\\right)^2\\right]=\\mathbb{E}\\left[g(x)^2\\right]-I^2$$ Take independent identical copies of $X$:$$X_1,X_2,\\cdots\\sim p(x)$$ Estimate the mean using a sample average: $$\\bar{I}_N=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}g(X_i)$$ Estimate the variance by: $$\\bar{V}_N=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}g^2(X_i)-\\bar{I}_N^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d7b31",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimates of various statistics\n",
    "### Estimating the cumulative distribution function\n",
    "- Take a random variable $X\\sim p(x)$ and some function $g(x)$, consider the derived random variable $Y=g(X)$, we would like to estimate the **cumulative distribution function**: $$F(y)=P(Y\\le y)=P(g(X)\\le y)$$\n",
    "- Consider the indicator function of a set $A$: $$1_{A}(y)=\\left\\{\\begin{aligned}&1 &y \\ \\mathrm{in}\\  A\\\\ &0&\\mathrm{otherwise}\\end{aligned}\\right.$$ Use it, we can write $F(y)$ as an expectation: $$F(y)=\\mathbb{E}\\left[1_{(-\\infty,y]}\\left(g(X)\\right)\\right]$$\n",
    "- Take $X_1,X_2,\\cdots$ independent identical copies of $X$\n",
    "- Estimate the CDF using a sample average: $$\\bar{F}_N(y)=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}1_{(-\\infty,y]}\\left(g(X_i)\\right)=\\frac{\\mathrm{number \\ of\\ }g(X_i)\\le y}{N}$$\n",
    "- This estimate is called the **empirical CDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45f5c9",
   "metadata": {},
   "source": [
    "### Estimating the probability density function via histograms\n",
    "- Take a random variable $X\\sim p(x)$ and some function $g(x)$, consider the derived random variable $Y=g(X)$, we would like to estimate the **probability density function** $p(y)$ of $Y$\n",
    "- Take $M$ small bins $[b_0,b_1],\\cdots,[b_{M-1},b_M]$ in the $y$ space\n",
    "- Approximate $p(y)$ with a constant inside each bin: $$\\bar{p}_M(y)=\\underset{j=1}{\\overset{M}{\\Sigma}}c_j1_{[b_{j-1},b_j]}(y)$$ The constants $c_j$ are: $$c_j=P(b_{j-1}\\le Y\\le b_j)=F(b_j)-F(b_j-1)$$\n",
    "- So, we can approximate the constants $c_j$ with the empirical CDF: $$\\bar{c}_{j,N}=\\bar{F}_N(b_j)-\\bar{F}_N(b_{j-1})=\\frac{\\mathrm{number \\ of \\ samples\\ that\\ fall\\ in\\ bin\\ }[b_{j-1},b_j]}{N}\\rightarrow c_j \\ \\mathrm{a.s.}$$\n",
    "- Putting everything together the approximation becomes: $$\\bar{p}_{M,N}(y)=\\underset{j=1}{\\overset{M}{\\Sigma}}\\bar{c}_{j,N}1_{[b_{j-1},b_j]}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f40205",
   "metadata": {},
   "source": [
    "### Estimating Predictive Quantiles\n",
    "- **Predictive Quantiles**:\n",
    "    - Take a random variable $X\\sim P(X)$ and some function $g(x)$, let $Y=g(X)$ and $F(y)$ be the CDF of $Y$, then the $q$-predictive quantile of $Y$ is the value $\\mu_q$ such that:$$P(Y\\le\\mu_q)=F_Y(\\mu_q)=\\frac{q}{100}$$\n",
    "        - $F(\\mu_{50})=50\\%$ is called *median*\n",
    "- To find the $q$-predictive quantile of $Y$:$$F_Y(\\mu_q)=\\frac{q}{100}$$ Turn it into a root finding problem: $$F(\\mu_q)-\\frac{q}{100}=0$$ Take $X_1,X_2,\\cdots$ independent identical copies of $X$, fins the sampling estimate of the CDF of $Y=g(X)$ denoted by $$\\bar{F}_N(y)$$ Then numerically solve the root finding problem and obtain the quantile $\\bar{\\mu}_{q,N}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b3bb4",
   "metadata": {},
   "source": [
    "## Quantifying uncertainties in Monte Carlo Estimates\n",
    "### The central limit theorem\n",
    "- *The central limit theorem* states that if you have a population with mean $\\mu$ and standard deviation $\\sigma$ and take sufficiently large random samples from the population with replacement , then the distribution of the sample means will be approximately normally distributed.\n",
    "- Take $X_1,X_2,\\cdots$ to be independent and identically distributed (i.i.d.) random variables with mean $\\mu$ and variance $\\sigma^2$, consider their average $$S_N=\\frac{X_1+\\cdots+X_N}{N}$$ The Central Limit Theorem States that: $$S_N\\sim\\mathcal{N}(\\mu,\\frac{\\sigma^2}{N})\\ \\mathrm{for\\ large\\ }N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e85a77",
   "metadata": {},
   "source": [
    "### Epistemic uncertainty of Monte Carlo Estimates\n",
    "- Use sampling to estimate:$$I=\\mathbb{E}[g(X)]$$ Take i.i.d. random variables: $$X_1,X_2,\\cdots$$ Consider the also i.i.d. random variables: $$Y_1=g(X_1),Y_2=g(X_2),\\cdots$$ Use the *law of large numbers*:$$\\bar{I}_N=\\frac{g(X_1)+\\cdots+g(X_N)}{N}=\\frac{Y_1+\\cdots+Y_N}{N}\\rightarrow I,\\ \\mathrm{a.s.}$$ Note that $Y_i=g(X_i)$ are i.i.d. random variables with mean:$$\\mathbb{E}[Y_i]=\\mathbb{E}[g(X_i)]=I$$ Assume their variance is finite:$$\\mathbb{V}[Y_i]=\\sigma^2<+\\infty$$ Then the *central limit theorem* holds and it gives:$$\\bar{I}_N=\\frac{Y_1+\\cdots+Y_N}{N}\\sim\\mathcal{N}\\left(I,\\frac{\\sigma^2}{N}\\right)\\ \\mathrm{for\\ large}\\ N$$ Rewrite as:$$\\bar{I}_N=I+\\frac{\\sigma}{\\sqrt{N}}\\cdot Z,\\ Z\\sim\\mathcal{N}(0,1)\\Rightarrow I=\\bar{I}_N-\\frac{\\sigma}{\\sqrt{N}}\\cdot Z$$ That is to say:$$I\\sim\\mathcal{N}\\left(\\bar{I}_N,\\frac{\\sigma^2}{N}\\right)$$Similarly, estimate the variance by: $$\\bar{V}_N=\\bar{\\sigma}_N^2=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}g^2(X_i)-\\bar{I}_N^2$$ And end up with $$I\\sim\\mathcal{N}\\left(\\bar{I}_N,\\frac{\\bar{\\sigma}^2_N}{N}\\right)\\ \\mathrm{for\\ large\\ }N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d553d",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a97df",
   "metadata": {},
   "source": [
    "# Basics of Information Theory\n",
    "## Information Entropy\n",
    "- In information theory, we would like to quantify information in a way that formalizes the intuition that *learning that an unlikely event has occurred is more informative than learning that a likely event has occurred*:\n",
    "    - Likely events should have low information content, and in the extreme case, events that a guaranteed to happen should have no information content whatsoever\n",
    "     - Less likely events should have higher information content\n",
    "     - Independent events should have additive information\n",
    "        - e.g. Finding out that a tossed coin has come up as head twice should convey twice as much information as finding out that a tossed coin has come up as heads once\n",
    "- To satisfy those properties, **self-information** of an event $X=x$ is defined to be $$I(x)=-\\mathrm{log}P(X=x)$$ As the base of logarithm is $e$, $I(x)$ is written in units of **nats**. One nat is the amount of information gained by observing an event of probability $\\frac{1}{e}$. When the base is $2$, then the unit is called **bits** or **shannons**.\n",
    "- Self-information deals only with a single outcome. In 1948, **Claude Shannon** came up with a function called **Information Entropy** quantifying the amount of uncertainty in an entire probability distribution: $$\\mathbb{H}(X)=\\mathbb{E}_{X\\sim P(X)}[I(x)]=-\\mathbb{E}_{X\\sim P(X)}[\\mathrm{log}p(x)]$$ Where $X$ is a **discrete random variable**. In other words, the Information Entropy of a distribution is the expected amount of information in an event drawn from that distribution\n",
    "    - Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy.\n",
    "    - Distributions that are closer to uniform have high entropy.\n",
    "- When $X$ is a **continuous random variable**, the naive extension of information entropy is called **[differential entropy](https://en.wikipedia.org/wiki/Differential_entropy)** and lose some abilities it has in discrete form. Usually we do not consider using Shannon Entropy directly for continuous random variables as it actually fails to quantify \"uncertainty\" over the whole distribution.\n",
    "    - Is no longer invariant under change of variables\n",
    "    - Can be negative\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9c2d0",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence\n",
    "- Given two separate probability distributions $P(X)$ and $Q(X)$ over the same random variable $X$, we can measure how different there two distributions are using the *Kullback-Leibler divergence*: $$D_{KL}(P\\|Q)=\\mathbb{E}_{X\\sim P(X)}\\left[\\mathrm{log}\\frac{P(x)}{Q(x)}\\right]=\\mathbb{E}_{X\\sim P(X)}[\\mathrm{log}P(x)-\\mathrm{log}Q(x)]$$\n",
    "- Can be viewed as the information loss when replacing $Q(X)$ with $P(X)$\n",
    "- Formally speaking, in the case of **discrete variables**, it is the extra amount of information needed to send a message containing symbols drawn from probability distribution $P(X)$ when we use a code that was designed to minimize the length of messages drawn from probability distribution $Q(X)$ \n",
    "- **Important Properties**:\n",
    "     - Being non-negative $D_{KL}(P\\|Q)\\ge 0$\n",
    "     - $D_{KL}(P\\|Q)=0$ is and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal \"almost every where\" in the case of continuous variables\n",
    "     - Although often used to measuring some sort of distance between distances, it is not a true distance $$D_{KL}(P\\|Q)\\ne D_{KL}(Q\\|P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a950f",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "- Given two separate probability distributions $P(X)$ and $Q(X)$ over the same random variable $X$, we can measure the average number of bits (or nats, depend on the base of logarithm) to identify an event drawn from the set if a coding scheme used for an estimated probability distribution $Q(X)$ rather than the true distribution $P(X)$:$$\\mathbb{H}(P,Q)=\\mathbb{H}(P)+D_{KL}(P\\|Q)=-\\mathbb{E}_{X\\sim P(X)}\\mathrm{log}Q(x)$$\n",
    "- Notice that $Q(X)$ does not participate in the $\\mathbb{H}(P)$ term, minimizing the cross-entropy with respect to $Q$ (keep $P$ unchanged) is **equivalent** to minimizing the KL divergence. Obviously, calculating the cross-entropy is much more convenient when the equivalence exists.\n",
    "- **Important Properties**:\n",
    "    - Like KL divergence, cross entropy is not symmetric $$\\mathbb{H}(P,Q)\\ne \\mathbb{H}(Q,P)$$\n",
    "    - Cross-entropy to a distribution itself is its information entropy $$\\mathbb{H}(P,P)=\\mathbb{H}(P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a34f6",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9b106",
   "metadata": {},
   "source": [
    "# Bayes Inference\n",
    "## Formal descriptions of Bayesian inference\n",
    "### Notations\n",
    "- $X$: A random variable or vector.\n",
    "- $\\theta$: The parameter of the random variable's distribution $X\\sim P(X|\\theta)$, could be a vector.\n",
    "- $\\alpha$: The *hyperparameters* of the parameter distribution $\\theta\\sim P(\\theta|\\alpha)$, could be a vector.\n",
    "- $\\mathbf{x}$: Sample, a set of observations of the random variable $\\mathbf{x}=\\{x_1,x_2,\\cdots,x_N\\}$\n",
    "- $\\tilde{x}$: A new observation whose distributions is to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63046cdc",
   "metadata": {},
   "source": [
    "### Bayesian inference\n",
    "- **Prior Distribution**:\n",
    "    - A *prior probability distribution*, often simply called the *prior*, of an uncertainty is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.\n",
    "    - In this case, it is the *distribution of the parameter* $\\theta$ before any observations is made: $$\\mathbf{Prior}=p(\\theta|\\alpha)$$\n",
    "- **Likelihood**:\n",
    "    - The *distribution of the observations* conditional on its parameters.$$\\mathbf{Likelihood}=\\mathrm{L}(\\theta|\\mathbf{x})=p(\\mathbf{x}|\\theta)$$ \n",
    "    - It measures the **goodness of fit** of a statistical model $X\\sim P(X|\\theta)$ to a sample of data $\\mathbf{x}=\\{x_1,x_2,\\cdots,x_N\\}$ for given *values* of the unknown parameters $\\theta\\sim P(\\theta|\\alpha)$\n",
    "    - When the measurements are *independent*: $$p(\\mathbf{x}|\\theta)=\\underset{i=1}{\\overset{N}{\\Pi}}p(x_i|\\theta)$$\n",
    "- **Marginal likelihood** \n",
    "    - The distribution of the observations marginalized over the parameters $$p(\\mathbf{x}|\\alpha)=\\int p(\\mathbf{x}|\\theta)p(\\theta|\\alpha)d\\theta$$\n",
    "    - Sometimes called *evidence*\n",
    "- **Posterior Probability**\n",
    "    - The *distribution of the parameters* after taking into the observations, which is determined by *Bayes' rule* $$\\begin{aligned}p(\\theta|\\mathbf{x},\\alpha)&=\\frac{p(\\theta,\\mathbf{x},\\alpha)}{p(\\mathbf{x},\\alpha)}\\\\&=\\frac{p(\\mathbf{x}|\\theta,\\alpha)p(\\theta,\\alpha)}{p(\\mathbf{x}|\\alpha)}\\\\&=\\frac{p(\\mathbf{x}|\\theta,\\alpha)p(\\theta|\\alpha)}{p(\\mathbf{x}|\\alpha)}\\end{aligned}$$ That is to say $$\\mathbf{Posterior}=p(\\theta|\\mathbf{x},\\alpha)=\\frac{p(\\mathbf{x}|\\theta,\\alpha)p(\\theta|\\alpha)}{p(\\mathbf{x}|\\alpha)}\\propto p(\\mathbf{x}|\\theta,\\alpha)p(\\theta|\\alpha)$$\n",
    "    - This is expressed in words as *\"posterior is proportional to likelihood times prior\" * $$\\mathbf{Posterior}\\propto\\mathbf{Likelihood}\\times\\mathbf{Prior}$$\n",
    "    - In practice, for almost all complex Bayesian models used in machine learning, the posterior distribution $p(\\theta|\\mathbf{x},\\alpha)$ is **not** obtained in a closed form distribution, mainly because the parameter space for $\\theta$ can be very high, or the Bayesian model retains certain hierarchical structure formulated from the observations $\\mathbf{x}$ and parameter $\\theta$. In such situations, approximation techniques are needed\n",
    "- **Credible Intervals**\n",
    "    - An interval within which an unobserved parameter $\\theta$ falls with a particular probability and it is in the domain of a posterior probability distribution\n",
    "        - For example, a $95\\%$ credible interval is an interval $[l,u] $where $$\\begin{aligned}p(l\\le \\theta\\le u|\\mathbf{x})&=0.95\\\\p(\\theta\\le l|\\mathbf{x})&=0.025\\\\p(\\theta\\le u|\\mathbf{x})&=0.975 \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea19ff6",
   "metadata": {},
   "source": [
    "### Bayesian Prediction\n",
    "- **Posterior Predictive Distribution**:\n",
    "    - The distribution of a new observation marginalized over the posterior: $$p(\\tilde{x}|\\mathbf{x},\\alpha)=\\int p(\\tilde{x}|\\theta)p(\\theta|\\mathbf{x},\\alpha)d\\theta$$\n",
    "- **Prior Predictive Distribution**:\n",
    "    - The distribution of a new observation marginalized over the prior: $$p(\\tilde{x}|\\alpha)=\\int p(\\tilde{x}|\\theta)p(\\theta|\\alpha)d\\theta$$\n",
    "- Remark:\n",
    "    - In Bayesian Prediction, instead of a fixed value as a prediction, a **distribution** over possible values is returned, in which way the entire posterior distribution of the parameters is used.\n",
    "    - By comparison, prediction in frequentist statistics often tries to find an optimum point estimate of the parameters (e.g. by maximum likelihood or maximum a posteriori estimation(MAP)) and then plugging this estimate into the formula for the distribution of the prediction. This leads to the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the prediction distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030762e",
   "metadata": {},
   "source": [
    "### Decision-Making Problem\n",
    "- To decide the best suitable parameter $\\theta$ given a set of observations $\\mathbf{x}$, the cost of making a mistake needs to be quantified and then a decision minimizes the cost should be made\n",
    "- Denote the **loss** when we guess $\\hat{\\theta}$ and the true value is $\\theta$ as $l(\\hat{\\theta},\\theta)$, then $$\\mathbb{E}\\left[l(\\hat{\\theta},\\theta)|\\mathbf{x}\\right]=\\int l(\\hat{\\theta},\\theta)p(\\theta|\\mathbf{x})d\\theta$$ The best decision $\\theta^*$ should be $$\\theta^*=\\mathrm{arg}\\underset{\\hat{\\theta}}{\\mathrm{min}}\\mathbb{E}\\left[l(\\hat{\\theta},\\theta)|\\mathbf{x}\\right]$$\n",
    "    - The $0-1$ loss: $$l_{01}(\\hat{\\theta},\\theta)=\\left\\{\\begin{aligned}&0&\\hat{\\theta}=\\theta\\\\&1&\\hat{\\theta}\\ne\\theta\\end{aligned}\\right.$$ Then $$\\theta^*=\\mathrm{arg}\\underset{\\theta}{\\mathrm{max}}p(\\theta|\\mathbf{x})$$\n",
    "    - The square loss ($L^2$ norm): $$l_2(\\hat{\\theta},\\theta)=(\\hat{\\theta}-\\theta)^2$$ Then $$\\theta^*=\\mathbb{E}[\\theta|\\mathbf{x}]=\\int\\theta p(\\theta|\\mathbf{x})d\\theta$$ \n",
    "    - The Absolute loss ($L^1$ norm):$$l_2(\\hat{\\theta},\\theta)=|\\hat{\\theta}-\\theta|$$ Then $$\\theta^*=\\mathrm{median\\ of\\ the\\ posterior},p(\\theta\\le\\theta^*|\\mathbf{x})=0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4719a",
   "metadata": {},
   "source": [
    "### Posterior Predictive checking\n",
    "- **Replicated data**\n",
    "    - Assume that we have built model using data $\\mathbf{x}$, then the replicated data $\\mathbf{x}^{rep}$ is given by $$\\begin{aligned}\\theta_s|\\mathbf{x}&\\sim p(\\theta|\\mathbf{x})\\\\ \\mathbf{x}^{rep}|\\theta=\\theta_s&\\sim p(\\mathbf{x}^{rep}|\\theta=\\theta_s)\\end{aligned}$$\n",
    "    - The idea is to sample $\\mathbf{x}^{rep}$ and compare their characteristics to the observed data $\\mathbf{x}$\n",
    "- **Test quantiles**\n",
    "     - We use test quantiles to characterize the discrepancy between the model and the data.\n",
    "     - Helps to zoom into the characteristics that are of particular interest\n",
    "     - Depends on *what important characteristics of the data the model is supposed to be capturing*\n",
    "     - Mathematically, a quantile is a scalar function of the data and the parameters $$T(\\mathbf{x},\\theta)$$\n",
    "     - The idea is to compare the histogram of the test quantity over replicated data (of $p(T(\\mathbf{x}^{rep},\\theta)|\\mathbf{x})$) with the observe value $T(\\mathbf{x},\\theta)$\n",
    "- **Bayesian $p$-values**\n",
    "    - Check the model by evaluating the probability that the replicated data give a test quantity that is more extreme than the observed value. The probability is known as the posterior (or Bayesian) $p$-value and it is defined by:$$p_B = P(T(x^{\\text{rep}}_{1:n},\\theta) > T(x_{1:n},\\theta) | x_{1:n})$$\n",
    "    - **Not** the probability that the model is correct\n",
    "    - Values close to $0$ or $1$ indicate some issue with the model with regard to that particular quantity\n",
    "    - Values close to $0.5$ indicate no issue, but they do not mean that the model is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16fabd",
   "metadata": {},
   "source": [
    "## Selecting Prior Information\n",
    "- Three widely accepted ways to select prior information following a systematic theoretical framework:\n",
    "    - The principle of insufficient reason\n",
    "    - The principle of maximum entropy\n",
    "    - The principle of transformation groups\n",
    "    \n",
    "### The principle of insufficient reason\n",
    "- The *principle of insufficient reason*, also called *principle of indifference* is a rule for assigning epistemic probabilities. The principle of insufficient reason states that in the absence of any relevant evidence, agents should distribute their *degrees of belief* equally among all the possible outcomes under consideration\n",
    "- Let $X$ be a discrete random variable taking $N$ different values $x_1,\\cdots,x_N$, if that is all we know, then the principle of insufficient reason states that we should assign:$$P(X=x_1)=P(X=x_2)=\\cdots=P(X=x_N)=\\frac{1}{N}$$\n",
    "- Generalizations of the principle of insufficient reason\n",
    "    - Principle of transformation groups (advanced)\n",
    "    - Principle of maximum entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a085cb",
   "metadata": {},
   "source": [
    "### The principle of maximum entropy\n",
    "- The uniform distribution required by the *principle of insufficient reason* is the most \"uncertain\" distribution. Mention that the *information entropy* is a measurement of uncertainty in an entire distribution, the idea is to assign the distribution that has the maximum uncertainty while being consistent with the data\n",
    "- For **discrete variables**:\n",
    "    - Definition of **testable information**: $$\\mathbb{E}[f(x)]=F$$ Where $f(x)$ is a known function and $F$ is a known value\n",
    "    - Let $X$ be a discrete random variable and $\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_N\\end{bmatrix}$ be a set of observations while some *testable information* is available, the principle of maximum entropy states that we should assign to $X$ the probability distribution that maximizes the entropy subject to the constraints imposed by the testable information\n",
    "    - The mathematical representation for a discrete random variable with $N$ different values is $$\\mathrm{arg}\\underset{P(X)}{\\mathrm{max}}\\mathbb{H}(P(X))=\\mathrm{arg}\\underset{P(X)}{\\mathrm{max}}\\left\\{-\\underset{i}{\\overset{N}{\\Sigma}}p(x_i)\\mathrm{log}p(x_i)\\right\\}$$ Subject to $$\\mathbb{E}\\left[f_k(x)\\right]=F_k,\\ k=1,2,\\cdots,K$$ Where $K$ is the number of constraints and $$\\underset{i=1}{\\overset{N}{\\Sigma}}p(x_i)=1$$ \n",
    "    - The general solution to maximum entropy problem is $$P(X=x_i)=\\frac{1}{z}\\mathrm{exp}\\left\\{\\underset{k=1}{\\overset{K}{\\Sigma}}\\lambda_kf_k(x_i)\\right\\}$$ Where $$z=\\underset{i=1}{\\overset{N}{\\Sigma}}\\mathrm{exp}\\left\\{\\underset{k=1}{\\overset{K}{\\Sigma}}\\lambda_kf_k(x_i)\\right\\}$$ And $\\lambda_k$ needs to be determined according to $$F_k=\\frac{\\partial z}{\\partial \\lambda_k}$$\n",
    "    - Examples:\n",
    "        - $X$ takes $N$ different values $\\Rightarrow P(X=x_i)=\\frac{1}{N}$\n",
    "        - $X$ takes two values $0$ and $1$, $\\mathbb{E}[X]=\\theta\\ \\Rightarrow X\\sim\\mathrm{Bernoulli}(\\theta)$\n",
    "        - $X$ takes values $0,1,2,\\cdots,N$, $\\mathbb{E}[X]=\\mu$ and $X$ is the number of successful trials in $N$ potentially correlated sequential experiments $\\Rightarrow X\\sim \\mathrm{B}(N,\\frac{\\mu}{N})$ ([Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution))\n",
    "        - $X$ takes values $0,1,2,\\cdots$, $\\mathbb{E}[X]=\\mu$ and $X$ is the number of successful trials in an infinite number of potentially correlated sequential experiments $\\Rightarrow X\\sim \\mathrm{Poisson}(\\mu)$\n",
    "- For **continuous variables**:\n",
    "    - Notice that naive extension of information entropy to continuous distributions is no longer invariant under change to variables. The Shannon Entropy is only defined for discrete spaces and only show the abilities to quantify \"uncertainty\" in discrete spaces\n",
    "    - *Edwin Jaynes* proposed the following formula $$\\mathbb{H}_c=-\\int p(x)\\mathrm{log}\\frac{p(x)}{q(x)}dx$$ where $q(x)$, called the \"invariant measure\", is proportional to the limiting density of discrete points.\n",
    "    - Then the mathematical representation for a continuous random variable is $$\\mathrm{arg}\\underset{P(X)}{\\mathrm{max}}\\mathbb{H}_c(P(X))=\\mathrm{arg}\\underset{P(X)}{\\mathrm{max}}-\\int p(x)\\mathrm{log}\\frac{p(x)}{q(x)}dx$$ Subject to $$\\mathbb{E}\\left[f_k(x)\\right]=F_k,\\ k=1,2,\\cdots,K$$ Where $K$ is the number of constraints and $$\\underset{i=1}{\\overset{N}{\\Sigma}}p(x_i)=1$$ \n",
    "    - The general solution to maximum entropy problem is $$P(X=x_i)=\\frac{q(x)}{z}\\mathrm{exp}\\left\\{\\underset{k=1}{\\overset{K}{\\Sigma}}\\lambda_kf_k(x_i)\\right\\}$$ Where $$z=\\int q(x)\\mathrm{exp}\\left\\{\\underset{k=1}{\\overset{K}{\\Sigma}}\\lambda_kf_k(x_i)\\right\\}dx$$ And $\\lambda_k$ needs to be determined according to $$F_k=\\frac{\\partial z}{\\partial \\lambda_k}$$\n",
    "    - Mathematical theory for finding the maximal uncertainty density $q(x)$\n",
    "        - Principle of transformation groups\n",
    "        - Theory of Haar measures\n",
    "    - Examples:\n",
    "        - $X$ takes values in $[a,b]$,$q(x)=1\\ \\Rightarrow X\\sim U([a,b])$\n",
    "        - $X$ takes values in $\\mathbb{R}$, $q(x)=1$, $\\mathbb{E}[X]=\\mu$, $\\mathbb{V}[X]=\\sigma^2\\ \\Rightarrow X\\sim \\mathcal{N}(\\mu,\\sigma^2)$\n",
    "- For an almost list of a commonly used maximum entropy distributions, see [Maximum entropy probability distribution entry of wikipedia](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
