{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbda3f5f",
   "metadata": {},
   "source": [
    "**Information:** *Brief introduction to gradient descent, how gradient descent is supported in pytorch, convex functions, and some numerical considerations to kept in mind*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:** *05.27.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10525d",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "## Motivation\n",
    "- Most ML/DL algorithms involve **optimization** of some sort. \n",
    "    - Optimization refers to the task of either **minimizing** or maximizing some function $f(\\mathbf{x})$ by altering $\\mathbf{x}$\n",
    "    - Usually phrase most optimization problems in terms of minimizing $f(\\mathbf{x})$\n",
    "    - Maximization may be accomplished via s minimization algorithm by minimizing $-f(\\mathbf{x})$\n",
    "- Usually the function we want to minimize is called the **objective function**, or **criterion**. In ML/DL contexts, the name **loss function** is often used.\n",
    "    - As mentioned in introduction, a loss function quantifies the *distance* between the **real** and **predicted** value of the target.\n",
    "    - Usually be a non-negative number where smaller values are better and perfect predictions incur a loss of $0$\n",
    "    - Usually denoted as $L(\\boldsymbol{\\theta})$ where $\\boldsymbol{\\theta}$ is usually the parameter of ML/DL models \n",
    "- Usually denote the value that minimizes a function with a superscript $*$\n",
    "    - $\\boldsymbol{\\theta}^*=\\text{arg}\\underset{\\boldsymbol{\\theta}}{\\text{min}}L(\\boldsymbol{\\theta})$\n",
    "- Most ML/DL algorithms are so complex that it is difficult or impossible to find the closed form solution for the optimization problem\n",
    "    - Use numerical optimization method instead\n",
    "- One common algorithm is **gradient descent**, other optimization algorithms are\n",
    "    - Expectation Maximization\n",
    "    - Sampling-based optimization\n",
    "    - Greedy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a47ef",
   "metadata": {},
   "source": [
    "## Definition\n",
    "- **Definition**:\n",
    "    - A **first-order iterative** optimization algorithm for finding **local minimum** of a **differential** function.\n",
    "        - The idea is to take **repeated steps** in the opposite direction of the **gradient** of the function at the current point, because this is the direction of steepest descent.\n",
    "        - As it calculates the **first-order** derivative, it requires the objective function to be **differential**\n",
    "        - Converge when first-order derivative is zero, which only ensures reaching **local minimum** for general functions\n",
    "- **Theory**:\n",
    "    - Based on the observation that if the multi-variable function $F(\\mathbf{x})$ is defined and differentiable in a neighborhood of a point $\\mathbf{a}$, then $F(\\mathbf{x})$ decreases **fastest** if one goes from $\\mathbf{a}$ in the direction of the negative gradient of $F$ at $\\mathbf{a}$, which is $-\\nabla F(\\mathbf{a})$. It follows that if $$\\mathbf{a}_{n+1}=\\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n)$$ for a $\\gamma\\in\\mathbb{R}_+$ small enough, then $$F(\\mathbf{a}_n)\\ge F(\\mathbf{a}_{n+1})$$\n",
    "- Simple form of **vanilla gradient descent** (GD):\n",
    "    1. Start at random parameter $\\boldsymbol{\\theta}$\n",
    "    2. Repeat until converged\n",
    "        - $\\mathbf{d}\\leftarrow-\\nabla L(\\boldsymbol{\\theta})$\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\alpha\\mathbf{d}^T$\n",
    "    - $\\alpha$ is called **learning rate** or **step size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cc8df",
   "metadata": {},
   "source": [
    "## Select appropriate learning rate\n",
    "- Too large $\\alpha$ leads to instability and even divergence\n",
    "- Too small $\\alpha$ leads to slow convergence\n",
    "- **Steepest gradient descent** use line search to compute the best $\\alpha$\n",
    "    1. Start at random parameter $\\boldsymbol{\\theta}$\n",
    "    2. Repeat until converged\n",
    "        - $\\mathbf{d}\\leftarrow-\\nabla L(\\boldsymbol{\\theta})$\n",
    "        - $\\alpha^*\\leftarrow\\text{arg}\\underset{\\alpha}{\\text{min}}\\{L(\\boldsymbol{\\theta}+\\alpha\\mathbf{d}^T)\\}$\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\alpha^*\\mathbf{d}^T$\n",
    "- **Adaptive learning rates** may help, but not always\n",
    "    - $\\alpha=\\frac{1}{t}$, approaches 0 but can cover an infinite distance since $\\underset{a\\rightarrow\\infty}{\\text{lim}}\\underset{t=1}{\\overset{a}{\\Sigma}}\\frac{1}{t}=\\infty$\n",
    "- **Coordinate Descent** update one parameter at a time\n",
    "    - Removes problem of selecting step size\n",
    "    - Each update can be very fast, but lots of updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85690",
   "metadata": {},
   "source": [
    "## Slow convergence due to Poor Conditioning\n",
    "- **Conditioning** refers to how rapidly a function changes with respect to small changes in its inputs.\n",
    "- Consider the function $$f(x)=\\mathbf{A}^{-1}\\mathbf{x}$$ When $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ has an eigenvalue decomposition, its **condition number** is $$\\underset{i,j}{\\text{max}}\\left|\\frac{\\lambda_i}{\\lambda_j}\\right|$$ This is the ratio of the magnitude of the largest and smallest eigenvalue\n",
    "- A problem with a **low condition number** is said to be **well-conditioned**, while a problem with a high condition number is said to be ill-conditioned\n",
    "    - In non-mathematical terms, an ill-conditioned problem is one where, for a small change in the inputs there is a large change in the answer or dependent variable, which means the correct solution to the equation becomes hard to find\n",
    "    - Condition number is a property of the problem\n",
    "- **Gradient descent** is very sensitive to **condition number** of the problem\n",
    "    - No good choice of step size. Tiny change in one variable could lead to great change in dependent variable.\n",
    "- **Solutions:**\n",
    "    - **Newton's method:** Correct for local second derivative. (Sphere the ellipse)\n",
    "        - Too much computation and too difficult to implement\n",
    "    - **Alternative methods**:\n",
    "        - Preconditioning: Easy, but tends to be ad-hoc, not so robust\n",
    "        - Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef995704",
   "metadata": {},
   "source": [
    "## Compute Loss Gradient\n",
    "- Take the **mean square error** as an example: $$\\begin{aligned}\\nabla_{\\boldsymbol{\\theta}}L_{MSE}(\\boldsymbol{\\theta})&=\\nabla_{\\boldsymbol{\\theta}}\\left\\{\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}\\left\\|\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i)\\right\\|^2\\right\\}\\\\&=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}\\nabla_{\\boldsymbol{\\theta}}\\left\\{(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\right\\}\\end{aligned}$$ Use the chain rule and scale-by-vector matrix calculus identity that $$\\frac{\\partial \\mathbf{x}^T\\mathbf{x}}{\\partial \\mathbf{x}}=2\\mathbf{x}^T$$ We can get $$\\begin{aligned}\\nabla_{\\boldsymbol{\\theta}}L_{MSE}(\\boldsymbol{\\theta})&=\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\\\&=\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\\\&=-\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\end{aligned}$$\n",
    "- The result of the gradient usually includes three parts:\n",
    "    - Sum over training data. It consists of a lot of computations but the way of computation is relatively easy and straight forward\n",
    "    - Prediction error term such as $\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i)$ in MSE, which is usually easy to get\n",
    "    - Gradient of inference function $\\nabla_{\\boldsymbol{\\theta}}(f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))$, which is difficult to solve\n",
    "        - Enabled by automatic differentiation built into modern domain specific languages such as Pytorch, Tensorflow, ...\n",
    "        - For neural networks, this is known as **back propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb2106",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b666ed1",
   "metadata": {},
   "source": [
    "# Automatic Differentiation via Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d48f00",
   "metadata": {},
   "source": [
    "## Data Manipulation via Pytorch\n",
    "- The $n$-dimensional array is usually called the tensor\n",
    "- *tensor* class in Pytorch is similar to *NumPy*'s *ndarray* with several additional features\n",
    "    - GPU is well-supported to accelerate the computation whereas *NumPy* only supports CPU computation\n",
    "    - *tensor* class supports automatic differentiation\n",
    "- The [Pytorch documentation](https://pytorch.org/docs/master/torch.html) shows the full attributes\n",
    "\n",
    "### Create a tensor\n",
    "- To get started, import **torch**. Although it's called Pytorch, we should import **torch** instead of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6259cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check the version of a module\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b14069",
   "metadata": {},
   "source": [
    "- Common ways to creating a tensor\n",
    "    - torch.arange(start,end,step)\n",
    "    - torch.zeros(shape)\n",
    "    - torch.ones(shape)\n",
    "    - torch.randn(shape)\n",
    "    - torch.tensor(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d89ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a5abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df5fdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a71d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1833, -1.4853, -2.6226,  1.3522, -0.7521,  1.5946,  1.5680, -0.4013,\n",
       "        -1.8723, -1.3914])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81ac76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b91ff2",
   "metadata": {},
   "source": [
    "- One can access a tensor's shape by viewing the **shape** attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204807d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((4, 5)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a8afd",
   "metadata": {},
   "source": [
    "- **reshape** method can change the shape of a tensor without altering either the number of elements or their values.\n",
    "    - No need to manually specify every dimension\n",
    "    - Can place $-1$ for the dimension that we would like tensors to automatically infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2c4e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c0bcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086d1d8",
   "metadata": {},
   "source": [
    "### Type of a tensor\n",
    "- Usually, a tensor is created as **tensor.float32** (32-bit floating point) by default. One can view its type in the **dtype** attribute\n",
    "    - When creating a tensor using **torch.tensor**, tensor with all integers would be created as **torch.int64** (64-bit signed integer)\n",
    "- Full tensor types can be viewed in the [documentation](https://pytorch.org/docs/master/tensor_attributes.html#torch.torch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4116bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c769c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b81d2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only add one dot after the first element\n",
    "torch.tensor([[1., 2, 3], [4, 5, 6]]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147c6c0",
   "metadata": {},
   "source": [
    "- One can assign the wanted type when creating the tensor by setting the **dtype** attribute to\n",
    "    - *torch.float32*, 32-bit floating point\n",
    "    - *torch.float64*, 64-bit floating point\n",
    "    - *torch.uint8*, 8-bit unsigned integer\n",
    "    - *torch.int8*, 8-bit signed integer\n",
    "    - *torch.int32*, 32-bit signed integer\n",
    "    - *torch.int64*, 64-bit signed integer\n",
    "    - *torch.bool*, Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed180e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10, dtype=torch.float64).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fbd2cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10, dtype=torch.uint8).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b340cf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10, dtype=torch.int32).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e003f",
   "metadata": {},
   "source": [
    "- One can also construct the type of a tensor from list or numpy array using the method:\n",
    "    - *FloatTensor*, 32-bit floating point\n",
    "    - *DoubleTensor*, 64-bit floating point\n",
    "    - *ByteTensor*, 8-bit unsigned integer\n",
    "    - *CharTensor*, 8-bit signed integer\n",
    "    - *IntTensor*, 32-bit signed integer\n",
    "    - *LongTensor*, 64-bit signed integer\n",
    "    - *BoolTensor*, Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985d92aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.DoubleTensor([1, 2, 3]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2c0c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ByteTensor([1, 2, 3]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25840892",
   "metadata": {},
   "source": [
    "### Use GPU for tensor computation\n",
    "- Unless otherwise specified, a new tensor will be stored in main memory and designated for CPU-based computation\n",
    "- One can check which device the tensor is designated for by viewing the **device** attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7452789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926607cd",
   "metadata": {},
   "source": [
    "- One can always set the create a device if a GPU supporting cuda is available and use **to(device)** method to determine the device on which a tensor is or will be allocated\n",
    "    - Assign the **device** parameter when creating a tensor also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8591020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e73747b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5, device=cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da6418fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If available, indexing the cuda also works\n",
    "torch.ones(5, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca14f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If avaiable, the string also works\n",
    "torch.ones(5, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b9cbe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the to method the move a tensor\n",
    "x = torch.ones(5).to(cuda0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f1a5124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One can also move a tensor from GPU to CPU\n",
    "x.to(\"cpu\").device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b9ec2",
   "metadata": {},
   "source": [
    "### Operations\n",
    "- Common standard arithmetic operators have all been lifted to element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3cf31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3., 5., 9.]), tensor([1., 2., 4., 8.]), tensor([1., 2., 4., 8.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 4., 8.])\n",
    "c = 1.\n",
    "x + c, x * c, x**c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3f7680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 5., 6., 9.]),\n",
       " tensor([4., 6., 8., 8.]),\n",
       " tensor([ 1.,  8., 16.,  8.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 4., 8.])\n",
    "y = torch.tensor([4., 3., 2., 1.])\n",
    "x + y, x * y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eadd29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dc9f9",
   "metadata": {},
   "source": [
    "- Matrix multiplication is also supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "440c426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7055, -0.0886,  1.8231,  1.7564, -2.7292],\n",
       "         [-2.0987, -0.2093, -2.1729, -1.9152,  1.6844],\n",
       "         [ 0.1432,  0.4736,  0.6370, -0.4225, -2.1120],\n",
       "         [-0.5778,  0.4198, -0.0482, -1.1163, -1.9259]]),\n",
       " tensor([[ 1.7055, -0.0886,  1.8231,  1.7564, -2.7292],\n",
       "         [-2.0987, -0.2093, -2.1729, -1.9152,  1.6844],\n",
       "         [ 0.1432,  0.4736,  0.6370, -0.4225, -2.1120],\n",
       "         [-0.5778,  0.4198, -0.0482, -1.1163, -1.9259]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((4, 3))\n",
    "B = torch.randn((3, 5))\n",
    "# Two ways of matrix multiplication\n",
    "torch.mm(A, B), A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eacb01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0949, -1.5523, -0.2367,  0.4972],\n",
       "         [-0.4520,  0.1967, -0.0363,  0.3360],\n",
       "         [ 1.0668, -0.3951, -0.1567, -0.3507],\n",
       "         [ 0.3363,  0.1538, -0.0893,  0.0107],\n",
       "         [ 0.5939, -0.9762, -0.0580,  0.1045]]),\n",
       " tensor([[-0.0949, -1.5523, -0.2367,  0.4972],\n",
       "         [-0.4520,  0.1967, -0.0363,  0.3360],\n",
       "         [ 1.0668, -0.3951, -0.1567, -0.3507],\n",
       "         [ 0.3363,  0.1538, -0.0893,  0.0107],\n",
       "         [ 0.5939, -0.9762, -0.0580,  0.1045]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((5, 4))\n",
    "B = torch.randn((5, 4))\n",
    "# Two ways of elementwise multiplication\n",
    "torch.mul(A, B), A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c8c8c",
   "metadata": {},
   "source": [
    "- We can also **concatenate** multiple tensors together, stacking them end-to-end to form a larger tensor. We just need to provide a list of tensors and tell the system along which axis to concatenate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "093d0b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  1.,  1.,  1.,  1.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  1.,  1.,  1.],\n",
       "         [ 8.,  9., 10., 11.,  1.,  1.,  1.,  1.]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(0, 12, 1).reshape((3, 4))\n",
    "B = torch.ones((3, 4))\n",
    "# dim stands for the index of dimension in which the tensors are concatenated\n",
    "torch.cat((A, B), dim=0), torch.cat((A, B), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74edfa6",
   "metadata": {},
   "source": [
    "- Also, we can construct a binary tensor via logical statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "796ece79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False, False, False],\n",
       "         [False,  True, False, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([[False, False, False, False],\n",
       "         [False, False,  True,  True],\n",
       "         [ True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(0, 12, 1).reshape((3, 4))\n",
    "B = 5 * torch.ones((3, 4))\n",
    "A == B, A > B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855c15d",
   "metadata": {},
   "source": [
    "### Broadcasting Mechanism\n",
    "- Under certain conditions, even shapes differ, we can still perform **element-wise** operations by invoking the **broadcasting mechanism**.\n",
    "    - First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape.\n",
    "    - Second, carry out the element-wise operation on the resulting arrays\n",
    "- In most cases, we broadcast along an axis where an array initially only has length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "855266ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]),\n",
       " tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(3, 1)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "a, b, a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc4c6c",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "- As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "156988a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]),\n",
       " tensor([12, 13, 14, 15]),\n",
       " tensor([[ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16).reshape(4, 4)\n",
    "X, X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e5d1a",
   "metadata": {},
   "source": [
    "- Can also index using binary tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cda5b65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5.]]),\n",
       " tensor([ 6,  7,  8,  9, 10, 11]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(0, 12, 1).reshape((3, 4))\n",
    "B = 5 * torch.ones((3, 4))\n",
    "A, B, A[A > B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaa414",
   "metadata": {},
   "source": [
    "## Automatic Calculation of Gradients\n",
    "- In practice, based on our designed model, the system builds a **computational graph**, tracking which data combined through which operations to produce the output. Automatic differentiation enables the system to subsequently **backpropagate gradients**.\n",
    "    - Here *backpropagate* simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter\n",
    "\n",
    "### Intuition\n",
    "- All computation can be broken into simple components\n",
    "    - sum\n",
    "    - multiply\n",
    "    - exponential\n",
    "    - convolution\n",
    "    - ...\n",
    "- Derivatives for each simple component can be derived mathematically\n",
    "- Derivatives for **any composition** can be derived via **chain rule**\n",
    "\n",
    "### Forward Mode Mathematical Representation via a simple example\n",
    "- For a function $$f(x_1,x_2)=\\text{ln}(x_1)+x_1x_2-\\text{sin}(x_2)$$ the forward mode computation graph is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3401d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\r\n",
       " -->\r\n",
       "<!-- Title: ComputationGraph Pages: 1 -->\r\n",
       "<svg width=\"277pt\" height=\"151pt\"\r\n",
       " viewBox=\"0.00 0.00 277.19 151.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 147)\">\r\n",
       "<title>ComputationGraph</title>\r\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-147 273.19,-147 273.19,4 -4,4\"/>\r\n",
       "<!-- v_&#45;1 -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>v_&#45;1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"22.1\" cy=\"-112.5\" rx=\"22.2\" ry=\"22.2\"/>\r\n",
       "<text text-anchor=\"start\" x=\"13.1\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"20.1\" y=\"-109.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">&#45;1</text>\r\n",
       "</g>\r\n",
       "<!-- v_1 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>v_1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99.69\" cy=\"-123.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"92.69\" y=\"-120.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"99.69\" y=\"-120.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\r\n",
       "</g>\r\n",
       "<!-- v_&#45;1&#45;&gt;v_1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>v_&#45;1&#45;&gt;v_1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.16,-115.56C52.23,-116.74 61.58,-118.1 70.24,-119.36\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.92,-122.85 80.32,-120.82 70.93,-115.92 69.92,-122.85\"/>\r\n",
       "</g>\r\n",
       "<!-- v_2 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>v_2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99.69\" cy=\"-66.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"92.69\" y=\"-63.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"99.69\" y=\"-63.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">2</text>\r\n",
       "</g>\r\n",
       "<!-- v_&#45;1&#45;&gt;v_2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>v_&#45;1&#45;&gt;v_2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.4,-101.36C51.13,-95.44 63.24,-88.07 73.82,-81.64\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.89,-84.47 82.62,-76.28 72.25,-78.49 75.89,-84.47\"/>\r\n",
       "</g>\r\n",
       "<!-- v_0 -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>v_0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"22.1\" cy=\"-41.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"15.1\" y=\"-38.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"22.1\" y=\"-38.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\r\n",
       "</g>\r\n",
       "<!-- v_0&#45;&gt;v_2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>v_0&#45;&gt;v_2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.01,-47.42C50.05,-50.41 61.21,-54.1 71.3,-57.44\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.31,-60.8 80.91,-60.62 72.51,-54.15 70.31,-60.8\"/>\r\n",
       "</g>\r\n",
       "<!-- v_3 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>v_3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"174.69\" cy=\"-19.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"167.69\" y=\"-16.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"174.69\" y=\"-16.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">3</text>\r\n",
       "</g>\r\n",
       "<!-- v_0&#45;&gt;v_3 -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>v_0&#45;&gt;v_3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.56,-38.8C67.42,-35.02 114.65,-28.12 145.07,-23.68\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.69,-27.13 155.07,-22.22 144.67,-20.2 145.69,-27.13\"/>\r\n",
       "</g>\r\n",
       "<!-- v_4 -->\r\n",
       "<g id=\"node6\" class=\"node\">\r\n",
       "<title>v_4</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"174.69\" cy=\"-76.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"167.69\" y=\"-73.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"174.69\" y=\"-73.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">4</text>\r\n",
       "</g>\r\n",
       "<!-- v_1&#45;&gt;v_4 -->\r\n",
       "<g id=\"edge5\" class=\"edge\">\r\n",
       "<title>v_1&#45;&gt;v_4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M116.53,-113.3C126.1,-107.14 138.49,-99.16 149.28,-92.21\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151.37,-95.03 157.88,-86.68 147.58,-89.15 151.37,-95.03\"/>\r\n",
       "</g>\r\n",
       "<!-- v_2&#45;&gt;v_4 -->\r\n",
       "<g id=\"edge6\" class=\"edge\">\r\n",
       "<title>v_2&#45;&gt;v_4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119.11,-69.02C127.07,-70.11 136.55,-71.41 145.37,-72.62\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.89,-76.09 155.28,-73.98 145.84,-69.15 144.89,-76.09\"/>\r\n",
       "</g>\r\n",
       "<!-- v_5 -->\r\n",
       "<g id=\"node7\" class=\"node\">\r\n",
       "<title>v_5</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"249.69\" cy=\"-47.5\" rx=\"19.5\" ry=\"19.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"242.69\" y=\"-44.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">v</text>\r\n",
       "<text text-anchor=\"start\" x=\"249.69\" y=\"-44.8\" font-family=\"Times New Roman,serif\" baseline-shift=\"sub\" font-size=\"14.00\">5</text>\r\n",
       "</g>\r\n",
       "<!-- v_3&#45;&gt;v_5 -->\r\n",
       "<g id=\"edge8\" class=\"edge\">\r\n",
       "<title>v_3&#45;&gt;v_5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.36,-26.28C201.88,-29.54 212.28,-33.53 221.76,-37.17\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.59,-40.47 231.18,-40.78 223.1,-33.93 220.59,-40.47\"/>\r\n",
       "</g>\r\n",
       "<!-- v_4&#45;&gt;v_5 -->\r\n",
       "<g id=\"edge7\" class=\"edge\">\r\n",
       "<title>v_4&#45;&gt;v_5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.99,-69.63C201.56,-66.22 212.11,-62.03 221.72,-58.21\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223.26,-61.37 231.26,-54.42 220.67,-54.86 223.26,-61.37\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x2c2c87f0130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "f = Digraph('ComputationGraph')\n",
    "f.attr(rankdir='LR')\n",
    "f.attr('node', shape='circle')\n",
    "f.node('v_-1', label='<v<sub>-1</sub>>')\n",
    "f.node('v_0', label='<v<sub>0</sub>>')\n",
    "f.node('v_1', label='<v<sub>1</sub>>')\n",
    "f.node('v_2', label='<v<sub>2</sub>>')\n",
    "f.node('v_3', label='<v<sub>3</sub>>')\n",
    "f.node('v_4', label='<v<sub>4</sub>>')\n",
    "f.node('v_5', label='<v<sub>5</sub>>')\n",
    "f.edge('v_-1', 'v_1')\n",
    "f.edge('v_-1', 'v_2')\n",
    "f.edge('v_0', 'v_2')\n",
    "f.edge('v_0', 'v_3')\n",
    "f.edge('v_1', 'v_4')\n",
    "f.edge('v_2', 'v_4')\n",
    "f.edge('v_4', 'v_5')\n",
    "f.edge('v_3', 'v_5')\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18776008",
   "metadata": {},
   "source": [
    "- To find the **partial derivate** $\\frac{\\partial f}{\\partial x_1}$ when $x_1=2,x_2=5$, the Forward Evaluation Trace and Derivative Trace is:\n",
    "    1. Give the initial value and set initial derivative, notice the derivative of $x_1$ is set to be one while derivative of $x_2$ is set to $0$\n",
    "    $$\\begin{aligned}v_{-1}&=x_1=2\\\\ \\dot{v}_{-1}&=\\dot{x}_1=1\\end{aligned},\\ \\begin{aligned}v_0&=x_2=5\\\\ \\dot{v}_{0}&=\\dot{x}_2=0\\end{aligned}$$\n",
    "    2. Calculate $v_1$ and $v_2$ while keep tracking their derivatives\n",
    "    $$\\begin{aligned}v_1&=\\text{ln}v_{-1}=\\text{ln}2\\\\ \\dot{v}_1&=\\dot{v}_{-1}/v_{-1}=1/2\\end{aligned},\\  \\begin{aligned}v_2&=v_{-1}\\times v_0=10\\\\ \\dot{v}_2&=\\dot{v}_{-1}\\times v_0+\\dot{v}_0\\times v_{-1}=1\\times 5 +0\\times 2\\end{aligned}$$\n",
    "    3. Calculate $v_3$ and $v_4$ while keep tracking their derivatives\n",
    "    $$\\begin{aligned}v_3&=\\text{sin}v_0=\\text{sin}5\\\\ \\dot{v}_3&=\\dot{v}_0\\times\\text{cos}v_0=0\\times\\text{cos}5\\end{aligned},\\  \\begin{aligned}v_4&=v_1+v_2=0.693+10\\\\ \\dot{x}_4&=\\dot{v}_1+\\dot{v}_2=0.5+5\\end{aligned}$$\n",
    "    4. Get the final result and derivative\n",
    "    $$\\begin{aligned}y&=v_5=v_4-v_3=10.693+0.959\\\\ \\dot{y}&=\\dot{v}_5=\\dot{v}_4-\\dot{v}_3=5.5-0\\end{aligned}$$\n",
    "    5. Therefore, the result is $$\\frac{\\partial f}{\\partial x_1}=5.5|_{x_1=2,x_2=5}$$\n",
    "- Therefore, for a $n$-dimensional input and $1$-dimensional output\n",
    "    - Only calculate once for the output\n",
    "    - Need to calculate $n$ times for all the partial derivatives.\n",
    "        - Each time set one initial derivative to be $1$ and other initial derivative to be $0$\n",
    "    - **Low efficiency** when $n$ is large, which is common in complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e97ef8",
   "metadata": {},
   "source": [
    "### Build the computation graph by tracking data\n",
    "- PyTorch automatically creates a computation graph if **requires_grad=True**\n",
    "- For a given variable with **requires_grad=True**, identify the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e20cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
