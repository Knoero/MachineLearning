{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbda3f5f",
   "metadata": {},
   "source": [
    "**Information:** *Brief introduction to convexity, optimization, and gradient descent*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:** *05.29.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec5d92",
   "metadata": {},
   "source": [
    "# Convexity\n",
    "## Introduction\n",
    "- Convexity plays a vital rule in the design of optimization algorithms, which is largely due to fact that it is much easier to analyze and test algorithms in such a context. \n",
    "- If the algorithm performs poorly even in the convex setting, typically we should not hope to see great results otherwise.\n",
    "- Even though the optimization problems in ML/DL are generally non-convex, they often exhibit some properties of convex ones near local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370eac0",
   "metadata": {},
   "source": [
    "## Open and Closed Sets\n",
    "- Define $$A\\subset\\mathbb{R}^n$$ and open ball of diameter $\\epsilon$ is $B(r,\\epsilon)=\\{r\\in\\mathbb{R}^n:\\|r-r_o\\|<\\epsilon\\}$\n",
    "- A set $A$ is **open** if\n",
    "    - At every point, there is an open ball contained in $A$\n",
    "    - $\\forall r\\in A,\\ \\exists\\epsilon>0$ s.t. $B(r,\\epsilon)\\subset A$\n",
    "- A set $A$ is **closed** if $A^c=\\mathbb{R}^n-A$ is open\n",
    "- A set $A$ is compact if it is closed and bounded\n",
    "- Facts:\n",
    "    - $\\mathbb{R}^N$ is both open and closed, but it is not compact\n",
    "    - If $A$ is compact, then every sequence in $A$ has a limit point in $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743cf1c6",
   "metadata": {},
   "source": [
    "## Convex Sets\n",
    "### Definition\n",
    "- A set $C$ is convex if, for any $x,y\\in C$ and $\\theta\\in\\mathbb{R}$ with $0\\le\\theta\\le 1$: $$\\theta x+(1-\\theta)y\\in C$$\n",
    "    - Intuitively, it means if we take any two elements in $C$ and draw a line segment between these two elements, then every point on that line segment also belongs to $C$\n",
    "- The point $\\theta x+(1-\\theta)y$ is called a **convex combination** of the points $x$ and $y$\n",
    "\n",
    "### Examples\n",
    "- **All of** $\\mathbb{R}^n$. \n",
    "    - Given any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n$,$$\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y}\\in\\mathbb{R}^n$$\n",
    "- **The non-negative orthant** $\\mathbb{R}^n_+$.\n",
    "    - $\\mathbb{R}^n_+$ consists of all vectors in $\\mathbb{R}^n$ whose elements are all non-negative $$\\mathbb{R}^n_+=\\{\\mathbf{x}:x_i\\ge 0\\ \\forall i =1,\\cdots,n\\}$$\n",
    "    - Given any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^N_+$ and $0\\le\\theta\\le 1$, $$(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})_i=\\theta x_i+(1-\\theta)y_i\\ge 0\\ \\forall i$$\n",
    "- **Norm balls**\n",
    "    - Let $\\|\\cdot\\|$ be some norm on $\\mathbb{R}^n$ (e.g., the Euclidean norm $\\|\\mathbf{x}\\|_2=\\sqrt{\\Sigma^n_{i=1}x_i^2}$). Then the set $\\{\\mathbf{x}:\\|\\mathbf{x}\\|\\le 1\\}$ is a convex set.\n",
    "    - Given $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n$ with $\\|\\mathbf{x}\\|\\le 1,\\|\\mathbf{y}\\|\\le 1$ and $0\\le\\theta\\le 1$. Then $$\\|\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y}\\|\\le\\|\\theta \\mathbf{x}\\|+\\|(1-\\theta)\\mathbf{y}\\|=\\theta\\|\\mathbf{x}\\|+(1-\\theta)\\|\\mathbf{y}\\|\\le 1$$ where the **triangle inequality** and the **positive homogeneity** of norms are used\n",
    "- **Affine subspaces and polyhedra**\n",
    "    - Given a matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and a vector $\\mathbf{b}\\in\\mathbb{R}^m$, an affine subspace is the set $\\{\\mathbf{x}\\in\\mathbb{R}^n:\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\}$ (note this could possible be empty if $\\mathbf{b}$ is not in range of $\\mathbf{A}$).\n",
    "    - Given $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n$ s.t. $\\mathbf{Ax}=\\mathbf{Ay}=\\mathbf{b}$, then for $0\\le\\theta\\le 1$: $$\\mathbf{A}(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})=\\theta \\mathbf{Ax}+(1-\\theta)\\mathbf{Ay}=\\theta \\mathbf{b}+(1-\\theta)\\mathbf{b}=\\mathbf{b}$$\n",
    "    - Similarly, a polyhedron is the set $\\{\\mathbf{x}\\in\\mathbb{R}^n:\\mathbf{Ax}\\preceq \\mathbf{b}\\}$ (also possibly empty), where $\\preceq$ denotes componentwise inequality\n",
    "        - All the entries of $\\mathbf{Ax}$ are less than or equal to their corresponding element in $\\mathbf{b}$\n",
    "    - Given $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n$ that satisfy $\\mathbf{Ax}\\le \\mathbf{b}$ and $\\mathbf{Ay}\\le \\mathbf{b}$ and $0\\le\\theta\\le 1$: $$\\mathbf{A}(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})\\le\\theta \\mathbf{b}+(1-\\theta)\\mathbf{b}=\\mathbf{b}$$\n",
    "- **Intersection of convex sets**\n",
    "    - Suppose $C_1,C_2,\\cdots,C_k$ are convex sets. Then their intersection $$\\underset{i=1}{\\overset{k}{\\bigcap}}C_i=\\{x:x\\in C_i\\ \\forall i=1,\\cdots,k\\}$$ is also a convex set\n",
    "    - Given $x,y\\in\\underset{i=1}{\\overset{k}{\\bigcap}}C_i$ and $0\\le\\theta\\le 1$. Then $$\\theta x+(1-\\theta)y\\in C_i\\ \\forall i=1,\\cdots,k$$ by the definition of a convex set. Therefore $$\\theta x+(1-\\theta)y\\in\\underset{i=1}{\\overset{k}{\\bigcap}}C_i$$\n",
    "    - Note that the *union* of convex sets in general will not be convex\n",
    "- **Positive semidefinite matrices**\n",
    "    - The set of all symmetric positive semidefinite matrices, often times called the *positive semidefinite cone* and denoted $\\mathbb{S}_+^n$ is a convex set (in general, $\\mathbb{S}^n\\subset\\mathbb{R}^{n\\times n}$ denotes the set of symmetric $n\\times n$ matrices).\n",
    "    - A matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive semidefinite if and only if $\\mathbf{A}=\\mathbf{A}^T$ and for all $\\mathbf{x}\\in\\mathbb{R}^n,\\mathbf{x}^T\\mathbf{Ax}\\le 0$\n",
    "    - Given two symmetric positive semidefinite matrices $\\mathbf{A},\\mathbf{B}\\in\\mathbb{S}^n_+$ and $0\\le\\theta\\le 1$, then for any $\\mathbf{x}\\in\\mathbb{R}^n$, $$\\mathbf{x}^T(\\theta\\mathbf{A}+(1-\\theta)\\mathbf{B})\\mathbf{x}=\\theta \\mathbf{x}^T\\mathbf{Ax}+(1-\\theta)\\mathbf{x}^T\\mathbf{B}\\mathbf{x}\\ge 0$$\n",
    "    - The logic to show that all **positive definite**, **negative definite**, and **negative semidefinite** matrices are each also convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801e5c0",
   "metadata": {},
   "source": [
    "## Convex Functions\n",
    "### Definition\n",
    "- A function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ is convex if its domain (denoted $\\mathcal{D}(f)$) is a *convex set*, and if, for all $\\mathbf{x},\\mathbf{y}\\in\\mathcal{D}(f)$ and $\\theta\\in\\mathbb{R},0\\le\\theta\\le 1$: $$f(\\theta\\mathbf{x}+(1-\\theta)\\mathbf{y})\\le\\theta f(\\mathbf{x})+(1-\\theta)f(\\mathbf{y})$$\n",
    "    - Intuitively, it means if we pick any two points on the graph pf a convex function and draw a straight line between them, then the portion of function between these two points will lie below this straight line.\n",
    "- A function is called **strictly convex** if the definition holds with strict inequality for $\\mathbf{x}\\ne\\mathbf{y}$ and $0\\lt\\theta\\lt 1$ \n",
    "- A function $f$ is called **concave** if $-f$ is convex\n",
    "- A function $f$ is called **strictly concave** if $-f$ is strictly convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35631836",
   "metadata": {},
   "source": [
    "### First Order Condition for Convexity\n",
    "- Suppose a function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ is differentiable. Then $f$ is convex if and only if $\\mathcal{D}(f)$ is a convex set and for $\\mathbf{x},\\mathbf{y}\\in\\mathcal{D}(f)$, $$f(\\mathbf{y})\\ge f(\\mathbf{x})+\\nabla_x f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x})$$ where the function $f(\\mathbf{x})+\\nabla_x f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x})$ is called the **first-order approximation** to the function $f$ at the point $\\mathbf{x}$\n",
    "    - Intuitively, this can be thought of as approximating $f$ with its tangent line at the point $\\mathbf{x}$.\n",
    "- Similarly, $f$ would be \n",
    "    - strictly convex if this holds with strict inequality\n",
    "    - concave if the inequality is reversed\n",
    "    - strictly concave if the reverse inequality is strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb4e51",
   "metadata": {},
   "source": [
    "### Second Order Condition for Convexity\n",
    "- Suppose a function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ is twice differentiable. Then $f$ is convex if and only if $\\mathcal{D}(f)$ is a convex set and its *Hessian* is positive semidefinite: $$\\forall x\\in\\mathcal{D}(f),\\ \\nabla^2_xf(x)\\succeq 0$$\n",
    "    - Here the notation $\\succeq$ refers to positive semidefiniteness\n",
    "    - In one dimension, this is equivalent to the condition that the second derivative $f''(x)$ always be positive\n",
    "- Similarly, $f$ is\n",
    "    - strictly convex if its Hessian is positive definite\n",
    "    - concave if the Hessian is negative semidefinite\n",
    "    - strictly concave if the Hessian is negative definite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2d551",
   "metadata": {},
   "source": [
    "### Jensen's Inequality\n",
    "Start with the inequality in the basic definition of a convex function $$f(\\theta x+(1-\\theta)y)\\le\\theta f(x)+(1-\\theta)f(y)\\ \\text{for}\\ 0\\le\\theta\\le 1$$ Using induction, extend this definition to convex combinations of more than one point $$f\\left(\\underset{i=1}{\\overset{k}{\\Sigma}}\\theta_ix_i\\right)\\le\\underset{i=1}{\\overset{k}{\\Sigma}}\\theta_if(x_i)\\ \\ \\text{for}\\ \\underset{i=1}{\\overset{k}{\\Sigma}}\\theta_i=1,\\ \\theta_i\\ge 0\\ \\forall i$$ This can also extend to infinite sums or integrals. In the latter case, the inequality can be written as $$f\\left(\\int p(x)xdx\\right)\\le\\int p(x)f(x)dx \\ \\ \\ \\text{for}\\ \\ \\ \\int p(x)dx=1,\\ p(x)\\ge 0\\ \\ \\forall x$$ Since $\\int p(x)dx=1$, it is common to consider it a probability density, in which case the previous equation can be written in terms of expectations $$f(\\mathbb{E}[x])\\le\\mathbb{E}[f(x)]$$ which is called **Jensen's inequality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861fd28",
   "metadata": {},
   "source": [
    "### Examples\n",
    "- **Exponential**\n",
    "    - Let $f:\\mathbb{R}\\rightarrow\\mathbb{R},f(x)=e^{ax}$ for any $a\\in\\mathbb{R}$.\n",
    "    - $f''(x)=a^2e^{ax}$ is positive for all $x$\n",
    "- **Negative logarithm**\n",
    "    - Let $f:\\mathbb{R}\\rightarrow\\mathbb{R},f(x)=-\\text{log}x$ with domain $\\mathcal{D}(f)=\\mathbb{R}_{++}=\\{x:x>0\\}$\n",
    "    - $f''(x)=\\frac{1}{x^2}\\gt 0$ for all $x$\n",
    "- **Affine functions**\n",
    "    - Let $f:\\mathbb{R}^n\\rightarrow\\mathbb{R},f(\\mathbf{x})=\\mathbf{b}^T\\mathbf{x}+c$ for some $\\mathbf{b}\\in\\mathbb{R}^n,c\\in\\mathbb{R}$\n",
    "    - The Hessian $\\nabla^2_{\\mathbf{x}}f(\\mathbf{x})=0$ for all $\\mathbf{x}$\n",
    "    - Affine functions of this form are the **only** functions that are **both convex and concave**\n",
    "- **Quadratic function**\n",
    "    - Let $f:\\mathbb{R}^n\\rightarrow\\mathbb{R},f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^T\\mathbf{Ax}+\\mathbf{b}^T\\mathbf{x}+c$ for a symmetric matrix $\\mathbf{A}\\in\\mathbb{S}^n,b\\in\\mathbb{R}^n$ and $c\\in\\mathbb{R}$\n",
    "    - The Hessian for this function is $\\nabla^2_{\\mathbf{x}}f(\\mathbf{x})=\\mathbf{A}$\n",
    "    - The convexity or non-convexity of $f$ is determined entirely by whether or not $\\mathbf{A}$ is positive semidefinite\n",
    "    - The **squared Euclidean norm** $f(\\mathbf{x})=\\|\\mathbf{x}\\|^2_2=\\mathbf{x}^T\\mathbf{x}$ is a special case of quadratic functions where $\\mathbf{A}=\\mathbf{I},\\mathbf{b}=\\mathbf{0},c=0$, so it is therefore a **strictly convex function**\n",
    "- **Norms**\n",
    "    - Let $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ be some norm on $\\mathbb{R}^n$. \n",
    "    - By the **triangle inequality** and **positive homogeneity** of norms, for $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n,0\\le\\theta\\le 1$, $$f(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})\\le f(\\theta \\mathbf{x})+f((1-\\theta)\\mathbf{y})=\\theta f(\\mathbf{x})+(1-\\theta)f(\\mathbf{y})$$\n",
    "    - Not possible to prove convexity based on the first or second order conditions because norms are not generally differentiable\n",
    "- **Nonnegative weighted sums of convex functions**\n",
    "    - Let $f_1,f_2,\\cdots,f_k$ be convex functions and $w_1,w_2,\\cdots,w_k$ be nonnegative real numbers. Then $$f(x)=\\underset{i=1}{\\overset{k}{\\Sigma}}w_if_i(x)$$ is a convex function, since $$\\begin{aligned}f(\\theta x+(1-\\theta)y)&=\\underset{i=1}{\\overset{k}{\\Sigma}}w_if_i(\\theta x+(1-\\theta)y)\\\\&\\le\\underset{i=1}{\\overset{k}{\\Sigma}}w_i(\\theta f_i(x)+(1-\\theta)f_i(y))\\\\&=\\theta\\underset{i=1}{\\overset{k}{\\Sigma}}w_if_i(x)+(1-\\theta)\\underset{i=1}{\\overset{k}{\\Sigma}}w_if_i(y)\\\\&=\\theta f(x)+(1-\\theta)f(x)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b869b65",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3358c1",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "## Motivation\n",
    "- Most ML/DL algorithms involve **optimization** of some sort. \n",
    "    - Optimization refers to the task of either **minimizing** or maximizing some function $f(\\mathbf{x})$ by altering $\\mathbf{x}$\n",
    "    - Usually phrase most optimization problems in terms of minimizing $f(\\mathbf{x})$\n",
    "    - Maximization may be accomplished via s minimization algorithm by minimizing $-f(\\mathbf{x})$\n",
    "- Usually the function we want to minimize is called the **objective function**, or **criterion**. In ML/DL contexts, the name **loss function** is often used.\n",
    "    - As mentioned in introduction, a loss function quantifies the *distance* between the **real** and **predicted** value of the target.\n",
    "    - Usually be a non-negative number where smaller values are better and perfect predictions incur a loss of $0$\n",
    "    - Usually denoted as $L(\\boldsymbol{\\theta})$ where $\\boldsymbol{\\theta}$ is usually the parameter of ML/DL models \n",
    "- Usually denote the value that minimizes a function with a superscript $*$\n",
    "    - $\\boldsymbol{\\theta}^*=\\text{arg}\\underset{\\boldsymbol{\\theta}}{\\text{min}}L(\\boldsymbol{\\theta})$\n",
    "- Most ML/DL algorithms are so complex that it is difficult or impossible to find the closed form solution for the optimization problem\n",
    "    - Use numerical optimization method instead\n",
    "- One common algorithm is **gradient descent**, other optimization algorithms are\n",
    "    - Expectation Maximization\n",
    "    - Sampling-based optimization\n",
    "    - Greedy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d94c2a",
   "metadata": {},
   "source": [
    "## Local Minimum and Global Minimum\n",
    "### Local Minimum\n",
    "- Let $f:A\\rightarrow\\mathbb{R}$ where $A\\subset \\mathbb{R}^N$, a point $x$ is locally minimal if it is available and if there exists some $R\\lt 0$ such that all feasible points $z$ with $\\|x-z\\|_2\\le R$, satisfy $f(x)\\le f(z)$\n",
    "- **Necessary** condition for local minimum\n",
    "    - Let $f$ be continuously differentiable and let $x\\in A$ be a local minimum, then $\\nabla f(x)=0$\n",
    "- **Saddle Point**:\n",
    "    - We say that $x\\in A$ is a saddle point of $f$ if $\\nabla f(x)=0$ and $r_o$ is not a local minimum\n",
    "\n",
    "### Global Minimum\n",
    "- A point $x$ is globally minimal if it is available and for all feasible points $z$, $f(x)\\le f(z)$\n",
    "    - A global minimum must also be a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4f68d",
   "metadata": {},
   "source": [
    "## Optimization Theorems\n",
    "- Let $f:A\\rightarrow\\mathbb{R}$ where $A\\subset\\mathbb{R}^N$\n",
    "    - If $f$ is continuous and $A$ is **compact**, then $f$ takes on a global minimum in $A$\n",
    "    - If $f$ is **convex** on $A$, then any local minimum is a global minimum\n",
    "    - If $f$ is continuously differentiable and convex on $A$, then $\\nabla f(x)=0$ implies the $x\\in A$ is a global minimum of $f$\n",
    "- **Important Facts**:\n",
    "    - Global minimum **may not be unique**\n",
    "    - If $A$ is closed but not bounded, then $f$ may not take on a global minimum\n",
    "    - Most interesting functions in ML/DL are **not** convex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea886f1",
   "metadata": {},
   "source": [
    "## Convex Optimization\n",
    "- Formally, a convex optimization problem is an optimization problem of the form $$\\begin{aligned}&\\text{minimize}&f(x)\\\\&\\text{subject to}&x\\in C\\end{aligned}$$ where $f$ is a convex function, $C$ is a convex set, and $x$ is the optimization variable\n",
    "- Often written as $$\\begin{aligned}&\\text{minimize}&f(x)&\\ \\\\&\\text{subject to}&g_i(x)\\le 0 &\\ \\ \\ i=1,\\cdots,m\\\\& &h_i(x)=0 &\\ \\ \\ i=1,\\cdots,p \\end{aligned}$$ where $f$ is a convex function, $g_i$ are convex functions and $h_i$ are affine functions and $x$ is the optimization variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588cc144",
   "metadata": {},
   "source": [
    "## Constrained Optimization\n",
    "### Definition\n",
    "- Sometimes we wish not only to maximize or minimize a function $f(\\mathbf{x})$ over all possible values of $\\mathbf{x}$. Instead the maximal or minimal value of $f(\\mathbf{x})$ for values of $\\mathbf{x}$ in some set $\\mathbb{S}$. This is known as **constrained optimization**\n",
    "- Points $\\mathbf{x}$ that lies within the set $\\mathbb{S}$ are called **feasible** points in constrained optimization terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413eaf14",
   "metadata": {},
   "source": [
    "### Karush-Kuhn-Tucker (KKT) approach\n",
    "- **Intuition**: Design a different, **unconstrained** optimization problem whose solution can be converted into a solution to the original constrained optimization problem\n",
    "    - For example, to minimize $f(\\mathbf{x})$ for $\\mathbf{x}\\in\\mathbb{R}^2$ with $\\mathbf{x}$ constrained to have exactly unit $L^2$ norm, we can instead minimize $$g(\\theta)=f([\\text{cos}\\theta,\\text{sin}\\theta]^T)$$ with respect to $\\theta$, then return $[\\text{cos}\\theta,\\text{sin}\\theta]$ as the solution to the original problem\n",
    "    - Requires creativity\n",
    "    - The transformation between optimization problems must be designed specifically for each case we counter\n",
    "- **Karush-Kuhn-Tucker** (KKT) approach provides a very general solution to constrained optimization\n",
    "- **Generalized Lagrangian**\n",
    "    - Also called **generalized Lagrange function**\n",
    "    - Describe $\\mathbb{S}$ in terms of $m$ functions $g_i$ and $n$ functions $h_j$: $$\\mathbb{S}=\\{\\mathbf{x}|\\forall i,g_i(\\mathbf{x})=0\\text{ and }\\forall j,h_j(\\mathbf{x})\\le 0\\}$$ Equations involving $g_i$ are called the **equality constraints** and the inequalities involving $h_j$ are called **inequality constraints**\n",
    "    - Introduce new variables $\\lambda_i$ and $\\alpha_i$ for each constraint, which are called the **KKT multipliers**, then the generalized Lagrangian is then defined as $$L(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\alpha})=f(\\mathbf{x})+\\underset{i=1}{\\overset{m}{\\Sigma}}\\lambda_ig_i(\\mathbf{x})+\\underset{j}{\\overset{n}{\\Sigma}}\\alpha_jh_j(\\mathbf{x})$$ \n",
    "    - The generalized Lagrangian enables us to solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian. As long as at least one feasible point exists and $f(\\mathbf{x})$ is not permitted to have value $\\infty$, then $$\\underset{\\mathbf{x}}{\\text{min}}\\ \\underset{\\boldsymbol{\\lambda}}{\\text{max}}\\ \\underset{\\boldsymbol{\\alpha},\\boldsymbol{\\alpha}\\ge 0}{\\text{max}}L(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\alpha})$$ has the same optimal objective function value ans set of optimal points $\\mathbf{x}$ as $$\\underset{\\mathbf{x}\\in\\mathbb{S}}{\\text{min}}f(x)$$\n",
    "    - Any time the constraints are satisfied $$\\underset{\\boldsymbol{\\lambda}}{\\text{max}}\\ \\underset{\\boldsymbol{\\alpha},\\boldsymbol{\\alpha}\\ge 0}{\\text{max}}L(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\alpha})=f(\\mathbf{x})$$ while any time a constraint is violated $$\\underset{\\boldsymbol{\\lambda}}{\\text{max}}\\ \\underset{\\boldsymbol{\\alpha},\\boldsymbol{\\alpha}\\ge 0}{\\text{max}}L(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\alpha})=\\infty$$ These properties guarantee that no infeasible point can be optimal, and that the optimum within the feasible points is unchanged\n",
    "- **KKT Conditions**:\n",
    "    - A simple set of properties describe the optimal properties of constrained optimization problems\n",
    "    - **Necessary** conditions but **not always sufficient** conditions\n",
    "    - The conditions are:\n",
    "        - The gradient of the generalized Lagrangian is zero\n",
    "        - All constraints on both $\\mathbf{x}$ and the KKT multipliers are satisfied\n",
    "        - The inequality constraints exhibit \"complementary slackness\": $\\boldsymbol{\\alpha}\\odot\\mathbf{h}(\\mathbf{x})=\\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660de27",
   "metadata": {},
   "source": [
    "### Example: Linear Least Squares\n",
    "To find the value of $\\mathbf{x}$ that minimizes $$f(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{Ax}-\\mathbf{b}\\|^2_2$$ subject to the constraint $\\mathbf{x}^T\\mathbf{x}\\le 1$, introduce the Lagrangian $$L(\\mathbf{x},\\lambda)=f(\\mathbf{x})+\\lambda(\\mathbf{x}^T\\mathbf{x}-1)$$ then solve the problem $$\\underset{\\mathbf{x}}{\\text{min}}\\ \\underset{\\lambda,\\lambda\\ge 0}{\\text{max}}L(\\mathbf{x},\\lambda)$$ By differentiating the Lagrangian with respect to $\\mathbf{x}$, obtain $$\\mathbf{A}^T\\mathbf{Ax}-\\mathbf{A}^T\\mathbf{b}+2\\lambda\\mathbf{x}=0$$ which means the solution would take the form $$\\mathbf{x}=(\\mathbf{A}^T\\mathbf{A}+2\\lambda\\mathbf{I})^{-1}\\mathbf{A}^T\\mathbf{b}$$ To decide the magnitude of $\\lambda$ which makes the result obeys the constraint, perform gradient ascent on $\\lambda$: $$\\frac{\\partial}{\\partial \\lambda}L(\\mathbf{x},\\lambda)=\\mathbf{x}^T\\mathbf{x}-1$$ When the norm of $\\mathbf{x}$ exceeds $1$, the derivative is positive and we need to increase $\\lambda$. Because the coefficient on the $\\mathbf{x}^T\\mathbf{x}$ penalty has increased, solving the linear equation for $\\mathbf{x}$ will now yield a solution with smaller norm. Repeat the process of solving the linear equation and adjusting $\\lambda$ continues until $\\mathbf{x}$ has the correct norm and the derivative on $\\lambda$ is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b654f48",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a47ef",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Basic Concepts\n",
    "### Definition\n",
    "- **Definition**:\n",
    "    - A **first-order iterative** optimization algorithm for finding **local minimum** of a **differential** function.\n",
    "        - The idea is to take *repeated steps* in the opposite direction of the *gradient* of the function at the current point, because this is the direction of steepest descent.\n",
    "        - As it only calculates the *first-order* derivative, it requires the objective function to be *differential* and is called *first-order optimization algorithms*\n",
    "            - Some optimization algorithms that also use the Hessian matrix are called *second-order optimization algorithms*\n",
    "        - Converge when first-order derivative is zero, which only ensures reaching **local minimum** for general functions\n",
    "            - That is to say, the start point will sometimes affect final convergence\n",
    "        - Generally speaking, gradient descent algorithms converge to the **global minimum** of continuously differentiable **convex** functions\n",
    "- **Theory**:\n",
    "    - Based on the observation that if the multi-variable function $F(\\mathbf{x})$ is defined and differentiable in a neighborhood of a point $\\mathbf{a}$, then $F(\\mathbf{x})$ decreases **fastest** if one goes from $\\mathbf{a}$ in the direction of the negative gradient of $F$ at $\\mathbf{a}$, which is $-\\nabla F(\\mathbf{a})$. It follows that if $$\\mathbf{a}_{n+1}=\\mathbf{a}_n-\\eta\\nabla F(\\mathbf{a}_n)$$ for a $\\eta\\in\\mathbb{R}_+$ small enough, then $$F(\\mathbf{a}_n)\\ge F(\\mathbf{a}_{n+1})$$\n",
    "- Simple form of **vanilla gradient descent** (GD):\n",
    "    1. Start at random parameter $\\boldsymbol{\\theta}$\n",
    "    2. Repeat until converged\n",
    "        - $\\mathbf{d}\\leftarrow-\\nabla L(\\boldsymbol{\\theta})$\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\eta\\mathbf{d}^T$\n",
    "    - $\\eta$ is called **learning rate** or **step size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f957935",
   "metadata": {},
   "source": [
    "### Compute Loss Gradient\n",
    "- Take the **mean square error** as an example: $$\\begin{aligned}\\nabla_{\\boldsymbol{\\theta}}L_{MSE}(\\boldsymbol{\\theta})&=\\nabla_{\\boldsymbol{\\theta}}\\left\\{\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}\\left\\|\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i)\\right\\|^2\\right\\}\\\\&=\\frac{1}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}\\nabla_{\\boldsymbol{\\theta}}\\left\\{(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\right\\}\\end{aligned}$$ Use the chain rule and scale-by-vector matrix calculus identity that $$\\frac{\\partial \\mathbf{x}^T\\mathbf{x}}{\\partial \\mathbf{x}}=2\\mathbf{x}^T$$ We can get $$\\begin{aligned}\\nabla_{\\boldsymbol{\\theta}}L_{MSE}(\\boldsymbol{\\theta})&=\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\\\&=\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\\\&=-\\frac{2}{N}\\underset{i=1}{\\overset{N}{\\Sigma}}(\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))^T\\nabla_{\\boldsymbol{\\theta}}(f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))\\end{aligned}$$\n",
    "- The result of the gradient usually includes three parts:\n",
    "    - Sum over training data. It consists of a lot of computations but the way of computation is relatively easy and straight forward\n",
    "    - Prediction error term such as $\\mathbf{y}_i-f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i)$ in MSE, which is usually easy to get\n",
    "    - Gradient of inference function $\\nabla_{\\boldsymbol{\\theta}}(f_{\\boldsymbol{\\theta}}(\\mathbf{x}_i))$, which is difficult to solve\n",
    "        - Enabled by **automatic differentiation** built into modern domain specific languages such as Pytorch, Tensorflow, ...\n",
    "        - For neural networks, this is known as **back propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cc8df",
   "metadata": {},
   "source": [
    "### Select appropriate learning rate\n",
    "- Too large $\\eta$ leads to instability and even divergence\n",
    "- Too small $\\eta$ leads to slow convergence\n",
    "- **Steepest gradient descent** use **line search** to compute the best $\\eta$\n",
    "    1. Start at random parameter $\\boldsymbol{\\theta}$\n",
    "    2. Repeat until converged\n",
    "        - $\\mathbf{d}\\leftarrow-\\nabla L(\\boldsymbol{\\theta})$\n",
    "        - $\\eta^*\\leftarrow\\text{arg}\\underset{\\eta}{\\text{min}}\\{L(\\boldsymbol{\\theta}+\\eta\\mathbf{d}^T)\\}$\n",
    "        - $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\eta^*\\mathbf{d}^T$\n",
    "- **Adaptive learning rates** may help, but not always\n",
    "    - $\\alpha=\\frac{1}{t}$, approaches 0 but can cover an infinite distance since $\\underset{a\\rightarrow\\infty}{\\text{lim}}\\underset{t=1}{\\overset{a}{\\Sigma}}\\frac{1}{t}=\\infty$\n",
    "- **Coordinate Descent** update one parameter at a time\n",
    "    - Removes problem of selecting step size\n",
    "    - Each update can be very fast, but lots of updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85690",
   "metadata": {},
   "source": [
    "### Slow convergence due to Poor Conditioning\n",
    "- **Conditioning** refers to how rapidly a function changes with respect to small changes in its inputs.\n",
    "- Consider the function $$f(x)=\\mathbf{A}^{-1}\\mathbf{x}$$ When $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ has an eigenvalue decomposition, its **condition number** is $$\\underset{i,j}{\\text{max}}\\left|\\frac{\\lambda_i}{\\lambda_j}\\right|$$ This is the ratio of the magnitude of the largest and smallest eigenvalue\n",
    "- A problem with a **low condition number** is said to be **well-conditioned**, while a problem with a high condition number is said to be ill-conditioned\n",
    "    - In non-mathematical terms, an ill-conditioned problem is one where, for a small change in the inputs there is a large change in the answer or dependent variable, which means the correct solution to the equation becomes hard to find\n",
    "    - Condition number is a property of the problem\n",
    "- **Gradient descent** is very sensitive to **condition number** of the problem\n",
    "    - No good choice of step size. Tiny change in one variable could lead to great change in dependent variable.\n",
    "- **Solutions:**\n",
    "    - **Newton's method:** Correct for local second derivative.\n",
    "        - Too much computation and too difficult to implement\n",
    "        - Harmful when near saddle points\n",
    "    - **Alternative methods**:\n",
    "        - Preconditioning: Easy, but tends to be ad-hoc, not so robust\n",
    "        - Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad9e44",
   "metadata": {},
   "source": [
    "### Vanishing Gradients\n",
    "- The most insidious problem to encounter\n",
    "- Some function leads to almost zero gradients far away from local minimums, which makes the optimization stuck for a long time or even stop.\n",
    "- For example, assume that we want to minimize the function $$f(x)=\\text{tanh}(x)$$ The derivative is $$f'(x)=1-\\text{tanh}^2(x)$$ If we happen to get started at $x=4$ then the derivative at that point is $$f'(4)=0.0013$$ The gradient is close to nil. Consequently, optimization will get stuck for a long time before we make progress\n",
    "- **Possible Solutions**:\n",
    "    - Reparameterize the problem\n",
    "    - Good initialization of the parameter\n",
    "    - Reconstruct the objective function (e.g., change activation function in neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f6fe5",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e04571",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "In Machine Learning and Deep Learning, the objective function is usually the average of the loss functions for each example in the training dataset. Given a training dataset of $n$ examples, we assume that $f_i(\\mathbf{x})$ is the loss function with respect to the training example of index $i$, where $\\mathbf{x}$ is the parameter vector. The objective function is\n",
    "$$f(\\mathbf{x})=\\frac{1}{n}\\underset{i=1}{\\overset{n}{\\Sigma}}f_i(\\mathbf{x})$$\n",
    "Then the gradient of the objective function at $\\mathbf{x}$ is computed as\n",
    "$$\\nabla f(\\mathbf{x})=\\frac{1}{n}\\underset{i=1}{\\overset{n}{\\Sigma}}\\nabla f_i(\\mathbf{x})$$\n",
    "It's obvious that when the training dataset is larger and larger, the computation cost for one iteration would become higher and higher, which leads to slow convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f167d04",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Updates\n",
    "At each iteration, uniformly sample an index $i\\in\\{1,2,\\cdots,n\\}$ for data examples at random, and compute the gradient $\\nabla f_i(\\mathbf{x})$ to update $\\mathbf{x}$: $$\\mathbf{x}\\leftarrow\\mathbf{x}-\\eta\\nabla f_i(\\mathbf{x})$$ where $\\alpha$ is the learning rate\n",
    "- **Advantages**:\n",
    "    - Reduce the computation cost for each iteration from $\\mathcal{O}(n)$ to $\\mathcal{O}(1)$\n",
    "    - $\\nabla f_i(\\mathbf{x})$ is an unbiased estimate of the full gradient $\\nabla f(\\mathbf{x})$: $$\\mathbb{E}_i[\\nabla f_i(\\mathbf{x})]=\\frac{1}{n}\\underset{i=1}{\\overset{n}{\\Sigma}}\\nabla f_i(\\mathbf{x})=\\nabla f(\\mathbf{x})$$ which means the stochastic gradient is a good estimate of the gradient on average\n",
    "- **Disadvantages**:\n",
    "    - Usually the trajectory of the variables in SGD would be much more noisy than the one observed in gradient descent, due to the stochastic nature of the gradient\n",
    "    - Even when we arrive near the minimum, we are still subject to the uncertainty injected by the instantaneous gradient via $\\eta\\nabla f_i(\\mathbf{x})$ and the performance usually won't improve\n",
    "    - No good choice of step size $\\Rightarrow$ *Dynamic Learning Rate*\n",
    "        - Large step size always make us hanging around the minimum\n",
    "        - Small step size prevents any meaningful progress initially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207747ac",
   "metadata": {},
   "source": [
    "### Dynamic Learning Rate\n",
    "As mentioned above, we usually want a large step size at the beginning to accelerate the convergence and a small step size near minimum to get better performance. The intuition is to replace $\\eta$ with a time-dependent learning rate $\\eta(t)$, which requires to figure out how rapidly $\\eta$ should decay. Decaying too quickly make us stop optimization prematurely and decaying to slowly will waste too much time on optimization. Several basic strategies that are used in adjusting $\\eta$ over time is:\n",
    "- *Piecewise constant*: $\\eta(t)=\\eta_i\\ \\text{if}\\ t_i\\le t\\le t_{i+1}$. Common strategy for training deep networks\n",
    "- *Exponential decay*: $\\eta(t)=\\eta_0\\cdot e^{-\\lambda t}$. Often leads to premature stopping before the algorithm has converged\n",
    "- *Polynomial decay*: $\\eta(t)=\\eta_0\\cdot(\\beta t+1)^{-\\alpha}$ A popular choice is $\\alpha = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e18914",
   "metadata": {},
   "source": [
    "### Stochastic Gradients and Finite Samples\n",
    "- Usually, the we do not perform exactly **stochastic** gradient descent\n",
    "- Instead of actually randomly select an instance from the training set with replacement, we iterated over all instances **exactly once**\n",
    "- Consider sampling $n$ observations from the discrete distribution with *replacement*. The probability of choosing an element $i$ at random is $\\frac{1}{n}$. Denote the probability of choose it $x$ times in $n$ samples is $P(X=x)$, then the probability of picking some sample **at least once** is $$P(X\\ge 1)=1-\\left(1-\\frac{1}{n}\\right)^n$$ When $n\\rightarrow\\infty$, it's easy to prove that $$\\underset{n\\rightarrow\\infty}{\\text{lim}}P(X\\ge 1)=1-\\frac{1}{e}\\approx 0.63$$ Also consider the probability of picking some sample **exactly once** is $$P(X=1)=C^1_n\\cdot\\frac{1}{n}\\left(1-\\frac{1}{n}\\right)^{n-1}$$ Similarly, $$\\underset{n\\rightarrow\\infty}{\\text{lim}}P(X= 1)=\\frac{1}{e}\\approx 0.37$$ That is to say, the sampling with replacement leads to an increased variance and decreased data efficiency relative to sampling without replacement. Therefore, in machine learning and deep learning context, we prefer **sampling without replacement**\n",
    "- The common way is to iterating over the training dataset in different random orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59420e",
   "metadata": {},
   "source": [
    "## Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e2392",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- Gradient Descent is not particularly **data efficient** whenever data is very similar\n",
    "- Stochastic Gradient Descent is not particularly **computationally efficient** since CPUs and GPUs cannot exploit the full power of **vectorization**\n",
    "- There might be a happy medium between **GD** and **SGD**\n",
    "\n",
    "### Vectorization\n",
    "- Vectorization is a basic method to increase computational efficiency\n",
    "    - Due to reduced overhead arising from the deep learning framework and due to better memory locality and caching on CPUs and GPUs\n",
    "- Use vectorization to replace **loops**, which usually leads high computation cost\n",
    "- It is highly advisable to use vectorization and matrices whenever possible\n",
    "- Consider matrix-matrix multiplication $\\mathbf{A}=\\mathbf{BC}$. Common methods to compute $\\mathbf{A}$ are:\n",
    "    - $\\mathbf{A}_{i,j}=\\mathbf{B}_{i,:}\\mathbf{C}^T_{:,j}$, compute it element-wise by means of dot product. In this way, we will need to copy one row and one column vector into the CPU each time we want to compute an element $\\mathbf{A}_{i,j}$. In the meantime, we are required to access many disjoint locations for one of the two vectors as we read them from memory due to the fact that the matrix elements are aligned sequentially.\n",
    "    - $\\mathbf{A}_{:,j}=\\mathbf{BC}^T_{:,j}$, compute one column at a time. We are able to keep the column vector $\\mathbf{C}_{:,j}$ in the CPU cache while we keep on traversing through $\\mathbf{B}$ \n",
    "    - $\\mathbf{A}=\\mathbf{BC}$ directly. Most desirable, but most matrices might not entirely fit into cache\n",
    "    - Break $\\mathbf{B}$ and $\\mathbf{C}$ into smaller block matrices and compute $\\mathbf{A}$ one block at a time. Offers a practically useful alternative: move blocks of the matrix into cache and multiply them locally\n",
    "- Element-wise computation $\\mathbf{A}_{i,j}=\\mathbf{B}_{i,:}\\mathbf{C}^T_{:,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f66b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise computation: 280.250549 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# Construct B,C\n",
    "A = np.zeros((256,256))\n",
    "B = np.random.normal(0,1,(256,256))\n",
    "C = np.random.normal(0,1,(256,256))\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Element-wise computation\n",
    "for ii in range(256):\n",
    "    for jj in range(256):\n",
    "        A[:,jj]=np.dot(B[ii,:],C[:,jj])\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "# Show the time cost\n",
    "print(\"Element-wise computation: %f ms\"%(1000*(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05111f",
   "metadata": {},
   "source": [
    "- Perform column-wise computation $\\mathbf{A}_{:,j}=\\mathbf{BC}^T_{:,j}$ is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b494e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise computation: 19.951344 ms\n"
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Column-wise computation\n",
    "for jj in range(256):\n",
    "    A[:,jj]=np.dot(B,C[:,jj])\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "# Show the time cost\n",
    "print(\"Column-wise computation: %f ms\"%(1000*(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee573f",
   "metadata": {},
   "source": [
    "- The most effective manner is to perform the entire operation in one block $\\mathbf{A}=\\mathbf{BC}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcb1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct computation: 0.996590 ms\n"
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Column-wise computation\n",
    "A=np.dot(B,C)\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "# Show the time cost\n",
    "print(\"Direct computation: %f ms\"%(1000*(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec5e7c",
   "metadata": {},
   "source": [
    "### Minibatches\n",
    "- Processing single observations requires us to perform many single matrix-vector (or even vector-vector) multiplications, which is quite expensive and which incurs a significant overhead on behalf of the underlying deep learning framework. This applies whenever we perform $$\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta_t\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\mathbf{x}_t)$$\n",
    "- Taking the **vectorization** method into consideration, we can increase the **computational efficiency** of this operation by applying it to a minibatch of observations at a time. That is to say, approximate the full gradient by the gradient of a batch of samples $$\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta_t\\underset{k=1}{\\overset{b}{\\Sigma}}\\frac{1}{b}\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\mathbf{x}_k)$$\n",
    "- Since both $\\mathbf{x}_t$ and also all elements of the minibatch are drawn uniformly at random from the training set, the **expectation** of the gradient remains unchanged\n",
    "- The variance is reduced significantly (relative to SGD)\n",
    "    - The standard deviation is reduced by a factor of $b^{-\\frac{1}{2}}$, which means the updates are more reliably aligned with the full gradient\n",
    "- Naively it indicates that choosing a large minibatch $b$ would be universally desirable while the additional reduction in standard deviation in minimal after some point when compared to the linear increase in computational cost\n",
    "    - In practice, usually pick a minibatch that is large enough to offer good computational efficiency while still fitting into the memory of a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6ecdc",
   "metadata": {},
   "source": [
    "### Minibatch SGD Summary\n",
    "- **Performance**\n",
    "    - In general, minibatch stochastic gradient descent is faster than stochastic gradient descent and gradient descent for convergence to a smaller risk, when measures in terms of clock time\n",
    "    - Balance the trade-off between **statistical efficiency** arising from stochastic gradient descent and **computational efficiency** arising from processing large batches of data at a time\n",
    "- **Batch size**\n",
    "    - Large batches: less *noise* in gradient\n",
    "        - Disadvantages: Slower updates per iteration; less exploration\n",
    "        - Advantages: Better local convergence\n",
    "    - Smaller batches: more *noise* in gradient\n",
    "        - Disadvantages: Hunts around local minimum\n",
    "        - Advantages: Faster updates per iteration; better exploration\n",
    "- **Patch size**:\n",
    "    - Many algorithms train on image *patches*\n",
    "    - Smaller patches might fit better into GPU cache\n",
    "    - Whether smaller patches speed training is apocryphal\n",
    "- **Learning rate**:\n",
    "    - Too large learning rate leads to oscillations around local minimum\n",
    "    - Too small learning rate leads to convergence\n",
    "    - It is advisable to use **adaptive learning rate** and decay the learning rates during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540db3c",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "### Motivation\n",
    "When performing optimization where only a noisy variant of the gradient is available, we need to be extra cautious when it comes to choosing the learning rate in the face of noise. If we decrease it too rapidly, convergence stalls. If we are too lenient, we fail to converge to a good enough solution since noise keeps on driving us away from optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cba0e",
   "metadata": {},
   "source": [
    "### Leaky Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6f598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
