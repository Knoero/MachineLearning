{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c44d4d",
   "metadata": {},
   "source": [
    "**Information:** *A brief review of Linear Algebra needed in Machine Learning.*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last updated date:** *Oct.15.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a0d4c",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "## Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "### Scalars\n",
    "- Single number\n",
    "- Denoted as italic lowercase letter such as $a$, $b$, $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2731e",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "- Array of numbers\n",
    "- Usually consider vectors to be \"column vectors\"\n",
    "- Denoted as lowercase letter (often bolded)\n",
    "    - $\\textbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\x_d \\end{bmatrix}$\n",
    "- Dimension is often denoted by $d$, $D$, or $p$\n",
    "    - $\\textbf{x} \\in \\mathbb{R}^d$\n",
    "- Access elements via subscript\n",
    "    - $x_i$ is the $i$-th element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885d1eb",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "- 2D array of numbers\n",
    "- Denoted as uppercase letter (often bolded)\n",
    "    - $\\mathbf{A}=\\begin{bmatrix}A_{1,1} & \\cdots & A_{1,n}\\\\\\vdots & \\ddots & \\vdots \\\\A_{m,1} & \\cdots & A_{m,n}\\end{bmatrix}$\n",
    "- Dimension is often denoted by $m\\times n$\n",
    "    - $\\textbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "- Access elements by double subscript \n",
    "    - $X_{i,j}$ or $x_{i,j}$ is the $i,j$-th entry of the matrix\n",
    "- Access rows or columns via subscript or numpy notation\n",
    "    - $X_{i,:}$ is the $i$-th row, $X_{:,j}$ is the $j$-th column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422121fa",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "- n-D array, array with more than two axes\n",
    "    - $\\textbf{A}\\in \\mathbb{R}^{c\\times w\\times h}$\n",
    "- Other notations are similar with Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022fa0c",
   "metadata": {},
   "source": [
    "### Addition of matrices, scalar multiplication and addition\n",
    "- When $\\textbf{A}=[A_{i,j}]$ and $\\textbf{B}=[B_{i,j}]$ have the same shape, the sum of them is written as $\\textbf{C}=\\textbf{A}+\\textbf{B}$ where $C_{i,j}=A_{i,j}+B_{i,j}$.\n",
    "    - In general, matrices of different sizes cannot be added.\n",
    "    - However, in the context of Deep Learning, notations like $\\textbf{C}=\\textbf{A}+\\textbf{b}$ is allowed where $C_{i,j}=A_{i,j}+b_{j}$, which means the vector $\\mathbf{b}$ is added to each row of the matrix. This is to avoid the need to define a matrix with $\\mathbf{b}$ copied into each row before doing the addition, This implicit copying is called **broadcasting**.\n",
    "- The product of any $m\\times n$ matrix $\\mathbf{A}=[A_{i,j}]$ and any scalar $c$ is written as $\\mathbf{C}=c\\mathbf{A}$ where $C_{i,j}=c\\cdot A_{i,j}$.\n",
    "- Similarly, the addition of any $m\\times n$ matrix $\\mathbf{A}=[A_{i,j}]$ and any scalar $b$ is written as $\\mathbf{C}=\\mathbf{A}+b$ where $C_{i,j}=A_{i,j}+b$.\n",
    "- Common calculation rules\n",
    "    - $\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}$\n",
    "    - $(\\mathbf{A}+\\mathbf{B})+\\mathbf{C}=\\mathbf{A}+(\\mathbf{B}+\\mathbf{C})$\n",
    "    - $c(\\mathbf{A}+\\mathbf{B})=c\\mathbf{A}+c\\mathbf{B}$\n",
    "    - $(c+k)\\mathbf{A}=c\\mathbf{A}+k\\mathbf{A}$\n",
    "    - $c(k\\mathbf{A})=ck\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3a557",
   "metadata": {},
   "source": [
    "### Multiplication (Standard Product)\n",
    "- The product $\\mathbf{C}=\\mathbf{A}\\mathbf{B}$ of an $m\\times n_1$ matrix $\\mathbf{A}=[A_{i,j}]$ times an $n_2\\times p$ matrix $\\mathbf{B}=[B_{i,j}]$ is defined if and only if $n_1=n_2$ and then $\\mathbf{C}$ will be an $m\\times p$ matrix $\\mathbf{C}$ with entries\n",
    "$$C_{i,j}=\\overset{n}{\\underset{k}{\\Sigma}}A_{i,k}B{k,j}$$\n",
    "- Called standard product or matrix product.\n",
    "- Common calculation rules\n",
    "    - $(k\\mathbf{A})\\mathbf{B}=k(\\mathbf{A}\\mathbf{B})=\\mathbf{A}(k\\mathbf{B})$\n",
    "    - $\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}$\n",
    "    - $(\\mathbf{A}+\\mathbf{B})\\mathbf{C}=\\mathbf{A}\\mathbf{C}+\\mathbf{B}\\mathbf{C}$\n",
    "    - $\\mathbf{C}(\\mathbf{A}+\\mathbf{B})=\\mathbf{C}\\mathbf{A}+\\mathbf{C}\\mathbf{B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd499984",
   "metadata": {},
   "source": [
    "### Element-wise product\n",
    "- A matrix containing the product of the individual elements from two matrix have the same size.\n",
    "- Denoted by $\\mathbf{C}=\\mathbf{A}\\odot\\mathbf{B}$ where $C_{i,j}=A_{i,j}\\cdot B_{i,j}$\n",
    "- Also called Hadamard product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fd19b",
   "metadata": {},
   "source": [
    "### Transposition of Matrices and Vectors\n",
    "- Denoted as $\\mathbf{A}^T$\n",
    "- The transpose of an $m\\times n$ matrix $\\mathbf{A}=[A_{i,j}]$ is the $n\\times m$ matrix $\\mathbf{A}^T$ that has the first row of $\\mathbf{A}$ as its first column, the second row as its second column, and so on.\n",
    "    - $\\mathbf{A}^T=[A_{j,i}]=\\begin{bmatrix}A_{1,1} & \\cdots & A_{m,1}\\\\\\vdots & \\ddots & \\vdots \\\\A_{1,n} & \\cdots & A_{m,n}\\end{bmatrix}$\n",
    "- For vector $\\mathbf{v}$, the transpose changes it from a column vector to a row vector.\n",
    "    - $\\mathbf{x}=\\begin{bmatrix}x_1 \\\\x_2 \\\\\\vdots \\\\x_d\\end{bmatrix}$, \n",
    "    $\\mathbf{x}^T=\\begin{bmatrix}x_1 & x_2 & \\cdots & x_d\\end{bmatrix}$\n",
    "- Rules for transposition\n",
    "    - $(\\mathbf{A}^T)^T=\\mathbf{A}$\n",
    "    - $(\\mathbf{A}+\\mathbf{B})^T=\\mathbf{A}^T+\\mathbf{B}^T$\n",
    "    - $(c\\mathbf{A})^T=c\\mathbf{A}^T$\n",
    "    - $(\\mathbf{A}\\mathbf{B})^T=\\mathbf{B}^T\\mathbf{A}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3d3f",
   "metadata": {},
   "source": [
    "### Special Matrices\n",
    "- Symmetric matrix: $\\mathbf{A}^T=\\mathbf{A},A_{i,j}=A_{j,i}$\n",
    "- Skew-symmetric matrix: $\\mathbf{A}^T=-\\mathbf{A}$\n",
    "- Triangular matrix:\n",
    "    - Upper triangular matrix can have non-zero entries only **on and above** the diagonal\n",
    "    - Lower triangular matrix can have non-zero entries only **on and below** the diagonal\n",
    "- Identity matrix:\n",
    "    - Identity matrix of size $n$ is the $n\\times n$ square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by $\\mathbf{I}_n$ or simply by $\\mathbf{I}$ if the size is immaterial or can be trivially determined by the context.\n",
    "    - Some times called unit matrix (depends on the context).\n",
    "- Scalar matrix:\n",
    "    - Any multiple of an Identity matrix.\n",
    "- Diagonal matrix:\n",
    "    - A square matrix in which the entries outside the diagonal are all zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d5129",
   "metadata": {},
   "source": [
    "## Linear System of equations\n",
    "### Represent linear set of equations in matrix equations\n",
    "- Linear set of equations can be compactly represented as matrix equation\n",
    "- In general:\n",
    "    $$\\begin{aligned}\n",
    "    a_{1,1}x_1+a_{1,2}x_2+ &\\cdots+ a_{1,n}x_n=b_1\\\\\n",
    "    a_{2,1}x_1+a_{2,2}x_2+ &\\cdots+ a_{2,n}x_n=b_2\\\\\n",
    "    &\\vdots\\\\\n",
    "    a_{m,1}x_1+a_{m,2}x_2+ &\\cdots+ a_{m,n}x_n=b_m\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    is **equivalent** to:\n",
    "    $$\\mathbf{Ax}=\\mathbf{b}$$\n",
    "    where $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$,$\\mathbf{x}\\in \\mathbb{R}^n$,$\\mathbf{b}\\in \\mathbb{R}^m$\n",
    "- Augmented matrix\n",
    "    $$\\tilde{\\mathbf{A}}=[\\mathbf{A},\\mathbf{b}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfd5e2",
   "metadata": {},
   "source": [
    "### Gaussian Elimination\n",
    "- **Goal**: Bring system to a triangular form\n",
    "- **Step**: \n",
    "    - Elementary operations on equations $\\longleftrightarrow$ Operation on matrices\n",
    "    - Interchange of two equations $\\longleftrightarrow$ Interchange two rows in a matrix\n",
    "    - Addition of a constant $\\longleftrightarrow$ Addition of a constant\n",
    "- Row equivalent\n",
    "    - We call a linear system $S_1$ row-equivalent to a linear system $S_2$ if $S_1$ can be obtained by finitely many row operations from $S_2$.\n",
    "- Theorem\n",
    "    - Row-equivalent linear systems have the same set of solutions.\n",
    "- Solution by Gaussian Elimination\n",
    "    - System:\n",
    "        - $\\mathbf{Ax}=\\mathbf{b}$ with augmented matrix $\\tilde{\\mathbf{A}}=[\\mathbf{A},\\mathbf{b}]$\n",
    "        - $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$,$\\mathbf{x}\\in \\mathbb{R}^n$,$\\mathbf{b}\\in \\mathbb{R}^m$\n",
    "    - Step 1: \n",
    "        - Pivot row: First row of $\\tilde{\\mathbf{A}}$\n",
    "        - Pivot: Coefficient of the $x_1$ term in pivot row\n",
    "        - Use pivot row to eliminate $x_1$ term in all other rows below\n",
    "    - Step 2:\n",
    "        - First equation remains as it is\n",
    "        - Pivot row: Second row of $\\tilde{\\mathbf{A}}$\n",
    "        - Pivot: Coefficient of the $x_2$ term in pivot row\n",
    "        - Use pivot row to eliminate $x_2$ term in all other rows below\n",
    "    - Step 3:\n",
    "        - Repeat the procedure which moves the pivot row from $s$ to $s+1$ and set pivot to be the coefficient of $x_{s+1}$ term in pivot row in each step, until $\\mathbf{A}$ is in upper triangular form\n",
    "    - Step 4: \n",
    "        - Back-substitution to get $x_n$, $x_{n-1}$, ..., $x_2$, $x_1$ sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7f1a5",
   "metadata": {},
   "source": [
    "### Classification of solutions of Linear Systems\n",
    "- At the end of Gaussian elimination, $\\mathbf{A}$ is in upper triangular form (row echelon form)\n",
    "    - $r$ = number of non-zero rows in $\\tilde{\\mathbf{A}}$ = **rank** of $\\tilde{\\mathbf{A}}$, $r \\le m$\n",
    "- In general, three possible cases\n",
    "    - Consistent if $r=m$ or $r<m$ but $\\tilde{b}_{r+1}$, ..., $\\tilde{b}_{m}$ are all zero\n",
    "        - One unique solution if consistent and $r=n$\n",
    "        - Infinite many solution if consistent and $r<n$. In this case, choose $x_{r+1}$, ...,$x_n$ arbitrarily.\n",
    "    - Inconsistent if $r<m$ and at least one of $\\tilde{b}_{r+1}$, ..., $\\tilde{b}_{m}$ is non-zero\n",
    "        - No solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295b564",
   "metadata": {},
   "source": [
    "## Linear Independence, Rank of Matrix, Vector Space\n",
    "### Linear Independence\n",
    "- Given: Set of vectors {$\\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\cdots,\\mathbf{v}^{(n)}$}\n",
    "- With $c_1,c_2,\\cdots,c_n$ are scalars, a linear combination of these vectors is of the form:\n",
    "$$c_1\\mathbf{v}^{(1)}+c_2\\mathbf{v}^{(2)}+\\cdots+c_n\\mathbf{v}^{(n)}$$\n",
    "- Consider $c_1\\mathbf{v}^{(1)}+c_2\\mathbf{v}^{(2)}+\\cdots+c_n\\mathbf{v}^{(n)}=0$ true for $c_1=c_2=\\cdots=c_n=0$\n",
    "    - If this is the only solution:**This set of vectors form a linear independent set**\n",
    "    - Otherwise: **Linear Dependent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c120d",
   "metadata": {},
   "source": [
    "### Rank of a matrix\n",
    "- The rank of a matrix $\\mathbf{A}$ is the number of linearly independent row vectors of $\\mathbf{A}$,\n",
    "- Denoted by **rank A**\n",
    "- Determine the rank of a matrix\n",
    "    - Observation: Number of linearly independent row vectors does not change by elementary row operations\n",
    "    - **Theorem 1**: \n",
    "        - Row equivalent matrices have the same rank\n",
    "    - Strategy: Reduce the matrix to row-echelon form (upper triangular form) and read off the rank directly\n",
    "    - **Theorem 2**: \n",
    "        - $p$ vectors with $n$ components each are independent if the matrix with these vectors as row vectors has rank $p$, but linearly dependent if that rank is less than $p$\n",
    "    - **Theorem 3**: \n",
    "        - The rank of a matrix $\\mathbf{A}$ equals the maximum number of linearly independent column vectors of $\\mathbf{A}$. Hence $\\mathbf{A}$ and its transpose $\\mathbf{A}^T$ have the same rank\n",
    "    - **Theorem 4**:\n",
    "        - $p$ vectors with $n<p$ components are always linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fc767",
   "metadata": {},
   "source": [
    "### Vector Space\n",
    "- **Vector Space**:\n",
    "    - Denoted by $V$\n",
    "    - Also called a **linear space**\n",
    "    - Nonempty set of vectors with the same number of components such that with any two vectors $\\mathbf{a}$ and $\\mathbf{b}$, all linear combinations $\\alpha\\mathbf{a}+\\beta\\mathbf{b}$ ($\\alpha,\\beta$ are real numbers) are elements of $V$ and these vectors satisfy the rules for vector addition and scalar multiplication.\n",
    "- **Dimension** of $V$:\n",
    "    - Maximal number of linearly independent vectors\n",
    "- **Basis**:\n",
    "    - Linear independent set of maximally possible vectors\n",
    "    - Number of vectors in the basis = dim $V$\n",
    "- **Span**:\n",
    "    - Set of all linear combinations given vectors $\\mathbf{a_1},\\mathbf{a_2},\\cdots,\\mathbf{a_p}$\n",
    "- **Subspace**:\n",
    "    - Nonempty set of vectors which forms itself a vector space with respect to addition and scalar multiplication\n",
    "- **Theorem 5**:\n",
    "    - The vector space $\\mathbb{R}^n$ consisting of all vectors with $n$ components (real) has dimension $n$ \n",
    "- **Theorem 6**:\n",
    "    - The row space and the column space of a matrix $\\mathbf{A}$ have the same dimension, equal to rank $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae0a24",
   "metadata": {},
   "source": [
    "## Solution of linear systems: Existence, Uniqueness\n",
    "### Submatrix of a matrix $\\mathbf{A}$\n",
    "- Any matrix obtained from $\\mathbf{A}$ by omitting some rows or columns\n",
    "\n",
    "### Theorems for linear systems(homogeneous systems)\n",
    "- **Homogeneous systems**\n",
    "    - A linear system of $m$ equations and $n$ unknowns in the form $$\\mathbf{Ax}=0$$\n",
    "    where $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$, $\\mathbf{x}\\in \\mathbb{R}^n$\n",
    "- Always has the trivial solution $\\mathbf{x}=0$\n",
    "- Nontrivial solutions exist if and only if rank $\\mathbf{A}=r<n$ \n",
    "- If $r<n$, the solution, together with $\\mathbf{x}=0$, form a vector space of dimension $n-r$, called the solution space of the system\n",
    "- In particular, if $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are solution vectors, so is $\\mathbf{x}=c_1\\mathbf{x}_1+c_2\\mathbf{x}_2$\n",
    "- Solution space of the system is called **Null Space**, $\\mathbf{Ax}=0$ for every $\\mathbf{x}$ from this solution space $N$\n",
    "    - dim $N$ = **Nullity**\n",
    "    - rank $\\mathbf{A}$ + nullity $\\mathbf{A}$ = $n$\n",
    "- A homogeneous system with fewer equations than unknowns always has non-trivial solution\n",
    "    - rank $\\mathbf{A}=r\\le m <n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba952ca",
   "metadata": {},
   "source": [
    "### Theorems for linear systems (non-homogeneous systems)\n",
    "- **Non-homogeneous systems**:\n",
    "    - A linear system of $m$ equations and $n$ unknowns in the form $$\\mathbf{Ax}=\\mathbf{b}$$\n",
    "    where $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$, $\\mathbf{x}\\in \\mathbb{R}^n$, $\\mathbf{b}\\in \\mathbb{R}^m$ and $\\mathbf{b}\\ne 0$\n",
    "- **Existence**:\n",
    "    - A non-homogeneous linear system is consistent (i.e. has solutions) if and only if the coefficient matrix $\\mathbf{A}$ and the augmented matrix $\\tilde{\\mathbf{A}}$ have the same rank.\n",
    "- **Uniqueness**:\n",
    "    - The system has precisely one solution if and only if the common rank $r$ of $\\mathbf{A}$ and $\\tilde{\\mathbf{A}}$ equals $n$\n",
    "- **Infinite many solutions**\n",
    "    - If this common rank is less than $n$, the system has infinitely many solutions. All the solutions can be obtained by determining $r$ unknowns in terms of the remaining $n-r$ unknowns.\n",
    "- **Solution**\n",
    "    - If a non-homogeneous system is consistent, then all the solutions are obtained as $\\mathbf{x}=\\mathbf{x}_o+\\mathbf{x}_h$\n",
    "        - $\\mathbf{x}_o$: Fixed solution of $\\mathbf{Ax}=\\mathbf{b}$\n",
    "        - $\\mathbf{x}_h$: Run through all solutions of $\\mathbf{Ax}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702017ac",
   "metadata": {},
   "source": [
    "## Determinants, Cramer's Rule\n",
    "### Determinant of order n\n",
    "- **Only defined for a square matrix**\n",
    "- $D=\\mathrm{det}\\mathbf{A}=\\begin{vmatrix}a_{1,1}&a_{1,2}&\\cdots&a_{1,n}\\\\a_{2,1}&a_{2,2}&\\cdots&a_{2,n}\\\\\\vdots&\\vdots&\\ddots&\\cdots\\\\a_{n,1}&a_{n,2}&\\cdots&a_{n,n}\\end{vmatrix}$\n",
    "- $n=1$, $D=a_1$\n",
    "- $n\\ge 2$,expand by $i-th$ rows ($i=1,2,\\cdots,n$)\n",
    "    - $D = a_{i,1}C_{i,1}+a_{i,2}C_{i,2}+\\cdots+a_{i,n}C_{i,n}$\n",
    "        - $C_{i,j}=(-1)^{i+j}M_{i,j}$\n",
    "        - $M_{i,j}$ is the determinant of order $n-1$, of a submatrix of $\\mathbf{A}$ obtained from $\\mathbf{A}$ by deleting the $i$-th row and the $j$-th column as indicated by the entry $a_{i,j}$\n",
    "    - $D = \\underset{j=1}{\\overset{n}{\\Sigma}}a_{i,j}C_{i,j}$ \n",
    "    - Or alternatively expand by $j-th$ column: $D = \\underset{i=1}{\\overset{n}{\\Sigma}}a_{i,j}C_{i,j}$ where $j=1,2,\\cdots,n$\n",
    "    - **Remark**: Easier for n upper triangular matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1bdb",
   "metadata": {},
   "source": [
    "### General properties of determinants\n",
    "- Behavior of $n$-th order determinant under elementary row operations\n",
    "    - Interchange of two rows or two columns multiplies the determinant by $-1$\n",
    "    - Addition of a multiple of one row/column to another row/column doesn't alter the value of the determinant\n",
    "    - Multiplication of a row/column by a constant $c$ multiplies the value of the determinant by $c$\n",
    "        - $\\mathrm{det}(c\\mathbf{A})=c^n\\mathrm{det}(\\mathbf{A})$\n",
    "        - $\\mathrm{det}(\\mathbf{A}^T)=\\mathrm{det}(\\mathbf{A})$\n",
    "        - $\\mathrm{det}(\\mathbf{AB})=\\mathrm{det}(\\mathbf{A})\\mathrm{det}(\\mathbf{B})$\n",
    "        - $\\mathrm{det}(\\mathbf{A+B})\\ne \\mathrm{det}(\\mathbf{A})+\\mathrm{det}(\\mathbf{B})$ (In general)\n",
    "    - Transposition leaves determinant the same\n",
    "    - A zero row or zero column renders the value of $\\mathrm{det}=0$\n",
    "    - Proportional rows or columns render the value of $\\mathrm{det}=0$\n",
    "- For practical purposes, to evaluate a determinant of $n$-th order:\n",
    "    - reduce the matrix to upper triangular form, which need to keep track of operations that change the determinant\n",
    "    - multiply the elements on the diagonal to calculate the determinant\n",
    "- Relationship between **Rank** and **Determinant**\n",
    "    - An $m\\times n$ matrix $\\mathbf{A}=[A_{i,j}]$ has rank $r\\ge 1$ if and only if it has an $r\\times r$ submatrix with non-zero determinant\n",
    "    - In particular, if $\\mathbf{A}$ is square with size $n\\times n$, it has $\\mathrm{rank}=n$ if and only if $\\mathrm{det}\\ne 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca04699",
   "metadata": {},
   "source": [
    "### Cramer's rule (Solution of linear system by determinants)\n",
    "- If a linear system of $n$ equations for $n$ unknowns:$$\\mathbf{Ax}=\\mathbf{b}$$\n",
    "    where $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, $\\mathbf{x}\\in \\mathbb{R}^n$, $\\mathbf{b}\\in \\mathbb{R}^n$ has non-zero coefficient determinant ($\\mathrm{det}(\\mathbf{A})=D\\ne 0$), it has precisely one solution\n",
    "- The solution is given by $x_1=\\frac{D_1}{D},x_2=\\frac{D_2}{D},\\cdots,x_n=\\frac{D_n}{D}$ where $D_k$ is the determinant of a matrix obtained from $\\mathbf{A}$ by replacing the $j$-th column by a column with entries $b_1,b_2,\\cdots,b_n$\n",
    "- If the system is homogeneous and $D\\ne 0$, it has only the trivial solution. If $D=0$, the system has non-trivial solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bad194",
   "metadata": {},
   "source": [
    "## Inverse of matrix, Gauss-Jordan eliminations\n",
    "### Inverse of matrix\n",
    "- Consider only square matrices\n",
    "- Inverse of an $n\\times n$ matrix $\\mathbf{A}=[A_{i,j}]$ is $\\mathbf{A}^{-1}$ such that:\n",
    "$$\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}_n$$\n",
    "- If $\\mathbf{A}$ has inverse: $\\mathbf{A}$ is non-singular, otherwise $\\mathbf{A}$ is singular\n",
    "    - Singular matrices are similar to zeros (similar to the idea that $0$ does not have an inverse)\n",
    "    - Called \"singular\" because a random matrix is unlikely to be singular just like choosing a random number is unlikely to be $0$\n",
    "- Motivation:\n",
    "    - $\\mathbf{Ax}=\\mathbf{b}\\Rightarrow \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}$ (usually not suitable for numerical calculation)\n",
    "- **Theorem**: Existence of $\\mathbf{A}^{-1}$\n",
    "    - The inverse $\\mathbf{A}^{-1}$ of an $n\\times n$ matrix $\\mathbf{A}$ exists if and only if the $\\mathrm{rank}\\mathbf{A}=n$, thus if and only if $\\mathrm{det}\\mathbf{A}\\ne 0$ \n",
    "- Formula for Inverse of $\\mathbf{A}$\n",
    "    - $\\mathbf{A}^{-1}=\\frac{1}{\\mathrm{det}\\mathbf{A}}[C_{i,j}]^T$\n",
    "    - $C_{i,j}=(-1)^{i+j}M_{i,j}$\n",
    "    - $M_{i,j}$ is the determinant of order $n-1$, of a submatrix of $\\mathbf{A}$ obtained from $\\mathbf{A}$ by deleting the $i$-th row and the $j$-th column as indicated by the entry $a_{i,j}$\n",
    "    - Usually used on only $2\\times 2$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046cb16",
   "metadata": {},
   "source": [
    "### Gauss-Jordan elimination\n",
    "- Method to find the inverse\n",
    "- Build an matrix $[\\mathbf{A}|\\mathbf{I}]$ containing $\\mathbf{A}$ and identity matrix $\\mathbf{I}$\n",
    "- Perform Gaussian elimination on $\\mathbf{A}$, but do the same steps on $\\mathbf{I}$, until get the result $[\\mathbf{I}|\\mathbf{B}]$. Thus, $\\mathbf{B}=\\mathbf{A}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3060b",
   "metadata": {},
   "source": [
    "## Norms\n",
    "### Definition\n",
    "- The \"size\" of a vector or matrix. \n",
    "- Intuitively,the norm of a vector $\\mathbf{x}$ measures the distance from the origin to the point $\\mathbf{a}$.\n",
    "- Functions mapping vectors or matrices to non-negative values\n",
    "- Formally, a norm is any function $f$ that satisfies the following properties:\n",
    "    - $f(\\mathbf{x})=0\\Rightarrow \\mathbf{x}=0$\n",
    "    - $f(\\mathbf{x}+\\mathbf{y})\\le f(\\mathbf{x})+f(\\mathbf{y})$ (the triangle inequality)\n",
    "    - $\\forall \\alpha \\in \\mathbb{R},f(\\alpha\\mathbf{x})=|\\alpha|f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71570f43",
   "metadata": {},
   "source": [
    "### $L^p$ norm\n",
    "- $||\\mathbf{x}||_p=\\left(\\underset{i}{\\Sigma}|x_i|^p\\right)^{\\frac{1}{p}}$\n",
    "- $p=2$: **Euclidean Norm**, used so frequently in machine learning that it is often denoted simply as $||\\mathbf{x}||$ with the subscript $2$ omitted. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $\\mathbf{x}^T\\mathbf{x}$\n",
    "    - In most machine learning cases, the squared $L^2$ norm is more convenient to work with mathematically and computationally than the $L^2$ norm itself. On example is that each derivative of the squared $L^2$ norm with respect to each element of $\\mathbf{x}$ depends only on the corresponding element of $\\mathbf{x}$.\n",
    "- $p=1$: commonly used in machine learning when the difference between zero and nonzero elements is very important. Every time an element of $\\mathbf{x}$ moves away from 0 by $\\epsilon$, the $L^1$ norm increases by $\\epsilon$\n",
    "    - Sometimes used to count the number of nonzero entries\n",
    "- $p=\\infty$:**Max Norm**, simplifies to the absolute value of the element with the largest magnitude in the vector \n",
    "    $$\n",
    "    ||\\mathbf{x}||_{\\infty}=\\underset{i}{\\mathrm{max}}|\\mathbf{x}_i|\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ed1db",
   "metadata": {},
   "source": [
    "### Frobenius Norm\n",
    "- Used to measure the size of a matrix\n",
    "- $||\\mathbf{A}||_{F}=\\sqrt{\\underset{i,j}{\\Sigma}A^2_{i,j}}$\n",
    "- Analogous to the $L^2$ norm of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43920d91",
   "metadata": {},
   "source": [
    "## Inner Product Space, Linear Transformations\n",
    "### Inner Product\n",
    "- A binary operation associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors, often denoted using angle brackets (as in ${\\displaystyle \\langle \\mathbf{a},\\mathbf{b}\\rangle}$).\n",
    "- **Dot product**: One widely used inner product on a finite dimensional Euclidean space. Apply for two vectors with the same length.\n",
    "    - $\\langle\\mathbf{a},\\mathbf{b}\\rangle=(\\mathbf{a},\\mathbf{b})=\\mathbf{a}\\bullet\\mathbf{b}=\\mathbf{a}^T\\mathbf{b}=\\underset{i=1}{\\overset{n}{\\Sigma}}a_ib_i$\n",
    "    - Two vectors $\\mathbf{a}, \\mathbf{b}$ are called **orthogonal** if $\\mathbf{a}\\bullet\\mathbf{b}=0$\n",
    "    - Can be written in terms of norms: $\\mathbf{a}^T\\mathbf{b}=||a||_2||b||_2\\mathrm{cos}\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fe199",
   "metadata": {},
   "source": [
    "### Abstract Real Inner Product Space\n",
    "- Real vector space $V$ is called real inner product space $V$ together with an inner product $(\\mathbf{a},\\mathbf{b})$ satisfying\n",
    "    - Linearity: $(q_1\\mathbf{a}+q_2\\mathbf{b},\\mathbf{c})=q_1(\\mathbf{a},\\mathbf{c})+q_2(\\mathbf{b},\\mathbf{c})$ where $\\mathbf{a},\\mathbf{b}\\in V, q_1,q_2\\in \\mathbb{R}$\n",
    "    - Symmetry: $(\\mathbf{a},\\mathbf{b})=(\\mathbf{b},\\mathbf{a})$\n",
    "    - Positive-definite: $(\\mathbf{a},\\mathbf{a})\\ge 0$, $(\\mathbf{a},\\mathbf{a})=0$ if and only if $\\mathbf{a}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835087c",
   "metadata": {},
   "source": [
    "### Linear Transformations\n",
    "- a mapping ${\\displaystyle X\\rightarrow Y}$ between two vector spaces that preserves the operations of vector addition and scalar multiplication\n",
    "- If $F$ is the mapping between $X$ and $Y$($F:X\\rightarrow Y,F(\\mathbf{x})=\\mathbf{y}$ where $\\mathbf{x}\\in X$ and $\\mathbf{y}\\in Y$), then $F$ is called a linear mapping/transformation if for all $\\mathbf{x},\\mathbf{x}_2\\in X$ and scalars $c$:\n",
    "    - $F(\\mathbf{x}+\\mathbf{x}_2)=F(\\mathbf{x})+F(\\mathbf{x}_2)$\n",
    "    - $F(c\\mathbf{x})=cF(\\mathbf{x})$\n",
    "- If $X=\\mathbb{R}^n$ and $Y=\\mathbb{R}^m$, any real matrix $\\mathbf{A}=[A_{i,j}]$ of size $m\\times n$ gives a linear transformation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathbf{y}=\\mathbf{A}\\mathbf{x}\\\\\n",
    "&\\mathbf{A}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\\\\n",
    "&\\mathbf{x}\\in \\mathbb{R}^n,\\mathbf{y}\\in\\mathbb{R}^m\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Conversely, every linear transformation $F:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ can be written in terms of an $m\\times n$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abed11",
   "metadata": {},
   "source": [
    "## Trace operator\n",
    "### Definition\n",
    "- Calculate the sum of all the diagonal entries of a matrix $$\\mathrm{Tr}\\left(\\mathbf{A}\\right)=\\underset{i}{\\Sigma}A_{i,i}$$\n",
    "\n",
    "### Properties\n",
    "- Provides an alternative way of writing the Frobenius norm of a matrix:$$||\\mathbf{A}||_F=\\sqrt{\\mathrm{Tr}\\left(\\mathbf{A}\\mathbf{A}^T\\right)}$$\n",
    "- Invariant to the transpose operator: $$\\mathrm{Tr}(\\mathbf{A})=\\mathrm{Tr}(\\mathbf{A}^T)$$\n",
    "- Rotational Equivalence: Invariant to the order of factors (if shapes of corresponding matrices allow changing the order):$$\\mathrm{Tr}(\\mathbf{ABC})=\\mathrm{Tr}(\\mathbf{CAB})=\\mathrm{Tr}(\\mathbf{BCA})$$\n",
    "    - Holds even if the resulting product has a different shape:$$\\mathrm{Tr}(\\mathbf{AB})=\\mathrm{Tr}(\\mathbf{BA})$$ where $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{B}\\in\\mathbb{R}^{n\\times m}$, which leads to $\\mathbf{AB}\\in\\mathbb{R}^{m\\times m}$ and $\\mathbf{BA}\\in\\mathbb{R}^{n\\times n}$\n",
    "- A scalar is its own trace: $a=\\mathrm{Tr}(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b731eb",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9074b",
   "metadata": {},
   "source": [
    "# Matrix Eigenvalue Problems\n",
    "## Determining Eigenvalues and Eigenvectors\n",
    "### Definition of eigenvalues and eigenvectors:\n",
    "- Let $\\mathbf{A}=[A_{i,j}]$ to be an $n\\times n$ matrix, then we say $\\mathbf{A}$ has an **eigenvector** $\\mathbf{v}$ corresponding to an **eigenvalue** $\\lambda$ if:$$\\begin{aligned}\\mathbf{Av}&=\\lambda\\mathbf{v}\\\\(\\mathbf{A}-\\lambda\\mathbf{I})\\mathbf{v}&=\\mathbf{0}\\\\ \\mathbf{v}&\\ne \\mathbf{0}\\end{aligned}$$ where $\\lambda\\in\\mathbb{R}$\n",
    "    - A non-trivial solution exists if and only if $\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})=0$, which gives a polynomial $p(\\lambda)$ called the **characteristic polynomial**\n",
    "    - Eigenvalues are the roots of the characteristic polynomial\n",
    "        - An $n\\times n$ matrix has at least one eigenvalue and at most $n$ numerically different eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fa11d",
   "metadata": {},
   "source": [
    "### Theorem: \n",
    "- The eigenvectors of a matrix $\\mathbf{A}$ corresponding to one and the same eigenvalue $\\lambda$ of $\\mathbf{A}$, together with $\\mathbf{0}$, form a vector space called the **eigenspace** of $\\mathbf{A}$\n",
    "- Eigenvectors are determined only up to a constant $\\Rightarrow$ can normalize to get a unit eigenvector\n",
    "    - $||\\mathbf{x}||=\\sqrt{\\underset{i=1}{\\overset{n}{\\Sigma}}x^2_i}$\n",
    "    - Unit eigenvector: $\\frac{\\mathbf{x}}{||\\mathbf{x}||}$\n",
    "- If $\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_k$ are eigenvectors corresponding to different eigenvalues, then $\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_k$ are linearly independent\n",
    "- If a matrix $\\mathbf{A}$ has $n$ different eigenvalues $\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$, there is a set of eigenvectors $\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n$ which are linearly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81efc5",
   "metadata": {},
   "source": [
    "### Remark\n",
    "- Only applies for square matrix and $\\mathbf{v}$ must be non-zero vector\n",
    "- If $\\mathbf{v}$ is an eigenvector, then so is any scaled vector $s\\mathbf{v}$ for $s\\in\\mathbb{R},s\\ne 0$ while sharing the same eigenvalue $\\lambda$. For this reason, we usually look only fro unit eigenvectors\n",
    "- Transpose of a square matrix $\\mathbf{A}$ has the same eigenvalues as $\\mathbf{A}$ but not necessarily same eigenvectors\n",
    "    - $\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})=0\\Rightarrow\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})^T=0\\Rightarrow\\mathrm{det}(\\mathbf{A}^T-\\lambda\\mathbf{I})=\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})^T=0$, which means the characteristic polynomials are same\n",
    "- For real matrices with complex eigenvalues, eigenvectors would come in complex conjugate pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677984fa",
   "metadata": {},
   "source": [
    "### Compute eigenvectors\n",
    "- Solve the characteristic polynomial for eigenvalues $\\lambda$\n",
    "- Solve the homogeneous system equation $(\\mathbf{A}-\\lambda\\mathbf{I})\\mathbf{v}=\\mathbf{0}$ for each eigenvalue\n",
    "- **Algebraic multiplicity $M_{\\lambda}$**: Order of an eigenvalue $\\lambda$ as a root in characteristic polynomial\n",
    "    - Sum of all algebraic multiplicity is $n$\n",
    "- **Geometric multiplicity $m_{\\lambda}$**: Number of linearly independent eigenvectors corresponding to $\\lambda$\n",
    "    - $m_{\\lambda}\\le M_{\\lambda} \\le n$\n",
    "- **Defect of $\\mathbf{A}$**: $\\Delta_{\\lambda}=M_{\\lambda}-m_{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015842f9",
   "metadata": {},
   "source": [
    "### Complex Matrices and Forms\n",
    "- Sometime the complex eigenvalues lead to application of complex matrices\n",
    "- Extend the concept of dot product to $n$-component vector with complex entries\n",
    "    - $\\mathbf{u},\\mathbf{v}\\in\\mathbb{C}^n,\\ (\\mathbf{u},\\mathbf{v})=\\bar{\\mathbf{u}}^T\\mathbf{v}$\n",
    "- Norm is still real\n",
    "    - $||\\mathbf{v}||=\\sqrt{\\bar{v}_1v_1+\\bar{v}_2v_2+\\cdots+\\bar{v}_nv_n}$\n",
    "- Extend symmetries\n",
    "    - Called **Hermitian** if $\\bar{\\mathbf{A}}^T=\\mathbf{A}$\n",
    "        - Real Hermitian matrix is Symmetric matrix\n",
    "    - Called **Skew-Hermitian** if $\\bar{\\mathbf{A}}^T=-\\mathbf{A}$\n",
    "        - Real Skew-Hermitian matrix is Skew-Symmetric matrix\n",
    "    - Called **Unitary** if $\\bar{\\mathbf{A}}^T=\\mathbf{A}^{-1}$\n",
    "        - Real Unitary matrix is Orthonormal matrix\n",
    "        - Determinant of a unitary matrix has absolute value $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fee56e",
   "metadata": {},
   "source": [
    "### Positive Definite Matrix\n",
    "- **Positive Definite**: A matrix whose eigenvalues are all positive is called positive definite\n",
    "- **Positive Semidefinite**: A matrix whose eigenvalues are all positive or zero value is called positive semidefinite\n",
    "- **Negative Definite**: A matrix whose eigenvalues are all negative is called negative definite\n",
    "- **Negative Semidefinite**: A matrix whose eigenvalues are all negative or zero value is called negative semidefinite\n",
    "- **Motivation**: \n",
    "    - A quadratic form in $\\mathbb{R}^n$ is an expression $\\mathbf{x}^T\\mathbf{A}\\mathbf{x}=\\underset{i,j=1}{\\overset{n}{\\Sigma}}A_{i,j}x_ix_j$ where $\\mathbf{x}\\in\\mathbb{R}^n$\n",
    "        - This can always be achieved by a symmetric matrix by replacing $A_{i,j}$ and $A_{j,i}$ by their average\n",
    "    - A positive semidefinite matrix guarantees that $\\forall\\mathbf{x},\\ \\mathbf{x}^T\\mathbf{Ax}\\ge 0$\n",
    "    - A positive definite matrix additionally guarantees that $\\mathbf{x}^T\\mathbf{A}\\mathbf{x}=0\\Rightarrow\\mathbf{x}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbc4c1",
   "metadata": {},
   "source": [
    "## Eigenvalues and eigenvectors of special matrices\n",
    "### Symmetric/Hermitian\n",
    "- For symmetric/hermitian square matrices $\\mathbf{A}=\\mathbf{A}^T$, the eigenvalues are always real\n",
    "- Symmetry matrices always have an orthogonal basis of eigenvectors for $\\mathbf{R}$^n\n",
    "- For hermitian matrices, eigenvectors corresponding to different eigenvalues are orthogonal.\n",
    "- Hermitian matrices always have a set of $n$ linearly independent eigenvectors, even if there're repeated roots\n",
    "\n",
    "### Skew-symmetric/Skew-hermitian\n",
    "- For skew-symmetric/skew-hermitian square matrices $\\mathbf{A}=-\\mathbf{A}^T$, the eigenvalues are always purely imaginary or zero.\n",
    "\n",
    "### Orthonormal/Unitary\n",
    "- **Orthogonal**: A real square matrix in which the row vectors (and also its column vectors) from an orthogonal system. \n",
    "    - $A_i\\bullet A_j=A_i^TA_j=0$ if $i \\ne j$\n",
    "    - $\\mathbf{A}^T\\mathbf{A}$ and $\\mathbf{A}\\mathbf{A}^T$ are diagonal matrices\n",
    "- **Orthonormal**: Orthogonal matrices with all the norms of row vectors and column vectors normalized to 1\n",
    "    - $A_i\\bullet A_j=A_i^TA_j=0$ if $i \\ne j$ and $A_i\\bullet A_j=A_i^TA_j=1$ if $i = j$\n",
    "    - $\\mathbf{A}^T\\mathbf{A}=\\mathbf{A}\\mathbf{A}^T=\\mathbf{I}$, which implies $\\mathbf{A}^{-1}=\\mathbf{A}^T$\n",
    "    - Determinant of an orthonormal matrix is always $+1$ or $-1$\n",
    "- For orthonormal/unitary matrices, the eigenvalues are either real or in complex conjugate pairs and always have absolute value 1\n",
    "- Unitary matrices always have a set of $n$ linearly independent eigenvectors, even if there're repeated roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d18bf9",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "### Motivation\n",
    "- To understand a matrix better by breaking it into constituent parts or finding some properties that are universal and not caused by the way to represent the matrix\n",
    "- Analagous to prime factorization of an integer, which allows us to determine whether things are divisible by other integers\n",
    "- Analagous to representing a signal in the time versus frequency domain, where both time and frequency domain represent the same object but are useful for different computations and derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0105ca",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "- If $\\mathbf{A}$ is an $n\\times n$ matrix and $\\mathbf{P}$ is an non-singular $n\\times n$ matrix. then $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ is called a similarity transformation of $\\mathbf{A}$ and the resulting matrix $\\hat{\\mathbf{A}}=\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ is called similar to $\\mathbf{A}$\n",
    "- If $\\hat{\\mathbf{A}}$ is similar to $\\mathbf{A}$, then $\\hat{\\mathbf{A}}$ and $\\mathbf{A}$ have the same eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f048a",
   "metadata": {},
   "source": [
    "### Diagonalization\n",
    "- If there is a matrix $\\mathbf{P}$ (non-singular) such that $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}$ where $\\mathbf{D}$ is diagonal, according to similarity, the resulting matrix $\\mathbf{D}$ would have the same eigenvalues as $\\mathbf{A}$. Therefore, $\\mathbf{D}$ has the eigenvalues of $\\mathbf{A}$ on diagonal and each $\\lambda_i$ would repeat as many times as its algebraic multiplicity.\n",
    "    - An $n\\times n$ matrix is diagonalizable $\\Leftrightarrow$ $\\mathbf{A}$ has $n$ linear independent eigenvectors.\n",
    "- In fact, the columns of $\\mathbf{P}$ are the eigenvectors of $\\mathbf{A}$\n",
    "    - Let the columns of $\\mathbf{P}$ be denoted by $\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n$:\n",
    "    $$\\begin{aligned}\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}&=\\mathbf{D}\\\\ \\mathbf{AP}&=\\mathbf{PD}\\\\ \\mathbf{A}\\left[\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n\\right]&=\\left[\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n\\right]\\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\ 0&\\lambda_2&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ 0& 0& \\cdots &\\lambda_n \\end{bmatrix}\\\\ \\mathbf{Av}_i&=\\lambda_i\\mathbf{v}_i\\end{aligned}$$ where $i=1,2,\\cdots,n$\n",
    "- General procedures to diagonalize a matrix $\\mathbf{A}$:\n",
    "    1. Find the roots of the characteristic polynomial $\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})$ to get the eigenvalues $\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$\n",
    "    2. Find the corresponding eigenvectors $\\left[\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n\\right]$ by solving the homogeneous equation $(\\mathbf{A}-\\lambda\\mathbf{I})\\mathbf{v}=\\mathbf{0}$ corresponding to each eigenvalue\n",
    "    3. Construct matrix $\\mathbf{P}=\\left[\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_n\\right]$ and $\\mathbf{D}=\\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\ 0&\\lambda_2&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ 0& 0& \\cdots &\\lambda_n \\end{bmatrix}$\n",
    "    4. Find the inverse of $\\mathbf{P}$ (sometimes optional for convenience)\n",
    "    5. Then the diagonalized form of $\\mathbf{A}$ is $\\mathbf{A}=\\mathbf{PDP}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169d559",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "### Motivation\n",
    "- A more **generally applicable** way to factorize a matrix into **singular vectors** and **singular values**, which reveals the same kind of information as the eigendecompostion does\n",
    "- Every real matrix (not necessarily to be square) has a singular value decomposition, but the same is not true of the eigenvalue decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0af95",
   "metadata": {},
   "source": [
    "### Definition\n",
    "- For a matrix $\\mathbf{A}$ to be decomposed, write it as a product of three matrices: $$\\mathbf{A}=\\mathbf{UDV}^T$$ where $\\mathbf{A}$ is an $m\\times n$ matrix, $\\mathbf{U}$ is defined to be an $m\\times m$ matrix, $\\mathbf{D}$ to be an $m\\times n$ matrix, and $\\mathbf{V}$ to be an $n\\times n$ matrix\n",
    "    - $\\mathbf{U}$ and $\\mathbf{V}$ are both defined to be orthonormal matrices\n",
    "    - $\\mathbf{D}$ is defined to be a diagonal matrix and is not necessarily square\n",
    "    - Elements along the diagonal of $\\mathbf{D}$ are known as the **singular values** of the matrix $\\mathbf{A}$\n",
    "    - Columns of $\\mathbf{U}$ are known as the **left-singular vectors**\n",
    "    - Columns of $\\mathbf{V}$ are known as the **right-singular vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56368b8d",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "- Left-singular vectors (columns of $\\mathbf{U}$) are the eigenvectors of $\\mathbf{AA}^T$\n",
    "- Right-singular vectors (columns of $\\mathbf{V}$) are the eigenvectors of $\\mathbf{A}^T\\mathbf{A}$\n",
    "- Nonzero singular values (diagonal elements of $\\mathbf{D}$) are the square roots of the eigenvalues of $\\mathbf{A}^T\\mathbf{A}$, which is the same as that of $\\mathbf{AA}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c345c",
   "metadata": {},
   "source": [
    "## Moore-Penrose Pseudoinverse\n",
    "### Motivation\n",
    "- Matrix inversion is not defined for matrices that are not square\n",
    "- Find a way to handle the situation where the system matrix is not square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df920e",
   "metadata": {},
   "source": [
    "### Definition\n",
    "- Formal Definition: $\\mathbf{A}^+=\\underset{\\alpha\\searrow 0}{\\mathrm{lim}}(\\mathbf{A}^T\\mathbf{A}+\\alpha\\mathbf{I})^{-1}\\mathbf{A}^T$\n",
    "- Practical computation: $\\mathbf{A}^+=\\mathbf{V}\\mathbf{D}^+\\mathbf{U}^T$\n",
    "    - $\\mathbf{U},\\mathbf{D},\\mathbf{V}$ are the singular value decomposition of $\\mathbf{A}$\n",
    "    - The pseudoinverse $\\mathbf{D}^+$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b683c",
   "metadata": {},
   "source": [
    "### Application\n",
    "- In the non-homogeneous system $\\mathbf{Ax}=\\mathbf{y}$, use this pseudoinverse to get the result $\\mathbf{x}=\\mathbf{A}^+\\mathbf{y}$\n",
    "    - If $\\mathbf{A}$ has more columns than rows, the result provides one of the many possible solutions. Specially, it provides the solution $\\mathbf{x}=\\mathbf{A}^+\\mathbf{y}$ with minimal Euclidean norm $||\\mathbf{x}||$ among all possible solutions\n",
    "    - If $\\mathbf{A}$ has more row than column, which means it is possible for there to be no solution, the result gives us the $\\mathbf{x}$ for which $\\mathbf{Ax}$ is as close as possible to $\\mathbf{y}$ in terms of Euclidean norm $||\\mathbf{Ax}-\\mathbf{y}||$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c2eb6",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d9177",
   "metadata": {},
   "source": [
    "# Derivatives with vectors and matrices\n",
    "##  Notation\n",
    "- **Matrix Notation** and **Tensor Index Notation** are two competing notational conventions which split the field of matrix calculus into two separate groups.\n",
    "    - The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector.\n",
    "    - **Matrix Notation** writes the derivative of a scalar with respect to a vector as a **row vector**, which is used throughout this notebook.\n",
    "- In **Matrix Notation**:\n",
    "    - A scalar is denoted with lowercase italic typeface\n",
    "    - A vector is denoted with a boldface lowercase letter\n",
    "    - A matrix is denoted with bold capital letters\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37385ee4",
   "metadata": {},
   "source": [
    "## Derivatives with vectors\n",
    "### Vector-by-scalar\n",
    "- The derivative of a vector $\\mathbf{y}=\\begin{bmatrix}y_1&y_2&\\cdots&y_m\\end{bmatrix}^T$, by a scalar $x$ is written as:\n",
    "$$\\frac{\\partial\\mathbf{y}}{\\partial x}=\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x}\\\\ \\frac{\\partial y_1}{\\partial x}\\\\ \\vdots \\\\ \\frac{\\partial y_n}{\\partial x}\\end{bmatrix}$$\n",
    "- Notice: Lay out according to $\\mathbf{y}$, which is called numerator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed4dd4",
   "metadata": {},
   "source": [
    "### Scalar-by-vector\n",
    "- The derivative of a scalar $y$ by a vector $\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_n\\end{bmatrix}^T$, is written as:\n",
    "$$\\frac{\\partial y}{\\partial \\mathbf{x}}=\\begin{bmatrix}\\frac{\\partial y}{\\partial x_1}&\\frac{\\partial y}{\\partial x_2}&\\cdots&\\frac{\\partial y}{\\partial x_n}\\end{bmatrix}$$\n",
    "- Notice: Lay out according to $\\mathbf{x}^T$, which is called numerator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747a15e",
   "metadata": {},
   "source": [
    "### Vector-by-vector\n",
    "- The derivative of a vector function $\\mathbf{y}=\\begin{bmatrix}y_1&y_2&\\cdots&y_m\\end{bmatrix}^T$, with respect to an input vector, $\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_n\\end{bmatrix}^T$, is written as:\n",
    "$$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1}&\\frac{\\partial y_1}{\\partial x_2}&\\cdots&\\frac{\\partial y_1}{\\partial x_n}\\\\ \\frac{\\partial y_2}{\\partial x_1}&\\frac{\\partial y_2}{\\partial x_2}&\\cdots&\\frac{\\partial y_2}{\\partial x_n}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ \\frac{\\partial y_m}{\\partial x_1}&\\frac{\\partial y_m}{\\partial x_2}&\\cdots&\\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{bmatrix}$$\n",
    "- Notice: Lay out according to $\\mathbf{y}$ and $\\mathbf{x}^T$, which is sometimes known as the **Jacobian formulation** and is called numerator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c1dfe",
   "metadata": {},
   "source": [
    "## Derivatives with matrices\n",
    "### Matrix-by-scalar\n",
    "- The derivative of a matrix function $\\mathbf{Y}$ by a scalar $x$ is known as the **tangent matrix** and is given by:\n",
    "$$\\frac{\\partial \\mathbf{Y}}{\\partial x}=\\begin{bmatrix}\\frac{\\partial y_{1,1}}{\\partial x}&\\frac{\\partial y_{1,2}}{\\partial x}&\\cdots&\\frac{\\partial y_{1,n}}{\\partial x}\\\\ \\frac{\\partial y_{2,1}}{\\partial x}&\\frac{\\partial y_{2,2}}{\\partial x}&\\cdots&\\frac{\\partial y_{2,n}}{\\partial x}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ \\frac{\\partial y_{m,1}}{\\partial x}&\\frac{\\partial y_{m,2}}{\\partial x}&\\cdots&\\frac{\\partial y_{m,n}}{\\partial x}\\\\ \\end{bmatrix}$$\n",
    "- Notice: Lay out according to $\\mathbf{Y}$, which is called numerator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc719f12",
   "metadata": {},
   "source": [
    "### Scalar-by-matrix\n",
    "- The derivative of a scalar $y$ function of a $p\\times q$ matrix $\\mathbf{X}$ of independent variables, with respect to the matrix $\\mathbf{X}$, is given by:\n",
    "$$\\frac{\\partial y}{\\mathbf{\\partial X}}=\\begin{bmatrix} \\frac{\\partial y}{\\partial x_{1,1}}&\\frac{\\partial y}{\\partial x_{2,1}}&\\cdots&\\frac{\\partial y}{\\partial x_{p,1}}\\\\ \\frac{\\partial y}{\\partial x_{1,2}}&\\frac{\\partial y}{\\partial x_{2,2}}&\\cdots&\\frac{\\partial y}{\\partial x_{p,2}}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ \\frac{\\partial y}{\\partial x_{1,q}}&\\frac{\\partial y}{\\partial x_{2,q}}&\\cdots&\\frac{\\partial y}{\\partial x_{p,q}} \\end{bmatrix}$$\n",
    "- Notice: Lay out according to $\\mathbf{X}^T$, which is called numerator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741e1c7",
   "metadata": {},
   "source": [
    "### Other matrix derivatives\n",
    "- Vectors by matrices, matrices by vectors, and matrices by matrices are not widely considered and a notation is not widely agreed upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b933ad7",
   "metadata": {},
   "source": [
    "## Identities often used in machine learning\n",
    "- For complete identities, refer to [Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus)\n",
    "\n",
    "### Vector-by-vector identities\n",
    "- $\\mathbf{A}$ is not a function of $\\mathbf{x}$: \n",
    "$$\\frac{\\partial \\mathbf{Ax}}{\\partial \\mathbf{x}}=\\mathbf{A}$$\n",
    "- $\\mathbf{A}$ is not a function of $\\mathbf{x}$: \n",
    "$$\\frac{\\partial \\mathbf{x}^T\\mathbf{A}}{\\partial \\mathbf{x}}=\\mathbf{A}^T$$\n",
    "- $v=v(\\mathbf{x}),\\mathbf{u}=\\mathbf{u}(\\mathbf{x})$: \n",
    "$$\\frac{\\partial v\\mathbf{u}}{\\partial \\mathbf{x}}=v\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}+\\mathbf{u}\\frac{\\partial v}{\\partial \\mathbf{x}}$$\n",
    "- $\\mathbf{A}$ is not a function of $\\mathbf{x}$, $\\mathbf{u}=\\mathbf{u}(\\mathbf{x})$:\n",
    "$$\\frac{\\partial \\mathbf{Au}}{\\partial \\mathbf{x}}=\\mathbf{A}\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$$\n",
    "- $\\mathbf{u}=\\mathbf{u}(\\mathbf{x})$: \n",
    "$$\\frac{\\partial\\mathbf{f}(\\mathbf{g}(\\mathbf{u}))}{\\partial\\mathbf{x}}=\\frac{\\partial \\mathbf{f}(\\mathbf{g})}{\\partial \\mathbf{g}}\\frac{\\partial\\mathbf{g}(\\mathbf{u})}{\\partial \\mathbf{u}}\\frac{\\partial \\mathbf{u}}{\\partial\\mathbf{x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07977fce",
   "metadata": {},
   "source": [
    "### Scalar-by-vector identities\n",
    "- $u=u(\\mathbf{x}),v=v(\\mathbf{x})$: \n",
    "$$\\frac{\\partial uv}{\\partial\\mathbf{x}}=u\\frac{\\partial v}{\\partial \\mathbf{x}}+v\\frac{\\partial u}{\\partial \\mathbf{x}}$$\n",
    "- $u=u(\\mathbf{x})$:\n",
    "$$\\frac{\\partial f(g(u))}{\\partial \\mathbf{x}}=\\frac{\\partial f(g)}{\\partial g}\\frac{\\partial g(u)}{\\partial u}\\frac{\\partial u}{\\partial \\mathbf{x}}$$\n",
    "- $\\mathbf{u}=\\mathbf{u}(\\mathbf{x}),\\mathbf{v}=\\mathbf{v}(\\mathbf{x})$:\n",
    "$$\\frac{\\partial (\\mathbf{u}\\bullet \\mathbf{v})}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{u}^T\\mathbf{v}}{\\partial \\mathbf{x}}=\\mathbf{u}^T\\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}}+\\mathbf{v}^T\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$$\n",
    "- $\\mathbf{a}$ is not a function of $\\mathbf{x}$: \n",
    "$$\\frac{\\partial (\\mathbf{a}\\bullet\\mathbf{x})}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{a}^T\\mathbf{x}}{\\partial \\mathbf{x}}=\\mathbf{a}^T$$\n",
    "- $\\mathbf{A}$ is not a function of $\\mathbf{x}$: \n",
    "$$\\frac{\\partial \\mathbf{x}^T\\mathbf{Ax}}{\\partial \\mathbf{x}}=\\mathbf{x}^T(\\mathbf{A}+\\mathbf{A}^T)$$\n",
    "- $\\mathbf{a},\\mathbf{b}$ are not functions of $\\mathbf{x}$: \n",
    "$$\\frac{\\partial\\mathbf{a}^T\\mathbf{xx}^T\\mathbf{b}}{\\partial\\mathbf{x}}=\\mathbf{x}^T(\\mathbf{ab}^T+\\mathbf{ba}^T)$$\n",
    "- $\\mathbf{A},\\mathbf{b},\\mathbf{C},\\mathbf{D},\\mathbf{e}$ are not functions of $\\mathbf{x}$:\n",
    "$$\\frac{\\partial (\\mathbf{Ax}+\\mathbf{b})^T\\mathbf{C}(\\mathbf{Dx}+\\mathbf{e})}{\\partial \\mathbf{x}}=(\\mathbf{Dx}+\\mathbf{e})^T\\mathbf{C}^T\\mathbf{A}+(\\mathbf{Ax}+\\mathbf{b})^T\\mathbf{CD}$$\n",
    "- $\\mathbf{a}$ is not a function of $\\mathbf{x}$:\n",
    "$$\\frac{\\partial ||\\mathbf{x}-\\mathbf{a}||}{\\partial \\mathbf{x}}=\\frac{(\\mathbf{x}-\\mathbf{a})^T}{||\\mathbf{x}-\\mathbf{a}||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf5950",
   "metadata": {},
   "source": [
    "### Vector-by-scalar identities\n",
    "- $\\mathbf{A}$ is not a function of $x$, $\\mathbf{u}=\\mathbf{u}(x)$:\n",
    "$$\\frac{\\partial\\mathbf{Au}}{\\partial x}=\\mathbf{A}\\frac{\\partial \\mathbf{u}}{\\partial x}$$\n",
    "- $\\mathbf{u}=\\mathbf{u}(x),\\mathbf{v}=\\mathbf{v}(x)$:\n",
    "$$\\frac{\\partial (\\mathbf{u}+\\mathbf{v})}{\\partial x}=\\frac{\\partial \\mathbf{u}}{\\partial x}+\\frac{\\partial \\mathbf{v}}{\\partial x}$$\n",
    "- $\\mathbf{u}=\\mathbf{u}(x)$:\n",
    "$$\\frac{\\partial \\mathbf{f(g(u))}}{\\partial x}=\\frac{\\partial \\mathbf{f(g)}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g(u)}}{\\partial \\mathbf{u}}\\frac{\\partial \\mathbf{u}}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a15800",
   "metadata": {},
   "source": [
    "### Scalar-by-matrix identities\n",
    "- $u=u(\\mathbf{X}),v=v(\\mathbf{X})$: \n",
    "$$\\frac{\\partial uv}{\\partial\\mathbf{X}}=u\\frac{\\partial v}{\\partial \\mathbf{X}}+v\\frac{\\partial u}{\\partial \\mathbf{X}}$$\n",
    "- $u=u(\\mathbf{X})$:\n",
    "$$\\frac{\\partial f(g(u))}{\\partial \\mathbf{X}}=\\frac{\\partial f(g)}{\\partial g}\\frac{\\partial g(u)}{\\partial u}\\frac{\\partial u}{\\partial \\mathbf{X}}$$\n",
    "- $\\mathbf{a}$ and $\\mathbf{b}$ are not function of $\\mathbf{X}$:\n",
    "$$\\frac{\\partial\\mathbf{a}^T\\mathbf{Xb}}{\\partial \\mathbf{X}}=\\mathbf{ba}^T$$\n",
    "- $\\mathbf{a}$ and $\\mathbf{b}$ are not function of $\\mathbf{X}$:\n",
    "$$\\frac{\\partial\\mathbf{a}^T\\mathbf{X}^T\\mathbf{b}}{\\partial \\mathbf{X}}=\\mathbf{ab}^T$$\n",
    "- $\\mathbf{a}$, $\\mathbf{b}$ and $\\mathbf{c}$ are not functions of $\\mathbf{X}$:\n",
    "$$\\frac{\\partial(\\mathbf{Xa}+\\mathbf{b})^T\\mathbf{C}(\\mathbf{Xa}+\\mathbf{b})}{\\partial \\mathbf{X}}=\\left(\\left(\\mathbf{C}+\\mathbf{C}^T\\right)\\left(\\mathbf{Xa}+\\mathbf{b}\\right)\\mathbf{a}^T\\right)^T$$\n",
    "- $\\mathbf{a}$, $\\mathbf{b}$ and $\\mathbf{c}$ are not functions of $\\mathbf{X}$:\n",
    "$$\\frac{\\partial(\\mathbf{Xa})^T\\mathbf{C}(\\mathbf{Xb})}{\\partial \\mathbf{X}}=\\left(\\mathbf{CXba}^T+\\mathbf{C}^T\\mathbf{Xab}^T\\right)^T$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
