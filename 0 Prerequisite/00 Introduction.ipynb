{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:** *Some basics concepts of Machine Learning.*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:**: *05.18.2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Artificial Intelligence, Machine Learning, Deep Learning\n",
    "\n",
    "## Artificial Intelligence:\n",
    "\n",
    "- A.I. is a very **broad** field.\n",
    "- Turing test:\n",
    "> 'A human tries to tell the difference between a human correspondent and a machine one. When they can't, according to Turing, the machine should be judged intelligent.'\n",
    "- In Oxford Dictionary \n",
    "> 'the theory and development of computers systems **able to perform tasks that normally require human intelligence**, such as visual perception, speech recognition, decision-making, and translation between languages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning:\n",
    "- A huge part in the concept of AI (Models, Algorithms, Theory) as well as useful tools for some AI applications (Computer Vision, Robotics).\n",
    "- **Train an algorithm to reproduce answers from data.**\n",
    "- Usually can be divided into **supervised learning** and **unsupervised learning**.\n",
    "- Supervised Learning: To estimate a function between input and output given only **input-output examples**.\n",
    "    - Examples: Regression, Classification, Filtering, ...\n",
    "- Unsupervised Learning: To **model the input data directly**, or in other words, understand the input data directly.\n",
    "    - Examples: Clustering, Dimensionality Reduction, Density Estimation, Generative models, ...\n",
    "    - Motivation: Labeling data is always expensive (supervised) while gathering raw data is cheap (unsupervised).\n",
    "    - Yann LeCun, Head of Facebook AI, 2016\n",
    "        > 'AI systems today do not possess \"common sense\", which humans and animals acquire by observing the world, acting in it, and understanding the physical constraints of it. Some of us see unsupervised learning as the key towards machines with common sense.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning:\n",
    "\n",
    "### What is Deep Learning?\n",
    "- A particular successful **Machine Learning method based on deep sequences of neural networks**.\n",
    "- Instead of handcrafting features in classical Machine Learning, Deep Learning let the deep model do all the feature engineering automatically.\n",
    "- Generally speaking, deep models are **sequential transformations learned from data**. If you use deep neural networks to define sequential transformations from input to output, then you do Deep Learning.\n",
    "\n",
    "### Successful Applications of Deep Learning\n",
    "- Any machine learning task:\n",
    "    - Regression, Classification, Clustering, Dimensionality reduction, Density estimation, Filtering, ...\n",
    "- Natural language processing\n",
    "- Reinforcement learning\n",
    "    - self-driving cars, playing Atari games, AlphaGo\n",
    "- Design of engineering systems\n",
    "\n",
    "### Why does Deep Learning work so well?\n",
    "- The universal approximation theorem (Cybenko, 1989)\n",
    "- Deep Learning can automatically learn **a hierarchy of representations of high-dimensional data**.\n",
    "- Advanced learning algorithms (back-propagation, stochastic gradient descent, etc.)\n",
    "- Advanced software (tensorflow, pytorch) and advanced hardware (GPU)\n",
    "\n",
    "### Problems of applying Deep Learning in engineering\n",
    "- Many approaches are **black boxes** with no guarantees on convergence and performance.\n",
    "- Deep Models would make errors if data are perturbed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General Machine Learning problems\n",
    "\n",
    "## Supervised Learning\n",
    "### Problem Definition\n",
    "- **Mathematical Representation**\n",
    "$$\n",
    "\\large{\n",
    "y=f_{gt}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "    - $x$: Input of the function\n",
    "        - Called features, attributes, covariates or variables\n",
    "        - Can be numeric, categorical, discrete or nominal\n",
    "        - Examples:\n",
    "            - [GRE scores, GPA, major]\n",
    "            - An image\n",
    "            - A sentence\n",
    "            - A $d$-dimentional vector of numbers\n",
    "    \n",
    "    - $y$: Output of the function\n",
    "        - Called output, response, target, or label\n",
    "        - Can be numeric, categorical, discrete or nominal\n",
    "            - The problem is known as Regression when output is numeric\n",
    "            - The problem is known as Classification when the output is categorical\n",
    "\n",
    "    - $f_{gt}$: Ground truth of the function to be estimated\n",
    "        - We know nothing about this function, such as its structure, type or basis functions.\n",
    "        - It stands for the true relationships between the input and output.\n",
    "\n",
    "- **Goal**: \n",
    "    - Capture the **features** of $f$ and make use of it, so that the ability to inferring corresponding $y$ from $x$ is obtained.\n",
    "\n",
    "### Machine Learning Approach\n",
    "- **Mathematical Representation**\n",
    "$$\n",
    "\\large{\n",
    "\\hat{y}=\\hat{f}_{\\theta}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "    - $x$: Input of the function.\n",
    "\n",
    "    - $\\hat{y}$: Output of the estimated function. Usually called predictions.\n",
    "\n",
    "    - $\\hat{f}_{\\theta}$: Estimations of the ground truth function. Consists of basic functions we're familiar with and can be tuned with the parameter $\\theta$.\n",
    "\n",
    "- **Approach Explanation**: \n",
    "\n",
    "    - Given a set of input-output pairs which is called **Training Set** and denoted by $\\mathcal{D}= \\{ (x_i,y_i) \\} ^{n}_{i=1}$, tune the parameter $\\theta$ to achieve the \"best\" or at least a \"good \"answer, which means trying to make the prediction/s $\\hat{y}$ as close as possible to the ground truth $y$\n",
    "        > \"Training set: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.\"\n",
    "\n",
    "- **Possible solution for function basis**: \n",
    "    - Support vector machines\n",
    "    - Radial basis functions\n",
    "    - Gaussian mixture functions\n",
    "    - Neural networks\n",
    "        - Very high capacity order\n",
    "        - Easy to train with modern computational tools\n",
    "        - Shallow neural networks: Use one easy-to-train layer\n",
    "        - Deep neural networks: Train a hierarchical stack of layers.\n",
    "    - ...\n",
    "\n",
    "- **How to define the \"best performance\"?**\n",
    "    - Measure the \"distance\" between ground truth $y$ and estimation $\\hat{y}=\\hat{f}_{\\theta}(x)$\n",
    "    - The function to measure this \"distance\" is usually called **loss function** and is denoted by $l(\\theta)$\n",
    "    - Largely depends on specific tasks and the output type. The required performance should be guaranteed when the loss is minimized.\n",
    "        - Regression: Mean Square Error\n",
    "        - Classification: Cross-Entropy Loss\n",
    "        - Localization: Intersection over Union\n",
    "        - ...\n",
    "\n",
    "- **How to determine the best value of $\\theta$?**\n",
    "    - Select $\\theta$ which minimizes the loss function\n",
    "    - Mathematically speaking: $\\theta^*=\\mathrm{arg} \\underset{\\theta}{\\mathrm{min}}\\{ l(\\theta)\\}$\n",
    "    - Finding this best parameter $\\theta^*$ is completed with the help of modern optimization algorithms such as gradient descent and expectation maximization.\n",
    "\n",
    "- **How to optimize the options set manually when building the structure of estimation function?**\n",
    "    - Given another set of input-output pairs which is usually called **Validation Set**, test the performance of the learned model ($\\hat{y}=\\hat{f}_{\\theta^*}(x)$) with different options and set the option to the one has the best performance.\n",
    "        > \"Validation set: A set of examples used to tune the parameters (i.e., architecture, not weights) of a classifier, for example to choose the number of hidden units in a neural network.\"\n",
    "\n",
    "- **How to measure the final performance of the estimation?**\n",
    "    - Given another set of input-output pairs which is usually called **Test Set**, check the performance either by watching the visualized results or calculate the loss function on this data set.\n",
    "        > \"Test Set: A set of examples used only to assess the performance (generalization) of a fully specified classifier.\"\n",
    "\n",
    "- **Supervised Learning Approach in summary**\n",
    "    1. Collect train set which contains input-output pairs\n",
    "    2. Determine the function basis (model structure)\n",
    "    3. Determine the loss function needs to be optimized\n",
    "    4. Train the model by finding out the best $\\theta$ which minimized the loss function\n",
    "    5. Train the model under different hyper-parameters set manually and select the best hyper-parameters based on its performance on validation set\n",
    "    6. Test such a well specific model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "### Problem Definition\n",
    "- In supervised learning, the objective is the mappings between input data $x$ and output (or label) $y$. However, in unsupervised learning, the input data do not have a label while some patterns or features are supposed to be implicitly or explicitly included in the data set. Then the objective is to get the ability of **finding out these patterns or features** without any prior information.\n",
    "\n",
    "### Machine Learning Approach\n",
    "- In unsupervised learning, the **training set** is only a set of input values $\\mathcal{D}= \\{ (x_i) \\} ^{n}_{i=1}$. Thus, in my understanding, common unsupervised learning approach do not share similar procedures (PCA, clustering, generating). It depends on what specific features or results we want to get even though sometimes the same optimization algorithms in supervised learning are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# A single layer Neural Network\n",
    "- **General definition**\n",
    "$$\n",
    "\\large{\n",
    "\\hat{y}=B\\sigma(Ax+b)\n",
    "}\n",
    "$$\n",
    "    - $A$: A matrix of multiplicative weights\n",
    "\n",
    "    - $b$: A column vector of additive offsets\n",
    "\n",
    "    - $\\sigma$: A point-wise activation function\n",
    "\n",
    "    - $B$: A matrix of multiplicative weights\n",
    "\n",
    "- **Why does this structure work?**\n",
    "    - [Universal Approximation Theorem (Cybenko,1989)](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "        > Fix a continuous function $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ (activation function) and positive integers $d$, $D$. The function $\\sigma$ is not a polynomial if and only if, for every continuous function $f:\\mathbb{R}^d \\to \\mathbb{R}^D$ (target function), every compact subset $K$ of $\\mathbb{R}^d$, and every $\\epsilon \\gt0$ there exists a continuous function $f_{\\epsilon}:\\mathbb{R}^d \\to \\mathbb{R}^D$ (the layer output) with representation\n",
    "    $$f_{\\epsilon}=W_2\\circ \\sigma \\circ W_1$$\n",
    "    where $W_2$, $W_1$ are composable affine maps and $\\circ$ denotes component-wise composition, such that the approximation bound\n",
    "    $$\\underset{x \\in K}{\\mathrm{sup}}||f(x)-f_{\\epsilon}(x)|| \\lt \\epsilon$$\n",
    "    holds for any $\\epsilon$ arbitrarily small (distance from $f$ to $f_{\\epsilon}$ can be infinitely small).\n",
    "    - The point-wise activation function provides the network with the ability to approximate non-linearities. Otherwise, the combination of linear transformation would also be a linear transformation.\n",
    "    \n",
    "- **Common activation functions**\n",
    "    - Logistic sigmoid function: \n",
    "        > $\\sigma_i(z)=\\frac{1}{1+e^{-z_i}}$\n",
    "    - Rectified linear unit (ReLU): \n",
    "        > $\\sigma_i(z)= \\left \\{\n",
    "        \\begin{aligned} \n",
    "        & 0 & if \\ z_i \\le 0 \\\\ \n",
    "        & z_i & if \\ z_i > 0 \n",
    "        \\end{aligned}\n",
    "        \\right.\n",
    "        $\n",
    "    - Leaky ReLU\n",
    "        > $\\sigma_i(z) = \\left \\{\n",
    "        \\begin{aligned}\n",
    "        & \\alpha z_i &if \\ z_i \\le 0 \\\\\n",
    "        & z_i & if \\ z_i > 0\n",
    "        \\end{aligned}\n",
    "        \\right.\n",
    "        $\n",
    "    - Softmax\n",
    "        > $\\sigma_i(z)=\\frac{e^{z_i}}{\\Sigma_je^{z_j}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
