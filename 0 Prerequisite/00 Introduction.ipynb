{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:** *Some basics concepts of Machine Learning.*\n",
    "\n",
    "**Written by:** *Zihao Xu*\n",
    "\n",
    "**Last update date:**: *06.05.2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence, Machine Learning, Deep Learning\n",
    "\n",
    "## Artificial Intelligence:\n",
    "\n",
    "- A.I. is a very **broad** field.\n",
    "- Turing test:\n",
    "    - 'A human tries to tell the difference between a human correspondent and a machine one. When they can't, according to Turing, the machine should be judged intelligent.'\n",
    "- In Oxford Dictionary \n",
    "    - 'the theory and development of computers systems **able to perform tasks that normally require human intelligence**, such as visual perception, speech recognition, decision-making, and translation between languages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Definition\n",
    "- A huge part in the concept of AI (Models, Algorithms, Theory) as well as useful tools for some AI applications (Computer Vision, Robotics).\n",
    "- **Train an algorithm to reproduce answers from data.**\n",
    "- Usually can be divided into **supervised learning** and **unsupervised learning**.\n",
    "\n",
    "### Dataset\n",
    "- A dataset is a collection of many **examples**, which are sometimes called **data points**\n",
    "- One common way of describing a dataset is with a **design matrix**\n",
    "    - Containing a different example in each row\n",
    "    - Each column corresponds to a different feature\n",
    "\n",
    "\n",
    "### Supervised Learning\n",
    "- To estimate a **function between input and output** given a dataset containing **input-output examples**.\n",
    "    - Examples: Regression, Classification, Filtering, ...\n",
    "    \n",
    "### Unsupervised Learning\n",
    "- To **model the input data directly**, or in other words, understand the input data directly given a dataset containing many features\n",
    "    - Examples: Clustering, Dimensionality Reduction, Density Estimation, Generative models, ...\n",
    "    - Motivation: Labeling data is always expensive (supervised) while gathering raw data is cheap (unsupervised).\n",
    "    - Yann LeCun, Head of Facebook AI, 2016\n",
    "        - 'AI systems today do not possess \"common sense\", which humans and animals acquire by observing the world, acting in it, and understanding the physical constraints of it. Some of us see unsupervised learning as the key towards machines with common sense.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common machine learning tasks\n",
    "### Classification\n",
    "- Specify which of $k$ categories some input belongs to\n",
    "- Typically, produce a function $f:\\mathbb{R}^n\\rightarrow\\{1,\\cdots,k\\}$. \n",
    "    - Assigns a input $\\mathbf{x}$ to a category identified by numeric code $y$\n",
    "- In some cases. produce a probability distribution over classes\n",
    "- One common example is object recognition\n",
    "\n",
    "### Regression\n",
    "- Predict a numerical value given some input\n",
    "- Output a function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$\n",
    "- Similar to classification except that the output format is different\n",
    "\n",
    "### Transcription\n",
    "- Observe a relatively unstructured representation of some kind of data and transcribe the information into discrete textual form\n",
    "- One example is to produce a text in the form of a sequence of characters given a photograph containing an image of the text\n",
    "\n",
    "### Machine translation\n",
    "- Given a sequence of symbols in some language, convert it into a sequence of symbols in another language\n",
    "\n",
    "### Structured output\n",
    "- Any tasks where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements\n",
    "- One example is pixel-wise segmentation of images, where every pixel in an image is assigned to a specific category\n",
    "- The program must output several values that are all tightly interrelated\n",
    "\n",
    "### Anomaly detection\n",
    "- Sift through a set of events or objects and flags some of them as being unusual or atypical\n",
    "- One example is credit card\n",
    "\n",
    "### Synthesis and sampling\n",
    "- Generate new examples that are similar to those in the given data\n",
    "\n",
    "### Imputation of missing values\n",
    "- Given a new example $\\mathbf{x}\\in\\mathbb{R}^n$ but with some entries $x_i$ of $\\mathbf{x}$ missing, provide a prediction of the missing values\n",
    "\n",
    "### Denoising\n",
    "- Given as input a corrupted example $\\tilde{\\mathbf{x}}\\in\\mathbb{R}^n$ obtained by an unknown corruption process from a clean example $\\mathbf{x}\\in\\mathbb{R}^n$, predict the clean example $\\mathbf{x}$ from its corrupted version\n",
    "\n",
    "### Density estimation\n",
    "- Learn a function $p_{\\text{model}}:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ where $p_{\\text{model}}$ can be interpreted as a probability density function on the space that the examples were drawn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Definition\n",
    "- A particular successful **Machine Learning method based on deep sequences of neural networks**.\n",
    "- Generally speaking, deep models are **sequential transformations learned from data**. If you use deep neural networks to define sequential transformations from input to output, then you do Deep Learning.\n",
    "\n",
    "### Successful Applications\n",
    "- Any machine learning task:\n",
    "    - Regression, Classification, Clustering, Dimensionality reduction, Density estimation, Filtering, ...\n",
    "- Natural language processing\n",
    "- Reinforcement learning\n",
    "    - self-driving cars, playing Atari games, AlphaGo\n",
    "- Design of engineering systems\n",
    "\n",
    "### Why does Deep Learning work so well?\n",
    "- **Basic theory**\n",
    "    - The universal approximation theorem (Cybenko, 1989)\n",
    "- **Deterministic Reason**\n",
    "    - Advanced learning algorithms (back-propagation, stochastic gradient descent, etc.), advanced software (tensorflow, pytorch) and advanced hardware (GPU) allow larger model sizes and more complicated computation\n",
    "    - The amount of available training data has increased\n",
    "- **Popular in various applications**\n",
    "    - Instead of handcrafting features in classical Machine Learning, Deep Learning let the deep model do all the feature engineering automatically.\n",
    "    - Deep Learning can automatically learn **a hierarchy of representations of high-dimensional data**.\n",
    "\n",
    "### Problems of applying Deep Learning in engineering\n",
    "- Many approaches are **black boxes** with no guarantees on convergence and performance.\n",
    "- Deep Models would make errors if data are perturbed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General Supervised Learning Process\n",
    "## Problem Definition\n",
    "### Mathematical Representation\n",
    "$$\n",
    "\\large{\n",
    "y=f_{gt}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "- $x$: Input of the function\n",
    "    - Called **features**, attributes, covariates or variables\n",
    "    - Can be numeric, categorical, discrete or nominal\n",
    "    - Examples:\n",
    "        - [GRE scores, GPA, major]\n",
    "        - An image\n",
    "        - A sentence\n",
    "        - A $d$-dimentional vector of numbers\n",
    "    \n",
    "- $y$: Output of the function\n",
    "    - Called output, response, target, or **label**\n",
    "    - Can be numeric, categorical, discrete or nominal\n",
    "        - The problem is known as Regression when output is numeric\n",
    "        - The problem is known as Classification when the output is categorical\n",
    "\n",
    "- $f_{gt}$: Ground truth of the function to be estimated\n",
    "    - Sometimes we know nothing about this function, such as its structure, type or basis functions.\n",
    "    - It stands for the true relationships between the input and output.\n",
    "\n",
    "### Goal\n",
    "- Approximate $f$ and make use of it, so that the ability to inferring corresponding $y$ from $x$ is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach\n",
    "### Mathematical Representation\n",
    "$$\n",
    "\\large{\n",
    "\\hat{y}=\\hat{f}_{\\theta}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "- $x$: Input of the function.\n",
    "\n",
    "- $\\hat{y}$: Output of the estimated function. Usually called **predictions**.\n",
    "\n",
    "- $\\hat{f}_{\\theta}$: Estimations of the ground truth function. Consists of basic functions we're familiar with and can be tuned with the parameter $\\theta$.\n",
    "\n",
    "### Training set\n",
    "- Given a set of **input-output** pairs which is called **Training Set** and denoted by $\\mathcal{D}= \\{ (x_i,y_i) \\} ^{n}_{i=1}$, tune the parameter $\\theta$ to achieve the \"best\" or at least a \"good \"answer, which means trying to make the predictions $\\hat{y}$ as close as possible to the ground truth $y$\n",
    "- Use such a tuned model to **predict** the value of output $y$ when some unseen input $x$ occur\n",
    "\n",
    "### Common approximations\n",
    "- Support vector machines\n",
    "- Radial basis functions\n",
    "- Gaussian mixture functions\n",
    "- Neural networks\n",
    "    - Very high capacity order\n",
    "    - Easy to train with modern computational tools\n",
    "    - Shallow neural networks: Use one easy-to-train layer\n",
    "    - Deep neural networks: Train a hierarchical stack of layers.\n",
    "- ...\n",
    "\n",
    "### Performance Measure\n",
    "- Measure the \"distance\" between ground truth $y$ and prediction $\\hat{y}=\\hat{f}_{\\theta}(x)$\n",
    "- The function to measure this \"distance\" is usually called **loss function** and is denoted by $l(\\theta)$\n",
    "- Largely depends on specific tasks and the output type. The required performance should be guaranteed when the loss is minimized.\n",
    "    - Regression: Mean Square Error\n",
    "    - Classification: Cross-Entropy Loss\n",
    "    - Localization: Intersection over Union\n",
    "    - ...\n",
    "\n",
    "### Optimization\n",
    "- Select $\\theta$ which minimizes the **loss function computed on training set**\n",
    "- Mathematically speaking: $\\theta^*=\\mathrm{arg} \\underset{\\theta}{\\mathrm{min}}\\{ l(\\theta)\\}$\n",
    "- Modern optimization algorithms \n",
    "    - Gradient descent\n",
    "    - Expectation maximization\n",
    "\n",
    "### Validation Set\n",
    "- Some times we are unsure about some hyper-parameters which are set manually like the model structure (or function basis)\n",
    "- First train different models on the same training set with different hyper-parameters, then test the performance of these models and choose the one with best performance on **another set of input-output pairs** which is usually called **Validation Set**\n",
    "- Do not have any intersection with the training set\n",
    "\n",
    "### Test Set\n",
    "- Usually we're interested in how well the trained model performs on data that it **has not seen before**, since this determines how well it will work when deployed in the real world\n",
    "- Evaluate the performance of the trained model on a **test set** of data that is separate from the data used for training and validation\n",
    "- Do not have any intersection with training set and validation set\n",
    "\n",
    "### Summary\n",
    "1. Collect a dataset which contains **input-output pairs**\n",
    "2. If necessary, divide the data set to three parts for training, validation and testing.\n",
    "3. Determine the function basis (model structure)\n",
    "4. Determine the loss function needs to be optimized\n",
    "5. Train the model by finding out $\\theta^*$ which minimized the loss function computed on **train set**\n",
    "6. Train the model under different hyper-parameters set manually and select the best hyper-parameters based on its performance on **validation set**\n",
    "7. Test such a well specific model on **test set**\n",
    "8. Use this model for **prediction** in real problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Free Lunch Theorem\n",
    "### Brief summary\n",
    "- All models are approximations\n",
    "- All models make assumptions\n",
    "- Assumptions are never perfect\n",
    "\n",
    "### For supervised learning\n",
    "- While a general-purpose algorithm has average performance on all types of problem, a highly specialized algorithm can reach a incredibly high performance on a specific type of problem, sacrificing its performance on all the other type of problems.\n",
    "- Always check the assumptions made implicitly or explicitly when selecting the model and algorithm\n",
    "- Do not expect an algorithm performs well in every situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# A single-hidden-layer neural network\n",
    "## General definition\n",
    "$$\n",
    "\\large{\n",
    "\\hat{\\mathbf{y}}=\\mathbf{A}^{(2)}\\sigma(\\mathbf{A}^{(1)}\\mathbf{X}+\\mathbf{b}^{(1)})+\\mathbf{b}^{(2)}\n",
    "}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{A}$: A matrix of multiplicative weights\n",
    "- $\\mathbf{b}$: A column vector of additive offsets, sometimes called *bias*\n",
    "- $\\sigma$: A point-wise activation function\n",
    "- $\\mathbf{X}=\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_n\\}$ is usually called the **input layer**\n",
    "- $\\mathbf{H}=\\sigma(\\mathbf{A}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)})$ is usually called a **hidden layer**\n",
    "- $\\hat{\\mathbf{y}}=\\{\\hat{y}_1,\\cdots,\\hat{y}_m\\}$ is usually called the **output layer**\n",
    "\n",
    "## Why does this structure work?\n",
    "- [Universal Approximation Theorem (Cybenko,1989)](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "    - Fix a continuous function $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ (activation function) and positive integers $d$, $D$. The function $\\sigma$ is not a polynomial if and only if, for every continuous function $f:\\mathbb{R}^d \\to \\mathbb{R}^D$ (target function), every compact subset $K$ of $\\mathbb{R}^d$, and every $\\epsilon \\gt0$ there exists a continuous function $f_{\\epsilon}:\\mathbb{R}^d \\to \\mathbb{R}^D$ (the layer output) with representation\n",
    "$$f_{\\epsilon}=W_2\\circ \\sigma \\circ W_1$$\n",
    "where $W_2$, $W_1$ are composable affine maps and $\\circ$ denotes component-wise composition, such that the approximation bound\n",
    "$$\\underset{x \\in K}{\\mathrm{sup}}||f(x)-f_{\\epsilon}(x)|| \\lt \\epsilon$$\n",
    "holds for any $\\epsilon$ arbitrarily small (distance from $f$ to $f_{\\epsilon}$ can be infinitely small).\n",
    "- The point-wise activation function provides the network with the ability to approximate non-linearities. Otherwise, the **combination of linear transformation would always be a linear transformation**.\n",
    "    \n",
    "## Common activation functions\n",
    "- Logistic sigmoid function: \n",
    "    - $\\sigma_i(z)=\\frac{1}{1+e^{-z_i}}$\n",
    "- Rectified linear unit (ReLU): \n",
    "    - $\\sigma_i(z)= \\left \\{\\begin{aligned} & 0 & if \\ z_i \\le 0 \\\\ & z_i & if \\ z_i > 0 \\end{aligned}\\right.$\n",
    "- Leaky ReLU\n",
    "    - $\\sigma_i(z) = \\left \\{\\begin{aligned}& \\alpha z_i &if \\ z_i \\le 0 \\\\& z_i & if \\ z_i > 0\\end{aligned}\\right.$\n",
    "- Softmax\n",
    "    - $\\sigma_i(z)=\\frac{e^{z_i}}{\\Sigma_je^{z_j}}$\n",
    "    \n",
    "## Deep Neural Networks\n",
    "For many years, the **shallow learning** consists of $1$ or $2$ layers of network was thought to be enough since the universal approximation theorem claimed that a single-hidden-layer network can learn any function. But over the past decade, there has been overwhelming empirical evidence that **deep learning** typically consists of $10$ to $100$ layers **dramatically outperforms shallow learning** on a wide range of real applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
